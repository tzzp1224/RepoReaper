#!/usr/bin/env python3
"""
æ£€ç´¢ç³»ç»Ÿç¦»çº¿è¯„ä¼°è„šæœ¬

ç”¨äºæµ‹è¯• chunking å’Œæ£€ç´¢ç­–ç•¥çš„å‡†ç¡®ç‡ã€‚
ä½¿ç”¨ golden_dataset.json ä¸­çš„æ ‡æ³¨æ•°æ®ä½œä¸º ground truthã€‚

ä½¿ç”¨æ–¹æ³•:
    python evaluation/test_retrieval.py --repo https://github.com/tiangolo/fastapi
    python evaluation/test_retrieval.py --repo https://github.com/tiangolo/fastapi --top-k 5
    python evaluation/test_retrieval.py --repo https://github.com/tiangolo/fastapi --verbose

Author: Dexter
Date: 2026-01-28
"""

import json
import os
import sys
import asyncio
import argparse
from typing import List, Dict, Tuple
from dataclasses import dataclass, field
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.vector_service import store_manager
from app.services.github_service import get_repo_structure


@dataclass
class RetrievalTestResult:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœ"""
    query: str
    expected_files: List[str]
    retrieved_files: List[str]
    hit: bool                      # æ˜¯å¦å‘½ä¸­ä»»æ„ä¸€ä¸ªé¢„æœŸæ–‡ä»¶
    recall: float                  # å¬å›ç‡: å‘½ä¸­çš„é¢„æœŸæ–‡ä»¶ / æ€»é¢„æœŸæ–‡ä»¶
    precision: float               # ç²¾ç¡®ç‡: å‘½ä¸­çš„é¢„æœŸæ–‡ä»¶ / æ£€ç´¢ç»“æœæ•°
    reciprocal_rank: float         # å€’æ•°æ’å: 1 / ç¬¬ä¸€ä¸ªå‘½ä¸­çš„ä½ç½®
    difficulty: str = ""
    category: str = ""


@dataclass
class EvaluationReport:
    """å®Œæ•´è¯„ä¼°æŠ¥å‘Š"""
    repo_url: str
    top_k: int
    total_queries: int
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    # èšåˆæŒ‡æ ‡
    hit_rate: float = 0.0          # å‘½ä¸­ç‡: è‡³å°‘å‘½ä¸­ä¸€ä¸ªçš„æŸ¥è¯¢æ¯”ä¾‹
    mean_recall: float = 0.0       # å¹³å‡å¬å›ç‡
    mean_precision: float = 0.0    # å¹³å‡ç²¾ç¡®ç‡
    mrr: float = 0.0               # Mean Reciprocal Rank
    
    # æŒ‰éš¾åº¦åˆ†ç»„
    by_difficulty: Dict[str, Dict] = field(default_factory=dict)
    
    # è¯¦ç»†ç»“æœ
    results: List[RetrievalTestResult] = field(default_factory=list)
    failed_cases: List[Dict] = field(default_factory=list)


class RetrievalEvaluator:
    """æ£€ç´¢ç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, golden_dataset_path: str = "evaluation/golden_dataset.json"):
        self.golden_dataset = self._load_golden_dataset(golden_dataset_path)
        print(f"ğŸ“Š Loaded {len(self.golden_dataset)} test cases from golden dataset")
    
    def _load_golden_dataset(self, path: str) -> List[Dict]:
        """åŠ è½½é»„é‡‘æ•°æ®é›†"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Golden dataset not found: {path}")
        
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    async def evaluate(
        self,
        repo_url: str,
        session_id: str = "eval_test",
        top_k: int = 5,
        verbose: bool = False
    ) -> EvaluationReport:
        """
        è¿è¡Œå®Œæ•´çš„æ£€ç´¢è¯„ä¼°
        
        Args:
            repo_url: è¦è¯„ä¼°çš„ä»“åº“ URL
            session_id: ä¼šè¯ ID
            top_k: æ¯æ¬¡æ£€ç´¢è¿”å›çš„æ–‡ä»¶æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        print(f"\n{'='*60}")
        print(f"ğŸ” Retrieval Evaluation")
        print(f"{'='*60}")
        print(f"Repository: {repo_url}")
        print(f"Top-K: {top_k}")
        print(f"Test Cases: {len(self.golden_dataset)}")
        print(f"{'='*60}\n")
        
        # è·å–ä»“åº“æ–‡ä»¶åˆ—è¡¨
        print("ğŸ“‚ Fetching repository structure...")
        file_list = get_repo_structure(repo_url)  # åŒæ­¥å‡½æ•°ï¼Œä¸éœ€è¦ await
        print(f"   Found {len(file_list)} files")
        
        # è·å–å‘é‡å­˜å‚¨
        store = store_manager.get_store(session_id)
        chunk_count = store.collection.count()  # ä½¿ç”¨ collection.count()
        if chunk_count == 0:
            print("\nâš ï¸  Vector store is empty!")
            print("   Please run the agent first to index the repository.")
            print("   Example: Access http://localhost:8000 and analyze the repo first.")
            return None
        print(f"   Vector store has {chunk_count} chunks")
        
        # è¿è¡Œè¯„ä¼°
        report = EvaluationReport(
            repo_url=repo_url,
            top_k=top_k,
            total_queries=len(self.golden_dataset)
        )
        
        hits = 0
        recalls = []
        precisions = []
        reciprocal_ranks = []
        
        difficulty_stats = {}
        
        for i, sample in enumerate(self.golden_dataset):
            query = sample.get("query", "")
            expected_files = sample.get("expected_files", [])
            difficulty = sample.get("difficulty", "medium")
            category = sample.get("category", "general")
            
            if not query or not expected_files:
                continue
            
            # æ‰§è¡Œæ£€ç´¢ (ä½¿ç”¨ hybrid search)
            try:
                results = await store.search_hybrid(query, top_k=top_k)
            except Exception as e:
                if verbose:
                    print(f"  [ERR] Search failed: {e}")
                continue
            
            # æå–æ£€ç´¢åˆ°çš„æ–‡ä»¶è·¯å¾„
            retrieved_files = []
            for doc in results:
                if isinstance(doc, dict):
                    file_path = doc.get("file", "")
                    if file_path and file_path not in retrieved_files:
                        retrieved_files.append(file_path)
            
            # è®¡ç®—æŒ‡æ ‡
            expected_set = set(expected_files)
            retrieved_set = set(retrieved_files[:top_k])
            
            # å‘½ä¸­çš„æ–‡ä»¶
            hits_set = expected_set & retrieved_set
            
            # Hit: æ˜¯å¦å‘½ä¸­ä»»æ„ä¸€ä¸ª
            hit = len(hits_set) > 0
            if hit:
                hits += 1
            
            # Recall: å‘½ä¸­çš„ / æœŸæœ›çš„
            recall = len(hits_set) / len(expected_set) if expected_set else 0
            recalls.append(recall)
            
            # Precision: å‘½ä¸­çš„ / æ£€ç´¢çš„
            precision = len(hits_set) / min(len(retrieved_files), top_k) if retrieved_files else 0
            precisions.append(precision)
            
            # Reciprocal Rank: 1 / ç¬¬ä¸€ä¸ªå‘½ä¸­çš„ä½ç½®
            rr = 0.0
            for rank, file in enumerate(retrieved_files[:top_k], 1):
                if file in expected_set:
                    rr = 1.0 / rank
                    break
            reciprocal_ranks.append(rr)
            
            # è®°å½•ç»“æœ
            result = RetrievalTestResult(
                query=query,
                expected_files=expected_files,
                retrieved_files=retrieved_files[:top_k],
                hit=hit,
                recall=recall,
                precision=precision,
                reciprocal_rank=rr,
                difficulty=difficulty,
                category=category
            )
            report.results.append(result)
            
            # æŒ‰éš¾åº¦ç»Ÿè®¡
            if difficulty not in difficulty_stats:
                difficulty_stats[difficulty] = {"hits": 0, "total": 0, "recalls": [], "precisions": []}
            difficulty_stats[difficulty]["total"] += 1
            if hit:
                difficulty_stats[difficulty]["hits"] += 1
            difficulty_stats[difficulty]["recalls"].append(recall)
            difficulty_stats[difficulty]["precisions"].append(precision)
            
            # è®°å½•å¤±è´¥æ¡ˆä¾‹
            if not hit:
                report.failed_cases.append({
                    "query": query,
                    "expected": expected_files,
                    "retrieved": retrieved_files[:top_k],
                    "difficulty": difficulty
                })
            
            # æ‰“å°è¿›åº¦
            if verbose:
                status = "âœ…" if hit else "âŒ"
                print(f"  [{i+1:3d}] {status} Recall={recall:.2f} | {query[:50]}...")
            else:
                print(f"\r  Progress: {i+1}/{len(self.golden_dataset)}", end="")
        
        print("\n")
        
        # è®¡ç®—èšåˆæŒ‡æ ‡
        report.hit_rate = hits / len(self.golden_dataset) if self.golden_dataset else 0
        report.mean_recall = sum(recalls) / len(recalls) if recalls else 0
        report.mean_precision = sum(precisions) / len(precisions) if precisions else 0
        report.mrr = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0
        
        # æŒ‰éš¾åº¦æ±‡æ€»
        for diff, stats in difficulty_stats.items():
            report.by_difficulty[diff] = {
                "hit_rate": stats["hits"] / stats["total"] if stats["total"] else 0,
                "mean_recall": sum(stats["recalls"]) / len(stats["recalls"]) if stats["recalls"] else 0,
                "mean_precision": sum(stats["precisions"]) / len(stats["precisions"]) if stats["precisions"] else 0,
                "total": stats["total"]
            }
        
        return report
    
    def print_report(self, report: EvaluationReport):
        """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
        print(f"\n{'='*60}")
        print(f"ğŸ“Š RETRIEVAL EVALUATION REPORT")
        print(f"{'='*60}")
        print(f"Repository: {report.repo_url}")
        print(f"Top-K: {report.top_k}")
        print(f"Total Queries: {report.total_queries}")
        print(f"Timestamp: {report.timestamp}")
        print(f"{'='*60}\n")
        
        print("ğŸ“ˆ OVERALL METRICS")
        print(f"   Hit Rate:       {report.hit_rate:.1%}")
        print(f"   Mean Recall:    {report.mean_recall:.1%}")
        print(f"   Mean Precision: {report.mean_precision:.1%}")
        print(f"   MRR:            {report.mrr:.3f}")
        
        print(f"\nğŸ“Š BY DIFFICULTY")
        for diff, stats in sorted(report.by_difficulty.items()):
            print(f"   {diff.upper():8s} | Hit: {stats['hit_rate']:.1%} | Recall: {stats['mean_recall']:.1%} | n={stats['total']}")
        
        if report.failed_cases:
            print(f"\nâŒ FAILED CASES ({len(report.failed_cases)} total)")
            for case in report.failed_cases[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   Query: {case['query'][:60]}...")
                print(f"   Expected: {case['expected']}")
                print(f"   Got: {case['retrieved'][:3]}...")
                print()
        
        print(f"{'='*60}")
    
    def save_report(self, report: EvaluationReport, output_path: str = "evaluation/retrieval_report.json"):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        data = {
            "repo_url": report.repo_url,
            "top_k": report.top_k,
            "total_queries": report.total_queries,
            "timestamp": report.timestamp,
            "metrics": {
                "hit_rate": report.hit_rate,
                "mean_recall": report.mean_recall,
                "mean_precision": report.mean_precision,
                "mrr": report.mrr
            },
            "by_difficulty": report.by_difficulty,
            "failed_cases": report.failed_cases
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Report saved to: {output_path}")


async def main():
    parser = argparse.ArgumentParser(description="Evaluate retrieval system using golden dataset")
    parser.add_argument("--repo", required=True, help="GitHub repository URL to evaluate")
    parser.add_argument("--top-k", type=int, default=5, help="Number of results to retrieve (default: 5)")
    parser.add_argument("--session", default="eval_test", help="Session ID for vector store")
    parser.add_argument("--verbose", "-v", action="store_true", help="Print detailed results")
    parser.add_argument("--save", action="store_true", help="Save report to file")
    
    args = parser.parse_args()
    
    evaluator = RetrievalEvaluator()
    report = await evaluator.evaluate(
        repo_url=args.repo,
        session_id=args.session,
        top_k=args.top_k,
        verbose=args.verbose
    )
    
    if report:
        evaluator.print_report(report)
        if args.save:
            evaluator.save_report(report)


if __name__ == "__main__":
    asyncio.run(main())
