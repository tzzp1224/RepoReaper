# æ–‡ä»¶è·¯å¾„: evaluation/analyze_eval_results.py
"""
è‡ªåŠ¨åŒ–æ•°æ®åˆ†æè„šæœ¬
ç”¨äºåˆ†æè¯„ä¼°ç»“æœï¼Œè¯†åˆ«é—®é¢˜å¹¶ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š

æ ¸å¿ƒåŠŸèƒ½:
1. è‡ªåŠ¨è¯»å–æ‰€æœ‰è¯„ä¼°ç»“æœ
2. æŒ‰é—®é¢˜ç±»å‹åˆ†ç±» Bad Case
3. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
4. æ¨èä¼˜åŒ–æ–¹å‘

Author: Dexter
Date: 2025-01-27
"""

import os
from typing import Dict, List
from collections import Counter, defaultdict
from datetime import datetime

from evaluation.utils import read_jsonl


class EvaluationAnalyzer:
    """è¯„ä¼°ç»“æœåˆ†æå™¨"""
    
    def __init__(self, eval_results_file: str = "evaluation/sft_data/eval_results.jsonl"):
        self.eval_results_file = eval_results_file
        self.results: List[Dict] = read_jsonl(eval_results_file)
        if not self.results:
            print(f"âš ï¸ No results loaded from: {eval_results_file}")
    
    def get_basic_stats(self) -> Dict:
        """è·å–åŸºæœ¬ç»Ÿè®¡"""
        if not self.results:
            return {}
        
        scores = [r.get("overall_score", 0) for r in self.results]
        tiers = [r.get("data_quality_tier", "unknown") for r in self.results]
        
        return {
            "total_evaluations": len(self.results),
            "avg_score": sum(scores) / len(scores) if scores else 0,
            "max_score": max(scores) if scores else 0,
            "min_score": min(scores) if scores else 0,
            "median_score": sorted(scores)[len(scores)//2] if scores else 0,
            "quality_distribution": dict(Counter(tiers)),
            "sft_ready_count": sum(1 for r in self.results if r.get("sft_ready", False))
        }
    
    def identify_bad_cases(self, threshold: float = 0.6) -> List[Dict]:
        """
        è¯†åˆ« Bad Case (å¾—åˆ†ä½äºé˜ˆå€¼çš„ç»“æœ)
        è¿”å›æŒ‰å¾—åˆ†æ’åºçš„ç»“æœ
        """
        bad_cases = [r for r in self.results if r.get("overall_score", 1) < threshold]
        return sorted(bad_cases, key=lambda x: x.get("overall_score", 1))
    
    def categorize_failures(self) -> Dict[str, List[Dict]]:
        """
        æŒ‰å¤±è´¥åŸå› åˆ†ç±» Bad Case
        
        å¤±è´¥ç±»å‹:
        - retrieval_failure: æ£€ç´¢æœªå‘½ä¸­
        - generation_hallucination: ç”Ÿæˆå¹»è§‰
        - generation_incomplete: å›ç­”ä¸å®Œæ•´
        - tool_call_error: å·¥å…·è°ƒç”¨å¤±è´¥
        """
        categorized = defaultdict(list)
        
        for result in self.identify_bad_cases():
            reasons = []
            
            # æ£€æŸ¥æ£€ç´¢å¤±è´¥
            if result.get("retrieval"):
                retrieval = result["retrieval"]
                if retrieval.get("hit_rate", 1) == 0:
                    reasons.append("retrieval_failure")
                elif retrieval.get("recall_at_k", 1) < 0.5:
                    reasons.append("retrieval_low_recall")
            
            # æ£€æŸ¥ç”Ÿæˆé—®é¢˜
            if result.get("generation"):
                generation = result["generation"]
                if generation.get("faithfulness", 1) < 0.5:
                    reasons.append("generation_hallucination")
                if generation.get("answer_completeness", 1) < 0.4:
                    reasons.append("generation_incomplete")
                if generation.get("hallucination_count", 0) > 0:
                    reasons.append("hallucination_detected")
            
            # æ£€æŸ¥Agentè¡Œä¸º
            if result.get("agentic"):
                agentic = result["agentic"]
                if not agentic.get("success", True):
                    reasons.append("agentic_failure")
            
            # å¦‚æœæ²¡æœ‰å…·ä½“åŸå› ,æ ‡è®°ä¸ºunknown
            if not reasons:
                reasons.append("unknown")
            
            for reason in reasons:
                categorized[reason].append(result)
        
        return dict(categorized)
    
    def layer_performance(self) -> Dict[str, Dict]:
        """åˆ†æå„å±‚æ€§èƒ½"""
        layer_scores = defaultdict(list)
        
        for result in self.results:
            if result.get("query_rewrite"):
                score = result["query_rewrite"].get("overall_score", 0)
                if score:
                    layer_scores["query_rewrite"].append(score)
            
            if result.get("retrieval"):
                score = result["retrieval"].get("overall_score", 0)
                if score:
                    layer_scores["retrieval"].append(score)
            
            if result.get("generation"):
                score = result["generation"].get("overall_score", 0)
                if score:
                    layer_scores["generation"].append(score)
            
            if result.get("agentic"):
                score = result["agentic"].get("overall_score", 0)
                if score:
                    layer_scores["agentic"].append(score)
        
        # è®¡ç®—æ¯å±‚çš„ç»Ÿè®¡
        layer_stats = {}
        for layer, scores in layer_scores.items():
            if scores:
                layer_stats[layer] = {
                    "avg": sum(scores) / len(scores),
                    "min": min(scores),
                    "max": max(scores),
                    "count": len(scores)
                }
        
        return layer_stats
    
    def get_recommendations(self) -> List[str]:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†æå„å±‚æ€§èƒ½
        layer_perf = self.layer_performance()
        
        # æ£€ç´¢å±‚åˆ†æ
        if "retrieval" in layer_perf:
            retrieval_score = layer_perf["retrieval"]["avg"]
            if retrieval_score < 0.7:
                recommendations.append(
                    "ğŸ”´ RETRIEVAL å±‚æ€§èƒ½å·® (avg: {:.2f})\n"
                    "  å»ºè®®:\n"
                    "  1. æ£€æŸ¥ chunking ç­–ç•¥æ˜¯å¦è¿‡åº¦åˆ†å‰²\n"
                    "  2. ä¼˜åŒ– embedding æ¨¡å‹ (è€ƒè™‘æ›´å¼ºçš„æ¨¡å‹)\n"
                    "  3. è°ƒæ•´æ··åˆæ£€ç´¢çš„æƒé‡ (BM25 vs Vector)\n"
                    "  4. åˆ†æå®é™…å¬å›çš„æ–‡ä»¶,çœ‹æ˜¯å¦ä¸é¢„æœŸåç¦»".format(retrieval_score)
                )
        
        # ç”Ÿæˆå±‚åˆ†æ
        if "generation" in layer_perf:
            gen_score = layer_perf["generation"]["avg"]
            if gen_score < 0.7:
                recommendations.append(
                    "ğŸŸ¡ GENERATION å±‚å­˜åœ¨é—®é¢˜ (avg: {:.2f})\n"
                    "  å»ºè®®:\n"
                    "  1. æ£€æŸ¥ Prompt æ˜¯å¦æ¸…æ™° (å¯èƒ½LLMç†è§£åå·®)\n"
                    "  2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¹»è§‰ (ç”Ÿæˆä¸å­˜åœ¨çš„å‡½æ•°åç­‰)\n"
                    "  3. ä¼˜åŒ– Context çš„ç»„ç»‡æ–¹å¼\n"
                    "  4. è€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„LLMæ¨¡å‹".format(gen_score)
                )
        
        # Query Rewrite åˆ†æ
        if "query_rewrite" in layer_perf:
            rewrite_score = layer_perf["query_rewrite"]["avg"]
            if rewrite_score < 0.6:
                recommendations.append(
                    "ğŸŸ  QUERY_REWRITE å±‚å‡†ç¡®åº¦ä½ (avg: {:.2f})\n"
                    "  å»ºè®®:\n"
                    "  1. ä¼˜åŒ–å…³é”®è¯æå– Prompt\n"
                    "  2. å¢åŠ å¤šè¯­è¨€å¤„ç†æ”¯æŒ\n"
                    "  3. æ·»åŠ é¢†åŸŸè¯æ±‡è¡¨ (Domain Vocabulary)".format(rewrite_score)
                )
        
        # é€šç”¨å»ºè®®
        stats = self.get_basic_stats()
        if stats.get("sft_ready_count", 0) / max(stats.get("total_evaluations", 1), 1) < 0.5:
            recommendations.append(
                "âš ï¸ SFT å¯ç”¨æ•°æ®ä¸è¶³ (< 50%)\n"
                "  ç«‹å³è¡ŒåŠ¨:\n"
                "  1. è¿è¡Œ continuous_eval è„šæœ¬æ”¶é›†æ›´å¤šæ•°æ®\n"
                "  2. å¯¹ç°æœ‰æ•°æ®è¿›è¡Œè‡ªçº æ­£ (Self-Correction)\n"
                "  3. æ‰©å±•é»„é‡‘æ•°æ®é›†æ¥æ”¹è¿›æ¨¡å‹"
            )
        
        return recommendations
    
    def generate_report(self, output_file: str = "evaluation/analysis_report.md") -> str:
        """ç”Ÿæˆå®Œæ•´çš„åˆ†ææŠ¥å‘Š"""
        
        report = []
        report.append("# ğŸ“Š GitHub Agent è¯„ä¼°åˆ†ææŠ¥å‘Š\n")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        report.append("---\n")
        
        # 1. åŸºæœ¬ç»Ÿè®¡
        stats = self.get_basic_stats()
        report.append("## ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡\n")
        report.append(f"- æ€»è¯„ä¼°æ¬¡æ•°: {stats.get('total_evaluations', 0)}\n")
        report.append(f"- å¹³å‡å¾—åˆ†: {stats.get('avg_score', 0):.3f}\n")
        report.append(f"- æœ€é«˜å¾—åˆ†: {stats.get('max_score', 0):.3f}\n")
        report.append(f"- æœ€ä½å¾—åˆ†: {stats.get('min_score', 0):.3f}\n")
        report.append(f"- ä¸­ä½æ•°å¾—åˆ†: {stats.get('median_score', 0):.3f}\n")
        report.append(f"- SFT å¯ç”¨æ ·æœ¬: {stats.get('sft_ready_count', 0)}\n\n")
        
        # 2. è´¨é‡åˆ†çº§åˆ†å¸ƒ
        report.append("## ğŸ† è´¨é‡åˆ†çº§åˆ†å¸ƒ\n")
        distribution = stats.get("quality_distribution", {})
        for tier, count in sorted(distribution.items()):
            percentage = (count / stats.get('total_evaluations', 1)) * 100
            report.append(f"- {tier.upper()}: {count} ({percentage:.1f}%)\n")
        report.append("\n")
        
        # 3. å„å±‚æ€§èƒ½
        report.append("## ğŸ¯ å„å±‚æ€§èƒ½åˆ†æ\n\n")
        layer_perf = self.layer_performance()
        for layer in ["query_rewrite", "retrieval", "generation", "agentic"]:
            if layer in layer_perf:
                perf = layer_perf[layer]
                report.append(f"### {layer.upper()}\n")
                report.append(f"- å¹³å‡å¾—åˆ†: {perf['avg']:.3f}\n")
                report.append(f"- èŒƒå›´: [{perf['min']:.3f}, {perf['max']:.3f}]\n")
                report.append(f"- æ ·æœ¬æ•°: {perf['count']}\n\n")
        
        # 4. Bad Case åˆ†ç±»
        report.append("## ğŸ”´ Bad Case åˆ†æ\n\n")
        failures = self.categorize_failures()
        for reason, cases in sorted(failures.items(), key=lambda x: -len(x[1])):
            report.append(f"### {reason} ({len(cases)} cases)\n")
            for case in cases[:3]:  # æ˜¾ç¤ºtop 3
                report.append(f"- æŸ¥è¯¢: {case.get('query', 'N/A')[:60]}...\n")
                report.append(f"  å¾—åˆ†: {case.get('overall_score', 0):.3f}\n")
        report.append("\n")
        
        # 5. æ¨èè¡ŒåŠ¨
        report.append("## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n\n")
        recommendations = self.get_recommendations()
        for i, rec in enumerate(recommendations, 1):
            report.append(f"{i}. {rec}\n\n")
        
        # å†™å…¥æ–‡ä»¶
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.writelines(report)
        
        return "".join(report)
    
    def export_bad_cases_csv(self, output_file: str = "evaluation/bad_cases.csv") -> None:
        """å¯¼å‡º Bad Case ä¸º CSV (ç”¨äºäººå·¥å®¡æŸ¥)"""
        import csv
        
        bad_cases = self.identify_bad_cases()
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=[
                "query", "overall_score", "tier",
                "retrieval_score", "generation_score", "agentic_score",
                "error_message", "timestamp"
            ])
            
            writer.writeheader()
            for case in bad_cases:
                writer.writerow({
                    "query": case.get("query", ""),
                    "overall_score": case.get("overall_score", 0),
                    "tier": case.get("data_quality_tier", "unknown"),
                    "retrieval_score": case.get("retrieval", {}).get("overall_score", 0),
                    "generation_score": case.get("generation", {}).get("overall_score", 0),
                    "agentic_score": case.get("agentic", {}).get("overall_score", 0),
                    "error_message": case.get("error_message", ""),
                    "timestamp": case.get("timestamp", "")
                })
        
        print(f"âœ… Exported {len(bad_cases)} bad cases to {output_file}")


# ============================================================================
# å‘½ä»¤è¡Œå·¥å…·
# ============================================================================

def print_summary(analyzer: EvaluationAnalyzer):
    """æ‰“å°æ‘˜è¦"""
    print("\n" + "=" * 70)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
    print("=" * 70)
    
    stats = analyzer.get_basic_stats()
    
    print(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»è¯„ä¼°: {stats.get('total_evaluations', 0)}")
    print(f"  å¹³å‡åˆ†: {stats.get('avg_score', 0):.3f}")
    print(f"  åˆ†å¸ƒ: {stats.get('quality_distribution', {})}")
    print(f"  SFTå¯ç”¨: {stats.get('sft_ready_count', 0)}")
    
    print(f"\nğŸ¯ å„å±‚æ€§èƒ½:")
    layer_perf = analyzer.layer_performance()
    for layer, perf in layer_perf.items():
        print(f"  {layer:.<30} {perf['avg']:.3f} (avg)")
    
    print(f"\nğŸ”´ Bad Case Top 5:")
    bad_cases = analyzer.identify_bad_cases()[:5]
    for i, case in enumerate(bad_cases, 1):
        print(f"  {i}. {case.get('query', 'N/A')[:40]:<40} Score: {case.get('overall_score', 0):.3f}")
    
    print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    recommendations = analyzer.get_recommendations()
    for rec in recommendations[:3]:
        print(f"  - {rec.split(chr(10))[0]}")
    
    print("\n" + "=" * 70)


def main():
    import sys
    
    analyzer = EvaluationAnalyzer()
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "summary":
            print_summary(analyzer)
        
        elif command == "report":
            report = analyzer.generate_report()
            print(report)
        
        elif command == "bad-cases":
            analyzer.export_bad_cases_csv()
            bad_cases = analyzer.identify_bad_cases()
            print(f"\nâœ… Found {len(bad_cases)} bad cases")
            print("è¯¦è§ evaluation/bad_cases.csv")
        
        elif command == "layer-perf":
            layer_perf = analyzer.layer_performance()
            print("\nğŸ¯ å„å±‚æ€§èƒ½:")
            for layer, perf in layer_perf.items():
                print(f"\n{layer.upper()}:")
                print(f"  Average: {perf['avg']:.3f}")
                print(f"  Range: [{perf['min']:.3f}, {perf['max']:.3f}]")
                print(f"  Samples: {perf['count']}")
        
        elif command == "recommendations":
            recs = analyzer.get_recommendations()
            print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\n")
            for i, rec in enumerate(recs, 1):
                print(f"{i}.\n{rec}\n")
        
        else:
            print(f"Unknown command: {command}")
    
    else:
        print("è‡ªåŠ¨åŒ–è¯„ä¼°æ•°æ®åˆ†æå·¥å…·")
        print()
        print("ç”¨æ³•:")
        print("  python analyze_eval_results.py summary         # å¿«é€Ÿæ‘˜è¦")
        print("  python analyze_eval_results.py report          # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š")
        print("  python analyze_eval_results.py bad-cases       # å¯¼å‡ºBad Case")
        print("  python analyze_eval_results.py layer-perf      # å„å±‚æ€§èƒ½åˆ†æ")
        print("  python analyze_eval_results.py recommendations # ä¼˜åŒ–å»ºè®®")


if __name__ == "__main__":
    main()
