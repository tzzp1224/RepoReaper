#!/usr/bin/env python3
"""
SFT æ•°æ®æ¸…æ´—ä¸å¯¼å‡ºè„šæœ¬

åŠŸèƒ½:
1. ä» eval_results.jsonl è¯»å–åŸå§‹è¯„ä¼°æ•°æ®
2. åº”ç”¨ä¸¥æ ¼çš„è´¨é‡è¿‡æ»¤è§„åˆ™
3. è½¬æ¢ä¸ºæ ‡å‡† SFT è®­ç»ƒæ ¼å¼
4. å¯¼å‡ºä¸ºå¯ç›´æ¥ç”¨äºè®­ç»ƒçš„æ•°æ®é›†

Author: Dexter
Date: 2026-01-28
"""

import json
import os
from datetime import datetime
from typing import Dict, List, Tuple
from pathlib import Path

from evaluation.utils import is_chatty_query, has_code_indicators


# ============================================================================
# é…ç½®
# ============================================================================

class CleaningConfig:
    """æ•°æ®æ¸…æ´—é…ç½®"""
    # è´¨é‡é˜ˆå€¼
    MIN_OVERALL_SCORE = 0.7          # æœ€ä½ç»¼åˆåˆ†
    MIN_FAITHFULNESS = 0.6           # æœ€ä½ faithfulness
    MIN_ANSWER_RELEVANCE = 0.6       # æœ€ä½ answer_relevance
    
    # é•¿åº¦é˜ˆå€¼
    MIN_QUERY_LENGTH = 10            # æœ€çŸ­ query
    MIN_ANSWER_LENGTH = 100          # æœ€çŸ­ answer
    MIN_CONTEXT_LENGTH = 50          # æœ€çŸ­ context
    MAX_CONTEXT_LENGTH = 4000        # æœ€é•¿ contextï¼ˆæˆªæ–­ï¼‰
    
    # å¿…é¡»æ¡ä»¶
    REQUIRE_REPO_URL = True          # å¿…é¡»æœ‰ä»“åº“ URL
    REQUIRE_CODE_IN_CONTEXT = True   # ä¸Šä¸‹æ–‡å¿…é¡»åŒ…å«ä»£ç 
    
    # è¾“å‡ºé…ç½®
    OUTPUT_DIR = "evaluation/sft_data/cleaned"


# ============================================================================
# æ•°æ®æ¸…æ´—é€»è¾‘
# ============================================================================

def validate_sample(sample: Dict, config: CleaningConfig) -> Tuple[bool, str]:
    """
    éªŒè¯å•ä¸ªæ ·æœ¬æ˜¯å¦ç¬¦åˆè´¨é‡æ ‡å‡†
    
    Returns:
        (is_valid, rejection_reason)
    """
    # 1. æ£€æŸ¥åŸºæœ¬å­—æ®µå­˜åœ¨
    if not sample.get("query"):
        return False, "missing_query"
    
    if not sample.get("generation"):
        return False, "missing_generation"
    
    gen = sample["generation"]
    
    # 2. æ£€æŸ¥ repo_url
    if config.REQUIRE_REPO_URL and not sample.get("repo_url"):
        return False, "missing_repo_url"
    
    # 3. æ£€æŸ¥è´¨é‡åˆ†æ•°
    overall_score = sample.get("overall_score", 0)
    if overall_score < config.MIN_OVERALL_SCORE:
        return False, f"low_score:{overall_score:.2f}"
    
    faithfulness = gen.get("faithfulness", 0)
    if faithfulness < config.MIN_FAITHFULNESS:
        return False, f"low_faithfulness:{faithfulness:.2f}"
    
    answer_relevance = gen.get("answer_relevance", 0)
    if answer_relevance < config.MIN_ANSWER_RELEVANCE:
        return False, f"low_relevance:{answer_relevance:.2f}"
    
    # 4. æ£€æŸ¥é•¿åº¦
    query = sample.get("query", "")
    if len(query) < config.MIN_QUERY_LENGTH:
        return False, f"short_query:{len(query)}"
    
    answer = gen.get("generated_answer", "")
    if len(answer) < config.MIN_ANSWER_LENGTH:
        return False, f"short_answer:{len(answer)}"
    
    context = gen.get("retrieved_context", "")
    if len(context) < config.MIN_CONTEXT_LENGTH:
        return False, f"short_context:{len(context)}"
    
    # 5. æ£€æŸ¥é—²èŠ
    if is_chatty_query(query):
        return False, "chatty_query"
    
    # 6. æ£€æŸ¥ä»£ç å­˜åœ¨
    if config.REQUIRE_CODE_IN_CONTEXT and not has_code_indicators(context):
        return False, "no_code_in_context"
    
    return True, "passed"


def transform_to_sft_format(sample: Dict, config: CleaningConfig) -> Dict:
    """
    å°†åŸå§‹è¯„ä¼°æ•°æ®è½¬æ¢ä¸ºæ ‡å‡† SFT æ ¼å¼
    """
    gen = sample["generation"]
    
    # æ¸…ç†å’Œæˆªæ–­ context
    context = gen.get("retrieved_context", "")
    if len(context) > config.MAX_CONTEXT_LENGTH:
        context = context[:config.MAX_CONTEXT_LENGTH] + "\n... [truncated]"
    
    # æ„å»ºæ ‡å‡† SFT æ ¼å¼
    sft_sample = {
        # === æ ¸å¿ƒè®­ç»ƒå­—æ®µ ===
        "instruction": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„GitHubä»£ç ä»“åº“åˆ†æåŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ä»£ç ä¸Šä¸‹æ–‡ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·å…³äºä»£ç å®ç°ã€æ¶æ„è®¾è®¡ã€åŠŸèƒ½é€»è¾‘ç­‰é—®é¢˜ã€‚å›ç­”æ—¶åº”è¯¥ï¼š1) ç›´æ¥å¼•ç”¨ç›¸å…³ä»£ç  2) è§£é‡Šä»£ç çš„å·¥ä½œåŸç† 3) å¦‚æœ‰å¿…è¦ï¼Œæä¾›ä»£ç ç¤ºä¾‹ã€‚",
        "input": f"[ç”¨æˆ·é—®é¢˜]\n{sample['query']}\n\n[ä»£ç ä¸Šä¸‹æ–‡]\n{context}",
        "output": gen.get("generated_answer", ""),
        
        # === å…ƒæ•°æ® ===
        "metadata": {
            "query": sample["query"],
            "repo_url": sample.get("repo_url", ""),
            "language": sample.get("language", "en"),
            "session_id": sample.get("session_id", ""),
            "timestamp": sample.get("timestamp", ""),
            "quality_tier": sample.get("data_quality_tier", ""),
            "overall_score": sample.get("overall_score", 0),
            "faithfulness": gen.get("faithfulness", 0),
            "answer_relevance": gen.get("answer_relevance", 0),
            "answer_completeness": gen.get("answer_completeness", 0),
            "code_correctness": gen.get("code_correctness", 0),
        }
    }
    
    return sft_sample


def clean_and_export(
    input_file: str = "evaluation/sft_data/eval_results.jsonl",
    config: CleaningConfig = None
) -> Dict:
    """
    æ¸…æ´—æ•°æ®å¹¶å¯¼å‡º
    
    Returns:
        ç»Ÿè®¡ä¿¡æ¯
    """
    config = config or CleaningConfig()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(config.OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç»Ÿè®¡
    stats = {
        "total_read": 0,
        "passed": 0,
        "rejected": 0,
        "rejection_reasons": {},
        "quality_distribution": {"gold": 0, "silver": 0, "bronze": 0}
    }
    
    # è¾“å‡ºæ–‡ä»¶
    output_file = output_dir / f"sft_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    rejected_file = output_dir / f"rejected_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
    
    print("=" * 60)
    print("ğŸ§¹ SFT æ•°æ®æ¸…æ´—ä¸å¯¼å‡º")
    print("=" * 60)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è´¨é‡é˜ˆå€¼: score >= {config.MIN_OVERALL_SCORE}")
    print()
    
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return stats
    
    passed_samples = []
    rejected_samples = []
    
    # è¯»å–å¹¶å¤„ç†
    with open(input_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                sample = json.loads(line)
                stats["total_read"] += 1
                
                # éªŒè¯
                is_valid, reason = validate_sample(sample, config)
                
                if is_valid:
                    # è½¬æ¢æ ¼å¼
                    sft_sample = transform_to_sft_format(sample, config)
                    passed_samples.append(sft_sample)
                    stats["passed"] += 1
                    
                    # ç»Ÿè®¡è´¨é‡åˆ†å¸ƒ
                    score = sample.get("overall_score", 0)
                    if score > 0.9:
                        stats["quality_distribution"]["gold"] += 1
                    elif score > 0.7:
                        stats["quality_distribution"]["silver"] += 1
                    else:
                        stats["quality_distribution"]["bronze"] += 1
                else:
                    rejected_samples.append({
                        "reason": reason,
                        "query": sample.get("query", "")[:50],
                        "score": sample.get("overall_score", 0)
                    })
                    stats["rejected"] += 1
                    stats["rejection_reasons"][reason] = stats["rejection_reasons"].get(reason, 0) + 1
                    
            except json.JSONDecodeError as e:
                print(f"  âš ï¸ ç¬¬ {line_num} è¡Œ JSON è§£æé”™è¯¯: {e}")
                continue
    
    # å†™å…¥é€šè¿‡çš„æ ·æœ¬
    if passed_samples:
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in passed_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        print(f"âœ… å·²å¯¼å‡º {len(passed_samples)} æ¡é«˜è´¨é‡æ ·æœ¬åˆ°: {output_file}")
    
    # å†™å…¥æ‹’ç»çš„æ ·æœ¬ï¼ˆç”¨äºåˆ†æï¼‰
    if rejected_samples:
        with open(rejected_file, 'w', encoding='utf-8') as f:
            for sample in rejected_samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
        print(f"ğŸ“ å·²è®°å½• {len(rejected_samples)} æ¡è¢«æ‹’ç»æ ·æœ¬åˆ°: {rejected_file}")
    
    # æ‰“å°ç»Ÿè®¡
    print()
    print("=" * 60)
    print("ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 60)
    print(f"æ€»è¯»å–: {stats['total_read']}")
    print(f"é€šè¿‡:   {stats['passed']} ({stats['passed']/max(stats['total_read'],1)*100:.1f}%)")
    print(f"æ‹’ç»:   {stats['rejected']} ({stats['rejected']/max(stats['total_read'],1)*100:.1f}%)")
    print()
    print("è´¨é‡åˆ†å¸ƒ:")
    print(f"  ğŸ¥‡ Gold (>0.9):   {stats['quality_distribution']['gold']}")
    print(f"  ğŸ¥ˆ Silver (>0.7): {stats['quality_distribution']['silver']}")
    print(f"  ğŸ¥‰ Bronze (>0.5): {stats['quality_distribution']['bronze']}")
    print()
    
    if stats["rejection_reasons"]:
        print("æ‹’ç»åŸå› åˆ†å¸ƒ:")
        for reason, count in sorted(stats["rejection_reasons"].items(), key=lambda x: -x[1]):
            print(f"  - {reason}: {count}")
    
    print()
    print("=" * 60)
    
    return stats


def export_for_training(
    input_file: str,
    output_file: str,
    format_type: str = "alpaca"
) -> int:
    """
    å°†æ¸…æ´—åçš„æ•°æ®å¯¼å‡ºä¸ºç‰¹å®šè®­ç»ƒæ ¼å¼
    
    Args:
        input_file: æ¸…æ´—åçš„ JSONL æ–‡ä»¶
        output_file: è¾“å‡ºæ–‡ä»¶
        format_type: æ ¼å¼ç±»å‹ (alpaca, sharegpt, messages)
    
    Returns:
        å¯¼å‡ºçš„æ ·æœ¬æ•°é‡
    """
    samples = []
    
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            sample = json.loads(line)
            
            if format_type == "alpaca":
                # Alpaca æ ¼å¼ï¼ˆé€‚ç”¨äº LLaMA-Factory ç­‰ï¼‰
                formatted = {
                    "instruction": sample["instruction"],
                    "input": sample["input"],
                    "output": sample["output"]
                }
            
            elif format_type == "sharegpt":
                # ShareGPT æ ¼å¼
                formatted = {
                    "conversations": [
                        {"from": "system", "value": sample["instruction"]},
                        {"from": "human", "value": sample["input"]},
                        {"from": "gpt", "value": sample["output"]}
                    ]
                }
            
            elif format_type == "messages":
                # OpenAI messages æ ¼å¼
                formatted = {
                    "messages": [
                        {"role": "system", "content": sample["instruction"]},
                        {"role": "user", "content": sample["input"]},
                        {"role": "assistant", "content": sample["output"]}
                    ]
                }
            
            else:
                formatted = sample
            
            samples.append(formatted)
    
    # å†™å…¥
    with open(output_file, 'w', encoding='utf-8') as f:
        if output_file.endswith('.json'):
            json.dump(samples, f, ensure_ascii=False, indent=2)
        else:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"âœ… å·²å¯¼å‡º {len(samples)} æ¡æ ·æœ¬ä¸º {format_type} æ ¼å¼: {output_file}")
    return len(samples)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="SFT æ•°æ®æ¸…æ´—ä¸å¯¼å‡ºå·¥å…·")
    parser.add_argument("--input", "-i", default="evaluation/sft_data/eval_results.jsonl",
                       help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--min-score", "-s", type=float, default=0.7,
                       help="æœ€ä½è´¨é‡åˆ†æ•° (é»˜è®¤: 0.7)")
    parser.add_argument("--format", "-f", choices=["alpaca", "sharegpt", "messages"],
                       default="alpaca", help="å¯¼å‡ºæ ¼å¼ (é»˜è®¤: alpaca)")
    parser.add_argument("--export", "-e", action="store_true",
                       help="åŒæ—¶å¯¼å‡ºä¸ºè®­ç»ƒæ ¼å¼")
    
    args = parser.parse_args()
    
    # é…ç½®
    config = CleaningConfig()
    config.MIN_OVERALL_SCORE = args.min_score
    
    # æ¸…æ´—
    stats = clean_and_export(args.input, config)
    
    # å¯¼å‡ºä¸ºè®­ç»ƒæ ¼å¼
    if args.export and stats["passed"] > 0:
        # æ‰¾åˆ°æœ€æ–°çš„æ¸…æ´—æ–‡ä»¶
        output_dir = Path(config.OUTPUT_DIR)
        cleaned_files = sorted(output_dir.glob("sft_train_*.jsonl"), reverse=True)
        if cleaned_files:
            latest_file = cleaned_files[0]
            export_file = output_dir / f"train_{args.format}.jsonl"
            export_for_training(str(latest_file), str(export_file), args.format)
