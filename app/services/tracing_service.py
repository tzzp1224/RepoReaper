# æ–‡ä»¶è·¯å¾„: app/services/tracing_service.py
"""
Langfuseé›†æˆæ¨¡å— - ç”¨äºç«¯åˆ°ç«¯è¿½è¸ªå’Œè§‚æµ‹

æ ¸å¿ƒèƒ½åŠ›:
1. è‡ªåŠ¨æ•è·æ¯ä¸€æ­¥çš„å»¶è¿Ÿã€Tokenæˆæœ¬ã€è¾“å…¥è¾“å‡º
2. è®°å½•å®Œæ•´çš„è°ƒç”¨é“¾è·¯: Query -> Rewrite -> Retrieval -> Generation
3. è®°å½•Toolè°ƒç”¨å’Œå‚æ•°
4. é›†æˆåˆ°è¯„ä¼°æµç¨‹

Langfuseæ”¯æŒ:
- æœ¬åœ°éƒ¨ç½² (docker run ... langfuse)
- äº‘ç«¯æ‰˜ç®¡ (app.langfuse.com)

Author: Dexter
Date: 2025-01-27
"""

import time
import json
import os
from typing import Dict, Any, Optional, List, Callable
from functools import wraps
from datetime import datetime
from dataclasses import dataclass


# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†: Langfuseå®¢æˆ·ç«¯åˆå§‹åŒ– (å¯é€‰)
# ============================================================================

try:
    from langfuse import Langfuse
    from langfuse.decorators import observe, langfuse_context
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False


@dataclass
class TracingConfig:
    """è¿½è¸ªé…ç½®"""
    enabled: bool = True
    backend: str = "langfuse"  # "langfuse" or "local"
    langfuse_host: str = os.getenv("LANGFUSE_HOST", "http://localhost:3000")
    langfuse_public_key: str = os.getenv("LANGFUSE_PUBLIC_KEY", "")
    langfuse_secret_key: str = os.getenv("LANGFUSE_SECRET_KEY", "")
    capture_token_usage: bool = True
    capture_latency: bool = True
    local_log_dir: str = "logs/traces"


class TracingService:
    """
    ç»Ÿä¸€çš„è¿½è¸ªæœåŠ¡
    æ”¯æŒLangfuseå’Œæœ¬åœ°æ—¥å¿—ä¸¤ç§åç«¯
    """
    
    def __init__(self, config: TracingConfig = None):
        self.config = config or TracingConfig()
        self.langfuse_client = None
        self.current_trace_id = None
        
        if self.config.enabled and self.config.backend == "langfuse":
            if not LANGFUSE_AVAILABLE:
                print("âš ï¸ Langfuse not installed. Install with: pip install langfuse. Falling back to local logging.")
                self.config.backend = "local"
            else:
                try:
                    self.langfuse_client = Langfuse(
                        host=self.config.langfuse_host,
                        public_key=self.config.langfuse_public_key,
                        secret_key=self.config.langfuse_secret_key,
                        enabled=True,
                        debug=False
                    )
                    print("âœ… Langfuse client initialized successfully")
                except Exception as e:
                    print(f"âš ï¸ Langfuse initialization failed: {e}. Falling back to local logging.")
                    self.config.backend = "local"
        
        # åˆ›å»ºæœ¬åœ°æ—¥å¿—ç›®å½•
        os.makedirs(self.config.local_log_dir, exist_ok=True)
    
    def start_trace(self, trace_name: str, session_id: str, metadata: Dict = None) -> str:
        """å¯åŠ¨ä¸€ä¸ªæ–°çš„è¿½è¸ªé“¾"""
        import uuid
        trace_id = str(uuid.uuid4())
        self.current_trace_id = trace_id
        
        if self.langfuse_client:
            self.langfuse_client.trace(
                name=trace_name,
                input=metadata or {},
                session_id=session_id
            )
            print(f"ğŸ“ Trace started: {trace_id}")
        else:
            self._log_locally("trace_start", {
                "trace_id": trace_id,
                "name": trace_name,
                "session_id": session_id,
                "metadata": metadata,
                "timestamp": datetime.now().isoformat()
            })
        
        return trace_id
    
    def record_span(
        self,
        span_name: str,
        operation: str,
        input_data: Any,
        output_data: Any,
        latency_ms: float,
        token_usage: Dict[str, int] = None,
        metadata: Dict = None
    ) -> None:
        """è®°å½•ä¸€ä¸ªæ“ä½œçš„è·¨åº¦"""
        
        span_record = {
            "span_name": span_name,
            "operation": operation,
            "latency_ms": latency_ms,
            "timestamp": datetime.now().isoformat(),
            "token_usage": token_usage or {},
            "metadata": metadata or {}
        }
        
        if self.langfuse_client:
            try:
                # Langfuse:è®°å½•åˆ°äº‘ç«¯
                self.langfuse_client.span(
                    name=span_name,
                    input=input_data,
                    output=output_data,
                    metadata={
                        "operation": operation,
                        "latency_ms": latency_ms,
                        **(token_usage or {}),
                        **(metadata or {})
                    }
                )
            except Exception as e:
                print(f"âš ï¸ Failed to record span to Langfuse: {e}")
        
        # æœ¬åœ°æ—¥å¿—
        self._log_locally("span", span_record)
    
    def record_tool_call(
        self,
        tool_name: str,
        parameters: Dict,
        result: Any,
        latency_ms: float,
        success: bool,
        error: str = None
    ) -> None:
        """è®°å½•å·¥å…·è°ƒç”¨"""
        
        tool_record = {
            "tool_name": tool_name,
            "parameters": parameters,
            "result": str(result)[:500] if result else None,
            "latency_ms": latency_ms,
            "success": success,
            "error": error,
            "timestamp": datetime.now().isoformat()
        }
        
        if self.langfuse_client:
            try:
                self.langfuse_client.event(
                    name=f"tool_call:{tool_name}",
                    input=parameters,
                    output=result,
                    metadata={
                        "latency_ms": latency_ms,
                        "success": success,
                        "error": error
                    }
                )
            except Exception as e:
                print(f"âš ï¸ Failed to record tool call: {e}")
        
        self._log_locally("tool_call", tool_record)
    
    def record_retrieval_debug(
        self,
        query: str,
        retrieved_files: List[str],
        vector_scores: List[float],
        bm25_scores: List[float],
        latency_ms: float
    ) -> None:
        """è®°å½•æ£€ç´¢è¿‡ç¨‹çš„è°ƒè¯•ä¿¡æ¯"""
        
        retrieval_record = {
            "query": query,
            "retrieved_count": len(retrieved_files),
            "files": retrieved_files,
            "vector_scores": vector_scores,
            "bm25_scores": bm25_scores,
            "latency_ms": latency_ms,
            "timestamp": datetime.now().isoformat()
        }
        
        if self.langfuse_client:
            try:
                self.langfuse_client.event(
                    name="retrieval_debug",
                    input={"query": query},
                    output={"files": retrieved_files},
                    metadata=retrieval_record
                )
            except Exception as e:
                print(f"âš ï¸ Failed to record retrieval debug: {e}")
        
        self._log_locally("retrieval", retrieval_record)
    
    def record_llm_generation(
        self,
        model: str,
        prompt_messages: List[Dict],
        generated_text: str,
        ttft_ms: float = None,
        total_latency_ms: float = None,
        prompt_tokens: int = None,
        completion_tokens: int = None,
        total_tokens: int = None,
        is_streaming: bool = False,
        metadata: Dict = None
    ) -> None:
        """
        è®°å½• LLM ç”Ÿæˆçš„å®Œæ•´ä¿¡æ¯ï¼ŒåŒ…æ‹¬ Token æ¶ˆè€—å’Œ TTFT
        
        Args:
            model: æ¨¡å‹åç§° (å¦‚ "gpt-4", "claude-3")
            prompt_messages: å‘é€ç»™ LLM çš„æ¶ˆæ¯åˆ—è¡¨
            generated_text: ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆå¯æˆªæ–­ï¼‰
            ttft_ms: Time To First Tokenï¼Œé¦– token å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
            total_latency_ms: æ€»ç”Ÿæˆå»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
            prompt_tokens: è¾“å…¥ token æ•°
            completion_tokens: è¾“å‡º token æ•°
            total_tokens: æ€» token æ•°
            is_streaming: æ˜¯å¦æµå¼è¾“å‡º
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        llm_record = {
            "model": model,
            "is_streaming": is_streaming,
            "prompt_preview": str(prompt_messages)[:500],  # æˆªæ–­é¿å…æ—¥å¿—è¿‡å¤§
            "generated_preview": generated_text[:500] if generated_text else "",
            "generated_length": len(generated_text) if generated_text else 0,
            # Token ç»Ÿè®¡
            "token_usage": {
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens
            },
            # å»¶è¿Ÿç»Ÿè®¡
            "latency": {
                "ttft_ms": ttft_ms,  # Time To First Token
                "total_ms": total_latency_ms,
                "tokens_per_second": round(completion_tokens / (total_latency_ms / 1000), 2) 
                    if completion_tokens and total_latency_ms and total_latency_ms > 0 else None
            },
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        if self.langfuse_client:
            try:
                self.langfuse_client.generation(
                    name="llm_generation",
                    model=model,
                    input=prompt_messages,
                    output=generated_text[:1000] if generated_text else "",
                    usage={
                        "prompt_tokens": prompt_tokens or 0,
                        "completion_tokens": completion_tokens or 0,
                        "total_tokens": total_tokens or 0
                    },
                    metadata={
                        "ttft_ms": ttft_ms,
                        "total_latency_ms": total_latency_ms,
                        "is_streaming": is_streaming,
                        **(metadata or {})
                    }
                )
            except Exception as e:
                print(f"âš ï¸ Failed to record LLM generation to Langfuse: {e}")
        
        self._log_locally("llm_generation", llm_record)
    
    def record_ttft(self, ttft_ms: float, model: str = None, metadata: Dict = None) -> None:
        """
        å•ç‹¬è®°å½• TTFT (Time To First Token)
        ç”¨äºæµå¼ç”Ÿæˆæ—¶åœ¨æ”¶åˆ°ç¬¬ä¸€ä¸ª token æ—¶ç«‹å³è®°å½•
        
        Args:
            ttft_ms: é¦– token å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
            model: æ¨¡å‹åç§°
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        ttft_record = {
            "ttft_ms": ttft_ms,
            "model": model,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }
        
        if self.langfuse_client:
            try:
                self.langfuse_client.event(
                    name="ttft",
                    input={},
                    output={"ttft_ms": ttft_ms},
                    metadata=ttft_record
                )
            except Exception as e:
                print(f"âš ï¸ Failed to record TTFT: {e}")
        
        self._log_locally("ttft", ttft_record)

    def add_event(self, event_name: str, event_data: Dict[str, Any] = None) -> None:
        """
        æ·»åŠ äº‹ä»¶è®°å½•
        
        Args:
            event_name: äº‹ä»¶åç§° (å¦‚ "repo_map_generated", "file_read_failed" ç­‰)
            event_data: äº‹ä»¶ç›¸å…³æ•°æ®
        """
        event_record = {
            "event_name": event_name,
            "event_data": event_data or {},
            "timestamp": datetime.now().isoformat()
        }
        
        if self.langfuse_client:
            try:
                self.langfuse_client.event(
                    name=event_name,
                    input={},
                    output=event_data or {},
                    metadata=event_data or {}
                )
            except Exception as e:
                print(f"âš ï¸ Failed to record event '{event_name}': {e}")
        
        self._log_locally("event", event_record)
    
    def _log_locally(self, log_type: str, data: Dict) -> None:
        """æœ¬åœ°æ—¥å¿—è®°å½•"""
        log_file = os.path.join(
            self.config.local_log_dir,
            f"{log_type}_{datetime.now().strftime('%Y%m%d')}.jsonl"
        )
        
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(data, ensure_ascii=False, default=str) + '\n')
    
    def get_trace_url(self, trace_id: str = None) -> str:
        """è·å–Langfuseä¸­è¯¥traceçš„URL (ç”¨äºå‰ç«¯è·³è½¬)"""
        if not self.langfuse_client or not trace_id:
            return None
        
        # Langfuseäº‘ç«¯URLæ ¼å¼
        return f"{self.config.langfuse_host}/traces/{trace_id}"


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†: è£…é¥°å™¨ - è‡ªåŠ¨è¿½è¸ª
# ============================================================================

def traced(operation_name: str, capture_args: List[str] = None):
    """
    è£…é¥°å™¨: è‡ªåŠ¨ä¸ºè¢«è£…é¥°å‡½æ•°æ·»åŠ è¿½è¸ª
    
    ä½¿ç”¨ç¤ºä¾‹:
    @traced("query_rewrite", capture_args=["user_query"])
    async def rewrite_query(user_query: str):
        ...
    """
    
    def decorator(func: Callable):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            
            # æ•è·è¾“å…¥å‚æ•°
            input_data = {}
            if capture_args:
                for arg_name in capture_args:
                    if arg_name in kwargs:
                        input_data[arg_name] = kwargs[arg_name]
            
            try:
                result = await func(*args, **kwargs)
                latency_ms = (time.time() - start_time) * 1000
                
                # è®°å½•è·¨åº¦
                tracing_service.record_span(
                    span_name=operation_name,
                    operation=func.__name__,
                    input_data=input_data,
                    output_data={"success": True},
                    latency_ms=latency_ms
                )
                
                return result
            except Exception as e:
                latency_ms = (time.time() - start_time) * 1000
                tracing_service.record_span(
                    span_name=operation_name,
                    operation=func.__name__,
                    input_data=input_data,
                    output_data={"error": str(e)},
                    latency_ms=latency_ms,
                    metadata={"error": True}
                )
                raise
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            
            input_data = {}
            if capture_args:
                for arg_name in capture_args:
                    if arg_name in kwargs:
                        input_data[arg_name] = kwargs[arg_name]
            
            try:
                result = func(*args, **kwargs)
                latency_ms = (time.time() - start_time) * 1000
                
                tracing_service.record_span(
                    span_name=operation_name,
                    operation=func.__name__,
                    input_data=input_data,
                    output_data={"success": True},
                    latency_ms=latency_ms
                )
                
                return result
            except Exception as e:
                latency_ms = (time.time() - start_time) * 1000
                tracing_service.record_span(
                    span_name=operation_name,
                    operation=func.__name__,
                    input_data=input_data,
                    output_data={"error": str(e)},
                    latency_ms=latency_ms,
                    metadata={"error": True}
                )
                raise
        
        # åˆ¤æ–­æ˜¯asyncè¿˜æ˜¯sync
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†: å…¨å±€å®ä¾‹
# ============================================================================

tracing_config = TracingConfig(
    enabled=True,
    backend="langfuse" if LANGFUSE_AVAILABLE else "local"
)

tracing_service = TracingService(config=tracing_config)


# ============================================================================
# ç¬¬å››éƒ¨åˆ†: é›†æˆç¤ºä¾‹ (å¦‚ä½•åœ¨agent_service.pyä¸­ä½¿ç”¨)
# ============================================================================

"""
åœ¨ä½ çš„agent_service.pyä¸­æ·»åŠ :

1. å¯¼å…¥è¿½è¸ªæœåŠ¡:
   from app.services.tracing_service import tracing_service

2. åœ¨agent_streamå‡½æ•°å¼€å§‹:
   trace_id = tracing_service.start_trace(
       trace_name="github_agent_analysis",
       session_id=session_id,
       metadata={"repo_url": repo_url, "language": language}
   )

3. åœ¨generate_repo_mapå‡½æ•°å‘¨å›´:
   start_time = time.time()
   file_tree_str, mapped_files = await generate_repo_map(repo_url, file_list, limit=limit)
   latency_ms = (time.time() - start_time) * 1000
   
   tracing_service.record_span(
       span_name="generate_repo_map",
       operation="repo_mapping",
       input_data={"file_count": len(file_list), "limit": limit},
       output_data={"files_in_map": len(mapped_files)},
       latency_ms=latency_ms
   )

4. åœ¨process_single_fileä¸­è®°å½•æ£€ç´¢:
   tracing_service.record_retrieval_debug(
       query=search_query,
       retrieved_files=valid_files,
       vector_scores=vector_scores,
       bm25_scores=bm25_scores,
       latency_ms=search_latency
   )

5. å·¥å…·è°ƒç”¨è®°å½•:
   start_time = time.time()
   try:
       result = get_file_content(repo_url, file_path)
       tracing_service.record_tool_call(
           tool_name="get_file_content",
           parameters={"file_path": file_path},
           result=result[:100] if result else None,
           latency_ms=(time.time() - start_time) * 1000,
           success=True
       )
   except Exception as e:
       tracing_service.record_tool_call(
           tool_name="get_file_content",
           parameters={"file_path": file_path},
           result=None,
           latency_ms=(time.time() - start_time) * 1000,
           success=False,
           error=str(e)
       )
"""

import asyncio
