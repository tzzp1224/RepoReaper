# æ–‡ä»¶è·¯å¾„: app/services/chat_service.py
import json
import asyncio
import re
import time
from dataclasses import dataclass, field
from typing import Dict, Optional, AsyncGenerator, List, Set
from app.core.config import settings
from app.utils.llm_client import client
from app.services.vector_service import store_manager
from app.services.github_service import get_file_content
from app.services.chunking_service import UniversalChunker, ChunkingConfig
from app.services.tracing_service import tracing_service
from app.utils.session import get_conversation_memory, ConversationMemory


# ============================================================
# é…ç½®ç±» - è§£è€¦æ‰€æœ‰å¯è°ƒå‚æ•°
# ============================================================

@dataclass
class ChatConfig:
    """Chat æœåŠ¡é…ç½® - é›†ä¸­ç®¡ç†æ‰€æœ‰å‚æ•°"""
    # JIT åŠ¨æ€åŠ è½½é…ç½®
    max_jit_rounds: int = 2           # æœ€å¤§ JIT è½®æ•°
    max_files_per_round: int = 3      # æ¯è½®æœ€å¤šåŠ è½½æ–‡ä»¶æ•°
    
    # LLM é…ç½®
    temperature_thinking: float = 0.1  # æ€è€ƒé˜¶æ®µæ¸©åº¦
    temperature_final: float = 0.2     # æœ€ç»ˆå›ç­”æ¸©åº¦
    max_tokens: int = 4096             # æœ€å¤§ token æ•°
    
    # æ£€ç´¢é…ç½®
    retrieval_top_k: int = 6          # RAG æ£€ç´¢ top-k
    context_max_chars: int = 2000     # å•æ–‡æ¡£æœ€å¤§å­—ç¬¦æ•°
    
    # å¯¹è¯ä¸Šä¸‹æ–‡é…ç½®
    max_history_turns: int = 6        # ä¿ç•™æœ€è¿‘ N è½®å¯¹è¯
    summary_threshold: int = 10       # è¶…è¿‡ N è½®å¼€å§‹å‹ç¼©
    
    # è°ƒè¯•é…ç½®
    show_debug_info: bool = False     # æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯


# å…¨å±€é…ç½®å®ä¾‹
chat_config = ChatConfig()


@dataclass
class ChatResult:
    """èŠå¤©ç»“æœ - ç”¨äºåç»­è‡ªåŠ¨è¯„ä¼°"""
    answer: str                    # æœ€ç»ˆå›ç­”
    retrieved_context: str        # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    generation_latency_ms: float  # ç”Ÿæˆè€—æ—¶
    retrieval_latency_ms: float = 0  # æ£€ç´¢è€—æ—¶


# === è¯„ä¼°æ•°æ®å­˜å‚¨ (ä¾› main.py è·å–) ===
# å­˜å‚¨æ¯ä¸ª session çš„è¯„ä¼°æ•°æ®ï¼Œkey ä¸º session_id
_eval_data_store: Dict[str, ChatResult] = {}

def get_eval_data(session_id: str) -> Optional[ChatResult]:
    """è·å–æŒ‡å®š session çš„è¯„ä¼°æ•°æ®"""
    return _eval_data_store.get(session_id)

def clear_eval_data(session_id: str) -> None:
    """æ¸…é™¤æŒ‡å®š session çš„è¯„ä¼°æ•°æ®"""
    if session_id in _eval_data_store:
        del _eval_data_store[session_id]


# [Fix 2] ä½¿ç”¨ Config å¯¹è±¡åˆå§‹åŒ–ï¼Œè€Œéç›´æ¥ä¼ å‚
# ä¹‹å‰çš„å†™æ³•: chunker = UniversalChunker(min_chunk_size=100)
# ç°åœ¨çš„å†™æ³•:
chunker = UniversalChunker(config=ChunkingConfig(min_chunk_size=100))

# === æ–°å¢ï¼šç®€å•çš„ä¸­æ–‡æ£€æµ‹ ===
def is_chinese_query(text: str) -> bool:
    """æ£€æµ‹å­—ç¬¦ä¸²ä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
    for char in text:
        if '\u4e00' <= char <= '\u9fff':
            return True
    return False

# === ä¼˜åŒ– 2ï¼šæŸ¥è¯¢é‡å†™ (è§£å†³ä¸­è‹±æ–‡æ£€ç´¢ä¸åŒ¹é…é—®é¢˜) ===
async def _rewrite_query(user_query: str):
    """
    ä½¿ç”¨ LLM å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡ï¼‰è½¬æ¢ä¸º 3-5 ä¸ªä»£ç æœç´¢å…³é”®è¯ï¼ˆè‹±æ–‡ï¼‰ã€‚
    """
    prompt = f"""
    You are a Code Search Expert.
    Task: Convert the user's query into 3-5 English keywords for code search (BM25/Vector).
    
    User Query: "{user_query}"
    
    Rules:
    1. Output ONLY a JSON list of strings.
    2. Translate concepts to technical terms (e.g., "é‰´æƒ" -> "auth", "login", "middleware").
    3. Keep it short.
    
    Example Output: ["authentication", "login_handler", "jwt_verify"]
    """
    try:
        response = await client.chat.completions.create(
            model=settings.default_model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=100
        )
        content = response.choices[0].message.content
        # ç®€å•æ¸…æ´—
        content = re.sub(r"^```(json)?|```$", "", content.strip(), flags=re.MULTILINE).strip()
        keywords = json.loads(content)
        if isinstance(keywords, list):
            return " ".join(keywords) # è¿”å›ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²ä¾› BM25 ä½¿ç”¨
        return user_query
    except Exception as e:
        print(f"âš ï¸ Query Rewrite Failed: {e}")
        return user_query # é™çº§ï¼šç›´æ¥ç”¨åŸå¥

async def process_chat_stream(user_query: str, session_id: str):
    """
    å¤„ç†èŠå¤©æµ - æ”¯æŒå¤šè½® JIT åŠ¨æ€åŠ è½½æ–‡ä»¶ + å¯¹è¯ä¸Šä¸‹æ–‡è®°å¿†
    
    æµç¨‹:
    1. è·å–å¯¹è¯è®°å¿†ï¼Œæ„å»ºä¸Šä¸‹æ–‡
    2. åˆå§‹æ£€ç´¢ RAG ä¸Šä¸‹æ–‡
    3. LLM æ€è€ƒå¹¶å›ç­”ï¼Œå¯èƒ½è¯·æ±‚æ–‡ä»¶
    4. å¦‚æœè¯·æ±‚æ–‡ä»¶ï¼ŒåŠ è½½åç»§ç»­å¯¹è¯ (æœ€å¤š max_jit_rounds è½®)
    5. æœ€ç»ˆç”Ÿæˆç­”æ¡ˆå¹¶ä¿å­˜åˆ°å¯¹è¯è®°å¿†
    """
    vector_db = store_manager.get_store(session_id)
    cfg = chat_config  # ä½¿ç”¨å…¨å±€é…ç½®
    
    # === è·å–å¯¹è¯è®°å¿† ===
    memory = get_conversation_memory(session_id)
    memory.add_user_message(user_query)  # ç«‹å³è®°å½•ç”¨æˆ·æ¶ˆæ¯
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‘˜è¦å‹ç¼©
    if memory.needs_summarization():
        yield "> ğŸ“ *Compressing conversation history...*\n\n"
        await _compress_conversation_history(memory)
    
    # === è¯„ä¼°æ•°æ®æ”¶é›†å˜é‡ ===
    collected_context = ""
    collected_response = ""
    collected_retrieval_latency = 0.0
    collected_generation_latency = 0.0
    
    # === JIT çŠ¶æ€è·Ÿè¸ª ===
    all_loaded_files: Set[str] = set()      # æ‰€æœ‰å·²åŠ è½½çš„æ–‡ä»¶
    all_failed_files: Set[str] = set()      # æ‰€æœ‰å¤±è´¥çš„æ–‡ä»¶
    jit_round = 0                            # å½“å‰ JIT è½®æ•°
    
    # === è¯­è¨€ç¯å¢ƒæ£€æµ‹ ===
    use_chinese = is_chinese_query(user_query)
    
    # UI æç¤ºè¯­
    ui_msgs = _get_ui_messages(use_chinese)

    # === æ­¥éª¤ 0: æŸ¥è¯¢é‡å†™ ===
    search_query = await _rewrite_query(user_query)
    yield f"{ui_msgs['thinking']}`{search_query}`...\n\n"
    
    # === æ­¥éª¤ 1: åˆå§‹ RAG æ£€ç´¢ ===
    retrieval_start = time.time()
    relevant_docs = await vector_db.search_hybrid(search_query, top_k=cfg.retrieval_top_k)
    retrieval_latency_ms = (time.time() - retrieval_start) * 1000
    collected_retrieval_latency = retrieval_latency_ms
    tracing_service.add_event("retrieval_completed", {
        "latency_ms": retrieval_latency_ms,
        "documents_retrieved": len(relevant_docs) if relevant_docs else 0
    })
    
    rag_context = _build_context(relevant_docs, cfg.context_max_chars)
    collected_context = rag_context
    
    # === æ­¥éª¤ 2: æ„å»ºåˆå§‹ Prompt ===
    global_context = vector_db.global_context or {}
    file_tree = global_context.get("file_tree", "(File tree not available.)")
    agent_summary = global_context.get("summary", "")
    
    # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡
    conversation_context = _build_conversation_context(memory)
    
    system_instruction = _build_system_prompt(
        file_tree=file_tree,
        agent_summary=agent_summary,
        rag_context=rag_context,
        use_chinese=use_chinese,
        is_final_round=False,
        conversation_context=conversation_context
    )
    
    augmented_user_query = f"""
    {user_query}
    
    (System Note: Priority 1: Answer using context. Priority 2: Use <tool_code> ONLY if critical info is missing.)
    """
    
    if not client: 
        yield "âŒ LLM Error: Client not initialized"
        return

    # åˆå§‹åŒ–å¯¹è¯å†å²
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": augmented_user_query}
    ]

    try:
        generation_start = time.time()
        
        # === å¤šè½® JIT å¾ªç¯ ===
        while jit_round <= cfg.max_jit_rounds:
            is_final_round = (jit_round == cfg.max_jit_rounds)
            
            # å¦‚æœæ˜¯æœ€ç»ˆè½®ï¼Œæ›´æ–°ç³»ç»Ÿæç¤ºç¦ç”¨å·¥å…·
            if is_final_round and jit_round > 0:
                # æ›´æ–°ç³»ç»Ÿæ¶ˆæ¯ï¼Œå‘ŠçŸ¥è¿™æ˜¯æœ€åä¸€è½®
                messages[0] = {"role": "system", "content": _build_system_prompt(
                    file_tree=file_tree,
                    agent_summary=agent_summary,
                    rag_context=collected_context,
                    use_chinese=use_chinese,
                    is_final_round=True,
                    failed_files=list(all_failed_files)
                )}
            
            # LLM æµå¼ç”Ÿæˆ
            stream = await client.chat.completions.create(
                model=settings.default_model_name,
                messages=messages,
                stream=True,
                temperature=cfg.temperature_final if is_final_round else cfg.temperature_thinking,
                max_tokens=cfg.max_tokens
            )
            
            buffer = ""
            round_response = ""
            requested_files: Set[str] = set()
            
            async for chunk in stream:
                content = chunk.choices[0].delta.content or ""
                if not content:
                    continue
                
                buffer += content
                round_response += content
                collected_response += content
                
                # æ£€æµ‹ tool_code æ ‡ç­¾
                if "</tool_code>" in buffer:
                    matches = re.findall(r"<tool_code>\s*(.*?)\s*</tool_code>", buffer, re.DOTALL)
                    for f in matches:
                        clean_f = f.strip().replace("'", "").replace('"', "").replace("`", "")
                        # è¿‡æ»¤å·²åŠ è½½å’Œå·²å¤±è´¥çš„æ–‡ä»¶
                        if clean_f and clean_f not in all_loaded_files and clean_f not in all_failed_files:
                            requested_files.add(clean_f)
                    yield content
                    buffer = ""
                else:
                    yield content
            
            # å¤„ç†ç¼“å†²åŒºæ®‹ç•™
            if "</tool_code>" in buffer:
                matches = re.findall(r"<tool_code>\s*(.*?)\s*</tool_code>", buffer, re.DOTALL)
                for f in matches:
                    clean_f = f.strip().replace("'", "").replace('"', "").replace("`", "")
                    if clean_f and clean_f not in all_loaded_files and clean_f not in all_failed_files:
                        requested_files.add(clean_f)
            
            # === åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­ JIT ===
            if not requested_files or is_final_round:
                # æ²¡æœ‰æ–°æ–‡ä»¶è¯·æ±‚ï¼Œæˆ–å·²è¾¾æœ€å¤§è½®æ•°ï¼Œç»“æŸå¾ªç¯
                break
            
            # === JIT æ–‡ä»¶åŠ è½½ ===
            jit_round += 1
            
            # é™åˆ¶æ¯è½®æ–‡ä»¶æ•°
            files_to_load = list(requested_files)[:cfg.max_files_per_round]
            file_list_str = ", ".join([f"`{f}`" for f in files_to_load])
            
            yield f"\n\n> ğŸ” **[JIT Round {jit_round}/{cfg.max_jit_rounds}]** {ui_msgs['action_short']}{file_list_str}...\n\n"
            
            if not vector_db.repo_url:
                yield ui_msgs['error_url']
                break
            
            # åŠ è½½æ–‡ä»¶
            round_loaded_docs = []
            round_failed_files = []
            
            for file_path in files_to_load:
                if file_path in vector_db.indexed_files:
                    docs = vector_db.get_documents_by_file(file_path)
                    round_loaded_docs.extend(docs)
                    all_loaded_files.add(file_path)
                    yield f"> âœ… Loaded: `{file_path}`\n"
                else:
                    success = await _download_and_index(vector_db, file_path)
                    if success:
                        docs = vector_db.get_documents_by_file(file_path)
                        round_loaded_docs.extend(docs)
                        all_loaded_files.add(file_path)
                        yield f"> âœ… Downloaded: `{file_path}`\n"
                    else:
                        round_failed_files.append(file_path)
                        all_failed_files.add(file_path)
                        yield f"> âš ï¸ Failed: `{file_path}`\n"
            
            # æ„å»ºåç»­æ¶ˆæ¯
            if round_loaded_docs:
                new_context = _build_context(round_loaded_docs, cfg.context_max_chars)
                collected_context += f"\n\n[JIT Round {jit_round} Context]\n{new_context}"
            
            # æ„å»ºçŠ¶æ€æ¶ˆæ¯
            status_msg = _build_jit_status_message(
                loaded_count=len(round_loaded_docs),
                failed_files=round_failed_files,
                remaining_rounds=cfg.max_jit_rounds - jit_round,
                use_chinese=use_chinese
            )
            
            context_section = f"\n\n[New Code Context]\n{_build_context(round_loaded_docs, cfg.context_max_chars)}" if round_loaded_docs else ""
            
            # æ›´æ–°å¯¹è¯å†å²ï¼Œç»§ç»­å¯¹è¯
            messages.append({"role": "assistant", "content": round_response})
            messages.append({"role": "user", "content": f"{status_msg}{context_section}\n\nPlease continue your analysis."})
            
            yield "\n\n"  # åˆ†éš”ç¬¦
        
        # === ç”Ÿæˆå®Œæˆ ===
        generation_latency_ms = (time.time() - generation_start) * 1000
        collected_generation_latency = generation_latency_ms
        
        tracing_service.add_event("generation_completed", {
            "latency_ms": generation_latency_ms,
            "jit_rounds": jit_round,
            "files_loaded": len(all_loaded_files),
            "files_failed": len(all_failed_files)
        })
        
        # === ä¿å­˜åŠ©æ‰‹å›å¤åˆ°å¯¹è¯è®°å¿† ===
        memory.add_assistant_message(collected_response)
        
        # å­˜å‚¨è¯„ä¼°æ•°æ®
        _eval_data_store[session_id] = ChatResult(
            answer=collected_response,
            retrieved_context=collected_context,
            generation_latency_ms=collected_generation_latency,
            retrieval_latency_ms=collected_retrieval_latency
        )
        print(f"ğŸ“¦ [EvalData] Session {session_id}: {len(collected_context)} chars context, {len(collected_response)} chars answer, {jit_round} JIT rounds, {memory.get_turn_count()} turns")

    except Exception as e:
        import traceback
        traceback.print_exc()
        error_msg = str(e)
        # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜éƒ¨åˆ†å›å¤
        if collected_response:
            memory.add_assistant_message(collected_response + f"\n\n[Error: {error_msg}]")
        tracing_service.add_event("generation_error", {
            "error": error_msg,
            "error_type": type(e).__name__,
            "jit_round": jit_round
        })
        yield f"\n\nâŒ System Error: {error_msg}"


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================

def _get_ui_messages(use_chinese: bool) -> Dict[str, str]:
    """è·å– UI æ¶ˆæ¯ï¼ˆæ ¹æ®è¯­è¨€ï¼‰"""
    if use_chinese:
        return {
            "thinking": "> ğŸ§  **æ€è€ƒä¸­:** æ­£åœ¨æ£€ç´¢ç›¸å…³ä»£ç : ",
            "action_short": "æ­£åœ¨è¯»å–æ–‡ä»¶: ",
            "error_url": "> âš ï¸ é”™è¯¯: ä»“åº“é“¾æ¥ä¸¢å¤±ã€‚\n",
        }
    else:
        return {
            "thinking": "> ğŸ§  **Thinking:** Searching for code related to: ",
            "action_short": "Retrieving files: ",
            "error_url": "> âš ï¸ Error: Repository URL lost.\n",
        }


def _build_system_prompt(
    file_tree: str,
    agent_summary: str,
    rag_context: str,
    use_chinese: bool,
    is_final_round: bool,
    failed_files: List[str] = None,
    conversation_context: str = ""
) -> str:
    """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
    lang_instruction = (
        "IMPORTANT: The user is asking in Chinese. You MUST reply in Simplified Chinese (ç®€ä½“ä¸­æ–‡)."
        if use_chinese else "Reply in English."
    )
    
    if is_final_round:
        tool_instruction = """
    [INSTRUCTIONS - FINAL ROUND]
    This is your FINAL response. You MUST provide a complete answer NOW.
    - DO NOT request any more files
    - DO NOT use <tool_code> tags
    - Synthesize all available context and give your best answer
    - If some files were not accessible, explain what information is missing and provide the best possible answer with what you have
    """
        if failed_files:
            tool_instruction += f"\n    Note: The following files could not be accessed: {', '.join(failed_files)}"
    else:
        tool_instruction = """
    [INSTRUCTIONS]
    1. **CHECK CONTEXT FIRST**: Look at the [Current Code Context]. Does it contain the answer?
    2. **IF YES**: Answer directly. DO NOT use tools.
    3. **IF NO**: Request missing files using tags: <tool_code>path/to/file</tool_code>
    """
    
    # æ·»åŠ å¯¹è¯å†å²ä¸Šä¸‹æ–‡
    conversation_section = ""
    if conversation_context:
        conversation_section = f"""
    [Previous Conversation]
    {conversation_context}
    """
    
    return f"""
    You are a Senior GitHub Repository Analyst.
    {lang_instruction}
    
    [Global Context - Repo Map]
    {file_tree}
    
    [Agent Analysis Summary]
    {agent_summary}
    {conversation_section}
    [Current Code Context (Retrieved)]
    {rag_context}
    {tool_instruction}
    """


def _build_conversation_context(memory: ConversationMemory) -> str:
    """
    æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    
    åªåŒ…å«æœ€è¿‘å‡ è½®å¯¹è¯çš„æ‘˜è¦ï¼Œç”¨äº system prompt
    """
    messages = memory.get_context_messages()
    
    if len(messages) <= 2:
        # åªæœ‰å½“å‰è½®ï¼Œä¸éœ€è¦å†å²
        return ""
    
    # æ’é™¤æœ€åä¸€æ¡ï¼ˆå½“å‰ç”¨æˆ·æ¶ˆæ¯ï¼‰
    history_messages = messages[:-1]
    
    if not history_messages:
        return ""
    
    context_parts = []
    for msg in history_messages[-6:]:  # æœ€å¤š 6 æ¡ï¼ˆ3 è½®ï¼‰
        role = "User" if msg["role"] == "user" else "Assistant"
        # æˆªæ–­è¿‡é•¿çš„å†…å®¹
        content = msg["content"][:500]
        if len(msg["content"]) > 500:
            content += "..."
        context_parts.append(f"{role}: {content}")
    
    return "\n".join(context_parts)


async def _compress_conversation_history(memory: ConversationMemory) -> None:
    """
    å‹ç¼©å¯¹è¯å†å² - ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
    """
    messages_to_summarize = memory.get_messages_to_summarize()
    
    if not messages_to_summarize:
        return
    
    # æ„å»ºæ‘˜è¦è¯·æ±‚
    conversation_text = "\n".join([
        f"{'User' if m['role'] == 'user' else 'Assistant'}: {m['content'][:300]}"
        for m in messages_to_summarize
    ])
    
    prompt = f"""Summarize the following conversation in 2-3 sentences, focusing on:
1. What questions were asked
2. Key information discovered
3. Important conclusions

Conversation:
{conversation_text}

Summary (be concise):"""

    try:
        response = await client.chat.completions.create(
            model=settings.default_model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=200
        )
        summary = response.choices[0].message.content.strip()
        
        # ä¿å­˜æ‘˜è¦
        end_idx = len(memory._messages) - chat_config.max_history_turns * 2
        memory.set_summary(summary, end_idx)
        
        print(f"ğŸ“ Conversation compressed: {len(messages_to_summarize)} messages -> summary")
    except Exception as e:
        print(f"âš ï¸ Failed to compress conversation: {e}")


def _build_jit_status_message(
    loaded_count: int,
    failed_files: List[str],
    remaining_rounds: int,
    use_chinese: bool
) -> str:
    """æ„å»º JIT çŠ¶æ€æ¶ˆæ¯"""
    if use_chinese:
        if loaded_count > 0 and not failed_files:
            return f"ç³»ç»Ÿé€šçŸ¥: æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæ–‡ä»¶ã€‚"
        elif loaded_count > 0 and failed_files:
            failed_list = ", ".join(failed_files)
            return f"ç³»ç»Ÿé€šçŸ¥: åŠ è½½äº† {loaded_count} ä¸ªæ–‡ä»¶ï¼Œä½†ä»¥ä¸‹æ–‡ä»¶æ— æ³•è®¿é—®: {failed_list}ã€‚"
        else:
            failed_list = ", ".join(failed_files)
            if remaining_rounds > 0:
                return f"ç³»ç»Ÿé€šçŸ¥: æ–‡ä»¶ ({failed_list}) æ— æ³•è®¿é—®ã€‚ä½ è¿˜æœ‰ {remaining_rounds} æ¬¡æœºä¼šè¯·æ±‚å…¶ä»–æ–‡ä»¶ï¼Œæˆ–è€…åŸºäºç°æœ‰ä¸Šä¸‹æ–‡å›ç­”ã€‚"
            else:
                return f"ç³»ç»Ÿé€šçŸ¥: æ–‡ä»¶ ({failed_list}) æ— æ³•è®¿é—®ã€‚è¯·åŸºäºç°æœ‰ä¸Šä¸‹æ–‡ç»™å‡ºæœ€ä½³å›ç­”ã€‚"
    else:
        if loaded_count > 0 and not failed_files:
            return f"System Notification: Successfully loaded {loaded_count} files."
        elif loaded_count > 0 and failed_files:
            failed_list = ", ".join(failed_files)
            return f"System Notification: Loaded {loaded_count} files, but the following could not be accessed: {failed_list}."
        else:
            failed_list = ", ".join(failed_files)
            if remaining_rounds > 0:
                return f"System Notification: Files ({failed_list}) could not be accessed. You have {remaining_rounds} more attempts to request other files, or answer based on available context."
            else:
                return f"System Notification: Files ({failed_list}) could not be accessed. Please provide the best possible answer based on existing context."

async def _download_and_index(vector_db, file_path):
    """ä¸‹è½½å¹¶ç´¢å¼•æ–‡ä»¶"""
    try:
        content = await get_file_content(vector_db.repo_url, file_path)
        if not content: return False
        
        chunks = await asyncio.to_thread(chunker.chunk_file, content, file_path)
        if not chunks: 
            chunks = [{
                "content": content,
                "metadata": {"file": file_path, "type": "text", "name": "root", "class": ""}
            }]
            
        documents = [c["content"] for c in chunks]
        metadatas = []
        for c in chunks:
            meta = c["metadata"]
            metadatas.append({
                "file": meta["file"],
                "type": meta["type"],
                "name": meta.get("name", ""),
                "class": meta.get("class") or ""
            })
        await vector_db.add_documents(documents, metadatas)
        return True
    except Exception as e:
        print(f"Download Error: {e}")
        return False


def _build_context(docs: List[Dict], max_chars: int = 2000) -> str:
    """æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
    if not docs:
        return "(No relevant code snippets found yet)"
    
    context = ""
    for doc in docs:
        file_info = doc.get('file', 'unknown')
        metadata = doc.get('metadata', {})
        
        if 'class' in metadata and metadata['class']:
            file_info += f" (Class: {metadata['class']})"
        
        content = doc.get('content', '')[:max_chars]
        context += f"\n--- File: {file_info} ---\n{content}\n"
    
    return context