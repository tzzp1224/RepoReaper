# æ–‡ä»¶è·¯å¾„: app/services/chat_service.py
import json
import asyncio
import re
import time
from dataclasses import dataclass
from typing import Dict, Optional, AsyncGenerator
from app.core.config import settings
from app.utils.llm_client import client
from app.services.vector_service import store_manager
from app.services.github_service import get_file_content
from app.services.chunking_service import UniversalChunker, ChunkingConfig
from app.services.tracing_service import tracing_service


@dataclass
class ChatResult:
    """èŠå¤©ç»“æœ - ç”¨äºåç»­è‡ªåŠ¨è¯„ä¼°"""
    answer: str                    # æœ€ç»ˆå›ç­”
    retrieved_context: str        # æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    generation_latency_ms: float  # ç”Ÿæˆè€—æ—¶
    retrieval_latency_ms: float = 0  # æ£€ç´¢è€—æ—¶


# === è¯„ä¼°æ•°æ®å­˜å‚¨ (ä¾› main.py è·å–) ===
# å­˜å‚¨æ¯ä¸ª session çš„è¯„ä¼°æ•°æ®ï¼Œkey ä¸º session_id
_eval_data_store: Dict[str, ChatResult] = {}

def get_eval_data(session_id: str) -> Optional[ChatResult]:
    """è·å–æŒ‡å®š session çš„è¯„ä¼°æ•°æ®"""
    return _eval_data_store.get(session_id)

def clear_eval_data(session_id: str) -> None:
    """æ¸…é™¤æŒ‡å®š session çš„è¯„ä¼°æ•°æ®"""
    if session_id in _eval_data_store:
        del _eval_data_store[session_id]


# [Fix 2] ä½¿ç”¨ Config å¯¹è±¡åˆå§‹åŒ–ï¼Œè€Œéç›´æ¥ä¼ å‚
# ä¹‹å‰çš„å†™æ³•: chunker = UniversalChunker(min_chunk_size=100)
# ç°åœ¨çš„å†™æ³•:
chunker = UniversalChunker(config=ChunkingConfig(min_chunk_size=100))

# === æ–°å¢ï¼šç®€å•çš„ä¸­æ–‡æ£€æµ‹ ===
def is_chinese_query(text: str) -> bool:
    """æ£€æµ‹å­—ç¬¦ä¸²ä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
    for char in text:
        if '\u4e00' <= char <= '\u9fff':
            return True
    return False

# === ä¼˜åŒ– 2ï¼šæŸ¥è¯¢é‡å†™ (è§£å†³ä¸­è‹±æ–‡æ£€ç´¢ä¸åŒ¹é…é—®é¢˜) ===
async def _rewrite_query(user_query: str):
    """
    ä½¿ç”¨ LLM å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€ï¼ˆå¯èƒ½æ˜¯ä¸­æ–‡ï¼‰è½¬æ¢ä¸º 3-5 ä¸ªä»£ç æœç´¢å…³é”®è¯ï¼ˆè‹±æ–‡ï¼‰ã€‚
    """
    prompt = f"""
    You are a Code Search Expert.
    Task: Convert the user's query into 3-5 English keywords for code search (BM25/Vector).
    
    User Query: "{user_query}"
    
    Rules:
    1. Output ONLY a JSON list of strings.
    2. Translate concepts to technical terms (e.g., "é‰´æƒ" -> "auth", "login", "middleware").
    3. Keep it short.
    
    Example Output: ["authentication", "login_handler", "jwt_verify"]
    """
    try:
        response = await client.chat.completions.create(
            model=settings.default_model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=100
        )
        content = response.choices[0].message.content
        # ç®€å•æ¸…æ´—
        content = re.sub(r"^```(json)?|```$", "", content.strip(), flags=re.MULTILINE).strip()
        keywords = json.loads(content)
        if isinstance(keywords, list):
            return " ".join(keywords) # è¿”å›ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²ä¾› BM25 ä½¿ç”¨
        return user_query
    except Exception as e:
        print(f"âš ï¸ Query Rewrite Failed: {e}")
        return user_query # é™çº§ï¼šç›´æ¥ç”¨åŸå¥

async def process_chat_stream(user_query: str, session_id: str):
    vector_db = store_manager.get_store(session_id)
    
    # === è¯„ä¼°æ•°æ®æ”¶é›†å˜é‡ ===
    collected_context = ""  # æ”¶é›†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    collected_response = ""  # æ”¶é›†å®Œæ•´å“åº”
    collected_retrieval_latency = 0.0
    collected_generation_latency = 0.0
    
    # === 1. è¯­è¨€ç¯å¢ƒæ£€æµ‹ ===
    use_chinese = is_chinese_query(user_query)
    
    # å®šä¹‰ UI æç¤ºè¯­ (æ ¹æ®è¯­è¨€åˆ‡æ¢)
    ui_msgs = {
        "thinking": f"> ğŸ§  **Thinking:** Searching for code related to: " if not use_chinese else f"> ğŸ§  **æ€è€ƒä¸­:** æ­£åœ¨æ£€ç´¢ç›¸å…³ä»£ç : ",
        "action": f"\n\n> ğŸ” **Agent Action:** Retrieving missing files: " if not use_chinese else f"\n\n> ğŸ” **Agent åŠ¨ä½œ:** æ­£åœ¨è¯»å–ç¼ºå¤±æ–‡ä»¶: ",
        "error_url": f"> âš ï¸ Error: Repository URL lost.\n" if not use_chinese else f"> âš ï¸ é”™è¯¯: ä»“åº“é“¾æ¥ä¸¢å¤±ã€‚\n",
        "warning_file": f"> âš ï¸ Warning: Failed to access " if not use_chinese else f"> âš ï¸ è­¦å‘Š: æ— æ³•è¯»å– ",
        "system_note": "Please provide the FINAL answer." if not use_chinese else "System Notification: Files loaded. Please provide the FINAL answer in Chinese."
    }

    # === æ­¥éª¤ 0: æŸ¥è¯¢é‡å†™ (å¢å¼ºæ£€ç´¢å‘½ä¸­ç‡) ===
    # æ¯”å¦‚ç”¨æˆ·é—® "é‰´æƒåœ¨å“ªé‡Œï¼Ÿ" -> rewrite -> "auth login verify"
    search_query = await _rewrite_query(user_query)
    # å¯ä»¥åœ¨è¿™é‡Œ yield ä¸€ä¸ª debug ä¿¡æ¯ç»™å‰ç«¯ï¼Œå¦‚æœä¸æƒ³è¦å¯ä»¥æ³¨é‡Šæ‰
    yield f"{ui_msgs['thinking']}`{search_query}`...\n\n"
    
    # === 1. æ£€ç´¢ RAG (ä½¿ç”¨é‡å†™åçš„ Query) å¹¶è®¡æ—¶ ===
    retrieval_start = time.time()
    relevant_docs = await vector_db.search_hybrid(search_query, top_k=6)
    retrieval_latency_ms = (time.time() - retrieval_start) * 1000
    collected_retrieval_latency = retrieval_latency_ms  # ä¿å­˜æ£€ç´¢è€—æ—¶
    tracing_service.add_event("retrieval_completed", {
        "latency_ms": retrieval_latency_ms,
        "documents_retrieved": len(relevant_docs) if relevant_docs else 0
    })
    
    rag_context = _build_context(relevant_docs)
    collected_context = rag_context  # ä¿å­˜æ£€ç´¢ä¸Šä¸‹æ–‡
    
    # 2. è·å–å…¨å±€ä¸Šä¸‹æ–‡
    global_context = vector_db.global_context or {}
    file_tree = global_context.get("file_tree", "(File tree not available.)")
    agent_summary = global_context.get("summary", "") 
    
    # 3. æ„é€  Prompt (Context Priority)
    lang_instruction = "IMPORTANT: The user is asking in Chinese. You MUST reply in Simplified Chinese (ç®€ä½“ä¸­æ–‡)." if use_chinese else "Reply in English."
    system_instruction = f"""
    You are a Senior GitHub Repository Analyst.
    {lang_instruction}
    
    [Global Context - Repo Map]
    {file_tree}
    
    [Agent Analysis Summary]
    {agent_summary}
    
    [Current Code Context (Retrieved)]
    {rag_context}
    
    [INSTRUCTIONS]
    1. **CHECK CONTEXT FIRST**: Look at the [Current Code Context]. Does it contain the answer?
    2. **IF YES**: Answer directly. DO NOT use tools.
    3. **IF NO**: Request missing files using tags.
    
    [Tool Usage]
    Format: <tool_code>path/to/file</tool_code>
    """
    
    augmented_user_query = f"""
    {user_query}
    
    (System Note: Priority 1: Answer using context. Priority 2: Use <tool_code> ONLY if critical info is missing.)
    """
    
    if not client: 
        yield "âŒ LLM Error: Client not initialized"
        return

    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": augmented_user_query}
    ]

    try:
        # === Phase 1: æ€è€ƒä¸å›ç­” ===
        generation_start = time.time()
        stream = await client.chat.completions.create(
            model=settings.default_model_name,
            messages=messages,
            stream=True,
            temperature=0.1, 
            max_tokens=4096
        )
        
        buffer = ""
        full_response = ""
        requested_files = set()
        
        async for chunk in stream:
            content = chunk.choices[0].delta.content or ""
            if not content: continue
            
            buffer += content
            full_response += content
            collected_response += content  # æ”¶é›†å®Œæ•´å“åº”
            
            # æ£€æµ‹æ ‡ç­¾
            if "</tool_code>" in buffer:
                matches = re.findall(r"<tool_code>\s*(.*?)\s*</tool_code>", buffer, re.DOTALL)
                for f in matches:
                    clean_f = f.strip().replace("'", "").replace('"', "").replace("`", "")
                    requested_files.add(clean_f)
                
                yield content
                buffer = "" 
            else:
                yield content

        if "</tool_code>" in buffer:
            matches = re.findall(r"<tool_code>\s*(.*?)\s*</tool_code>", buffer, re.DOTALL)
            for f in matches:
                clean_f = f.strip().replace("'", "").replace('"', "").replace("`", "")
                requested_files.add(clean_f)

        # === Phase 2: æŒ‰éœ€ä¸‹è½½ ===
        if requested_files:
            file_list_str = ", ".join([f"`{f}`" for f in requested_files])
            yield f"\n\n> ğŸ” **Agent Action:** Retrieving missing files: {file_list_str}...\n\n"
            
            if not vector_db.repo_url:
                yield f"> âš ï¸ Error: Repository URL lost.\n"
                return

            new_docs_accumulated = []
            for file_path in requested_files:
                if file_path in vector_db.indexed_files:
                    docs = vector_db.get_documents_by_file(file_path)
                    new_docs_accumulated.extend(docs)
                else:
                    success = await _download_and_index(vector_db, file_path)
                    if success:
                        docs = vector_db.get_documents_by_file(file_path)
                        new_docs_accumulated.extend(docs)
                    else:
                        yield f"> âš ï¸ Warning: Failed to access `{file_path}`.\n"

            # === Phase 3: æœ€ç»ˆå›ç­” ===
            if new_docs_accumulated:
                supplementary_context = _build_context(new_docs_accumulated)
                
                final_messages = [
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": augmented_user_query},
                    {"role": "assistant", "content": full_response},
                    {"role": "user", "content": f"System Notification: Requested files loaded.\n\n[New Code Context]\n{supplementary_context}\n\nPlease provide the FINAL answer."}
                ]
                
                stream_final = await client.chat.completions.create(
                    model=settings.default_model_name,
                    messages=final_messages,
                    stream=True,
                    temperature=0.2
                )
                
                async for chunk in stream_final:
                    content = chunk.choices[0].delta.content or ""
                    if content:
                        collected_response += content  # æ”¶é›†æœ€ç»ˆå›ç­”
                        yield content
        
        generation_latency_ms = (time.time() - generation_start) * 1000
        collected_generation_latency = generation_latency_ms
        tracing_service.add_event("generation_completed", {
            "latency_ms": generation_latency_ms,
            "token_count": len(full_response.split())
        })
        
        # === å­˜å‚¨è¯„ä¼°æ•°æ®ä¾› main.py è·å– ===
        _eval_data_store[session_id] = ChatResult(
            answer=collected_response,
            retrieved_context=collected_context,
            generation_latency_ms=collected_generation_latency,
            retrieval_latency_ms=collected_retrieval_latency
        )
        print(f"ğŸ“¦ [EvalData] Stored for session {session_id}: context={len(collected_context)} chars, answer={len(collected_response)} chars")

    except Exception as e:
        import traceback
        traceback.print_exc()
        error_msg = str(e)
        tracing_service.add_event("generation_error", {
            "error": error_msg,
            "error_type": type(e).__name__
        })
        yield f"âŒ System Error: {error_msg}"

# è¾…åŠ©å‡½æ•°ä¿æŒä¸å˜
def _build_context(docs):
    if not docs: return "(No relevant code snippets found yet)"
    context = ""
    for doc in docs:
        file_info = doc['file']
        if 'class' in doc.get('metadata', {}):
            cls = doc['metadata']['class']
            if cls: file_info += f" (Class: {cls})"
        context += f"\n--- File: {file_info} ---\n{doc['content'][:2000]}\n"
    return context

async def _download_and_index(vector_db, file_path):
    try:
        content = await get_file_content(vector_db.repo_url, file_path)
        if not content: return False
        
        chunks = await asyncio.to_thread(chunker.chunk_file, content, file_path)
        if not chunks: 
            chunks = [{
                "content": content,
                "metadata": {"file": file_path, "type": "text", "name": "root", "class": ""}
            }]
            
        documents = [c["content"] for c in chunks]
        metadatas = []
        for c in chunks:
            meta = c["metadata"]
            metadatas.append({
                "file": meta["file"],
                "type": meta["type"],
                "name": meta.get("name", ""),
                "class": meta.get("class") or ""
            })
        await vector_db.add_documents(documents, metadatas)
        return True
    except Exception as e:
        print(f"Download Error: {e}")
        return False