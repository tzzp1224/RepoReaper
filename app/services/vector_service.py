# -*- coding: utf-8 -*-
"""
å‘é‡æœåŠ¡å±‚ - Qdrant ç‰ˆ

ç‰¹æ€§:
1. æ··åˆæœç´¢ - Qdrant å‘é‡ + BM25 å…³é”®è¯ï¼ŒRRF èåˆ
2. å¼‚æ­¥åŸç”Ÿ - å…¨é“¾è·¯å¼‚æ­¥
3. ä¼šè¯éš”ç¦» - æ¯ä¸ª session ç‹¬ç«‹é›†åˆ
4. çŠ¶æ€æŒä¹…åŒ– - ä»“åº“ä¿¡æ¯ã€BM25 ç´¢å¼•ç¼“å­˜
"""

import asyncio
import json
import logging
import os
import pickle
import re
import tempfile
import time
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Set

from rank_bm25 import BM25Okapi

from app.core.config import settings
from app.storage.base import Document, SearchResult, CollectionStats
from app.storage.qdrant_store import QdrantVectorStore, QdrantConfig, get_qdrant_factory
from app.utils.embedding import get_embedding_service, EmbeddingConfig

logger = logging.getLogger(__name__)


# ============================================================
# ä½¿ç”¨ç»Ÿä¸€é…ç½®
# ============================================================

from app.core.config import vector_config as config

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(config.context_dir, exist_ok=True)

# === å‘åå…¼å®¹å¯¼å‡º (ä¾› main.py ä½¿ç”¨) ===
vector_config = config  # å…¼å®¹æ—§åç§°
CONTEXT_DIR = config.context_dir
QDRANT_DIR = config.data_dir  # Qdrant æ•°æ®ç›®å½•


# ============================================================
# Embedding æœåŠ¡
# ============================================================

_embedding_service = None

def get_embedding():
    """è·å– Embedding æœåŠ¡å•ä¾‹"""
    global _embedding_service
    if _embedding_service is None:
        emb_config = EmbeddingConfig(
            api_base_url=config.embedding_api_url,
            model_name=config.embedding_model,
            batch_size=config.embedding_batch_size,
            max_text_length=config.embedding_max_length,
            max_concurrent_batches=config.embedding_concurrency,
        )
        _embedding_service = get_embedding_service(emb_config)
    return _embedding_service


# ============================================================
# å‘é‡å­˜å‚¨æœåŠ¡
# ============================================================

class VectorStore:
    """
    å‘é‡å­˜å‚¨æœåŠ¡
    
    æ•´åˆ Qdrant å‘é‡æœç´¢å’Œ BM25 å…³é”®è¯æœç´¢
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    store = VectorStore("session_123")
    await store.initialize()
    
    # é‡ç½® (åˆ†ææ–°ä»“åº“æ—¶)
    await store.reset()
    
    # æ·»åŠ æ–‡æ¡£
    await store.add_documents(documents, metadatas)
    
    # æ··åˆæœç´¢
    results = await store.search_hybrid("how does auth work?")
    
    await store.close()
    ```
    """
    
    def __init__(self, session_id: str):
        self.session_id = self._sanitize_id(session_id)
        self.collection_name = f"repo_{self.session_id}"
        
        # Qdrant å­˜å‚¨
        self._qdrant: Optional[QdrantVectorStore] = None
        
        # BM25 ç´¢å¼• (å†…å­˜)
        self._bm25: Optional[BM25Okapi] = None
        self._doc_store: List[Document] = []
        self._indexed_files: Set[str] = set()
        
        # ä¸Šä¸‹æ–‡
        self.repo_url: Optional[str] = None
        self.global_context: Dict[str, Any] = {}
        
        # æ–‡ä»¶è·¯å¾„
        self._context_file = os.path.join(config.context_dir, f"{self.session_id}.json")
        self._cache_file = os.path.join(config.context_dir, f"{self.session_id}_bm25.pkl")
        
        self._initialized = False
    
    @staticmethod
    def _sanitize_id(session_id: str) -> str:
        """æ¸…ç† session ID"""
        clean = re.sub(r'[^a-zA-Z0-9_-]', '', session_id)
        if not clean:
            raise ValueError("Invalid session_id")
        return clean
    
    async def initialize(self) -> None:
        """åˆå§‹åŒ–å­˜å‚¨"""
        if self._initialized:
            return
        
        # åˆå§‹åŒ– Qdrant
        factory = get_qdrant_factory()
        self._qdrant = factory.create(self.collection_name)
        await self._qdrant.initialize()
        
        # åŠ è½½æœ¬åœ°çŠ¶æ€
        await self._load_state()
        
        self._initialized = True
        logger.debug(f"âœ… VectorStore åˆå§‹åŒ–: {self.session_id}")
    
    async def close(self) -> None:
        """å…³é—­è¿æ¥"""
        if self._qdrant:
            await self._qdrant.close()
            self._qdrant = None
        self._initialized = False
    
    async def _load_state(self) -> None:
        """åŠ è½½çŠ¶æ€"""
        # 1. åŠ è½½ä¸Šä¸‹æ–‡ JSON
        if os.path.exists(self._context_file):
            try:
                with open(self._context_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.repo_url = data.get("repo_url")
                    self.global_context = data.get("global_context", {})
            except Exception as e:
                logger.warning(f"åŠ è½½ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        
        # 2. å°è¯•åŠ è½½ BM25 ç¼“å­˜
        cache_loaded = False
        if os.path.exists(self._cache_file):
            try:
                with open(self._cache_file, 'rb') as f:
                    cache = pickle.load(f)
                    if isinstance(cache, dict) and cache.get("version") == config.cache_version:
                        self._bm25 = cache.get("bm25")
                        self._doc_store = cache.get("doc_store", [])
                        self._indexed_files = cache.get("indexed_files", set())
                        cache_loaded = True
                        logger.debug(f"ğŸ“¦ BM25 ç¼“å­˜å‘½ä¸­: {len(self._doc_store)} æ–‡æ¡£")
            except Exception as e:
                logger.warning(f"BM25 ç¼“å­˜æŸå: {e}")
                os.remove(self._cache_file)
        
        # 3. ç¼“å­˜æœªå‘½ä¸­: ä» Qdrant é‡å»º
        if not cache_loaded and self._qdrant:
            await self._rebuild_bm25_index()
    
    async def _rebuild_bm25_index(self) -> None:
        """ä» Qdrant é‡å»º BM25 ç´¢å¼•"""
        logger.info(f"ğŸ”„ é‡å»º BM25 ç´¢å¼•: {self.session_id}")
        
        documents = await self._qdrant.get_all_documents()
        
        if documents:
            self._doc_store = documents
            self._indexed_files = {doc.file_path for doc in documents if doc.file_path}
            
            tokenized = [self._tokenize(doc.content) for doc in documents]
            if tokenized:
                self._bm25 = BM25Okapi(tokenized)
            
            self._save_bm25_cache()
            logger.info(f"âœ… BM25 ç´¢å¼•é‡å»ºå®Œæˆ: {len(documents)} æ–‡æ¡£")
    
    def _save_bm25_cache(self) -> None:
        """ä¿å­˜ BM25 ç¼“å­˜ (åŸå­å†™å…¥)"""
        if not self._doc_store:
            return
        
        try:
            fd, tmp_path = tempfile.mkstemp(dir=config.context_dir)
            with os.fdopen(fd, 'wb') as f:
                pickle.dump({
                    "version": config.cache_version,
                    "bm25": self._bm25,
                    "doc_store": self._doc_store,
                    "indexed_files": self._indexed_files,
                }, f)
            
            if os.path.exists(self._cache_file):
                os.remove(self._cache_file)
            os.rename(tmp_path, self._cache_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜ BM25 ç¼“å­˜å¤±è´¥: {e}")
    
    def _tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        return [
            t.lower() for t in re.split(config.tokenize_regex, text)
            if t.strip()
        ]
    
    def save_context(self, repo_url: str, context_data: Dict[str, Any]) -> None:
        """ä¿å­˜ä»“åº“ä¸Šä¸‹æ–‡"""
        self.repo_url = repo_url
        self.global_context = context_data
        
        try:
            # è¯»å–ç°æœ‰æ•°æ®ä»¥ä¿ç•™ report
            existing = {}
            if os.path.exists(self._context_file):
                with open(self._context_file, 'r', encoding='utf-8') as f:
                    existing = json.load(f)
            
            # åˆå¹¶æ•°æ®
            existing.update({
                "repo_url": repo_url,
                "global_context": context_data,
            })
            
            with open(self._context_file, 'w', encoding='utf-8') as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    def save_report(self, report: str, language: str = "en") -> None:
        """
        ä¿å­˜æŠ€æœ¯æŠ¥å‘Šï¼ˆæŒ‰è¯­è¨€å­˜å‚¨ï¼‰
        
        Args:
            report: æŠ¥å‘Šå†…å®¹
            language: è¯­è¨€ä»£ç  ('en', 'zh')
        """
        try:
            existing = {}
            if os.path.exists(self._context_file):
                with open(self._context_file, 'r', encoding='utf-8') as f:
                    existing = json.load(f)
            
            # æŒ‰è¯­è¨€å­˜å‚¨æŠ¥å‘Š
            if "reports" not in existing:
                existing["reports"] = {}
            existing["reports"][language] = report
            
            # å…¼å®¹æ—§å­—æ®µï¼ˆä¿ç•™æœ€æ–°çš„æŠ¥å‘Šï¼‰
            existing["report"] = report
            existing["report_language"] = language
            
            with open(self._context_file, 'w', encoding='utf-8') as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜: {self.session_id} ({language})")
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def get_report(self, language: str = "en") -> Optional[str]:
        """
        è·å–æŒ‡å®šè¯­è¨€çš„æŠ¥å‘Š
        
        Args:
            language: è¯­è¨€ä»£ç  ('en', 'zh')
            
        Returns:
            æŠ¥å‘Šå†…å®¹ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        context = self.load_context()
        if not context:
            return None
        
        # ä¼˜å…ˆä» reports å­—å…¸è·å–
        reports = context.get("reports", {})
        if language in reports:
            return reports[language]
        
        # å…¼å®¹æ—§æ ¼å¼ï¼šå¦‚æœåªæœ‰ report å­—æ®µä¸”è¯­è¨€åŒ¹é…
        if "report" in context:
            stored_lang = context.get("report_language", "en")
            if stored_lang == language:
                return context["report"]
        
        return None
    
    def get_available_languages(self) -> List[str]:
        """è·å–å·²æœ‰æŠ¥å‘Šçš„è¯­è¨€åˆ—è¡¨"""
        context = self.load_context()
        if not context:
            return []
        
        reports = context.get("reports", {})
        return list(reports.keys())
    
    def load_context(self) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½ä»“åº“ä¸Šä¸‹æ–‡
        
        Returns:
            åŒ…å« repo_url, global_context, report ç­‰çš„å­—å…¸ï¼Œä¸å­˜åœ¨è¿”å› None
        """
        if not os.path.exists(self._context_file):
            return None
        
        try:
            with open(self._context_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ¢å¤å†…å­˜çŠ¶æ€
            self.repo_url = data.get("repo_url")
            self.global_context = data.get("global_context", {})
            
            return data
        except Exception as e:
            logger.error(f"åŠ è½½ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None
    
    def has_index(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•"""
        context = self.load_context()
        return context is not None and context.get("repo_url") is not None
    
    async def reset(self) -> None:
        """é‡ç½®å­˜å‚¨ (åˆ†ææ–°ä»“åº“æ—¶è°ƒç”¨)"""
        await self.initialize()
        
        # åˆ é™¤ Qdrant é›†åˆ
        if self._qdrant:
            await self._qdrant.delete_collection()
            await self._qdrant.initialize()
        
        # æ¸…ç†æœ¬åœ°æ–‡ä»¶
        for f in [self._context_file, self._cache_file]:
            if os.path.exists(f):
                os.remove(f)
        
        # é‡ç½®å†…å­˜çŠ¶æ€
        self._bm25 = None
        self._doc_store = []
        self._indexed_files = set()
        self.repo_url = None
        self.global_context = {}
        
        logger.info(f"ğŸ—‘ï¸ é‡ç½®å­˜å‚¨: {self.session_id}")
    
    # å…¼å®¹æ—§æ¥å£
    def reset_collection(self) -> None:
        """åŒæ­¥é‡ç½® (å…¼å®¹æ—§ä»£ç )"""
        asyncio.get_event_loop().run_until_complete(self.reset())
    
    async def add_documents(
        self,
        documents: List[str],
        metadatas: List[Dict[str, Any]]
    ) -> int:
        """
        æ·»åŠ æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£å†…å®¹åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            æˆåŠŸæ·»åŠ çš„æ•°é‡
        """
        if not documents:
            return 0
        
        await self.initialize()
        
        # 1. æ‰¹é‡è·å– Embedding
        logger.info(f"ğŸ“Š Embedding: {len(documents)} ä¸ªæ–‡æ¡£")
        embedding_service = get_embedding()
        embeddings = await embedding_service.embed_batch(documents, show_progress=True)
        
        # è¿‡æ»¤æ— æ•ˆçš„
        valid_indices = [i for i, emb in enumerate(embeddings) if emb]
        if not valid_indices:
            logger.error("æ‰€æœ‰ Embedding éƒ½å¤±è´¥äº†")
            return 0
        
        # 2. æ„å»º Document å¯¹è±¡
        docs = []
        for i in valid_indices:
            doc_id = f"{metadatas[i].get('file', 'unknown')}_{len(self._doc_store) + len(docs)}"
            doc = Document(
                id=doc_id,
                content=documents[i],
                metadata=metadatas[i],
            )
            docs.append(doc)
        
        valid_embeddings = [embeddings[i] for i in valid_indices]
        
        # 3. å†™å…¥ Qdrant
        added = await self._qdrant.add_documents(docs, valid_embeddings)
        
        # 4. æ›´æ–° BM25 ç´¢å¼•
        self._doc_store.extend(docs)
        self._indexed_files.update(doc.file_path for doc in docs)
        
        tokenized = [self._tokenize(doc.content) for doc in self._doc_store]
        self._bm25 = BM25Okapi(tokenized)
        
        # 5. ä¿å­˜ç¼“å­˜
        self._save_bm25_cache()
        
        return added
    
    async def embed_text(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬ Embedding"""
        embedding_service = get_embedding()
        return await embedding_service.embed_text(text)
    
    async def search_hybrid(
        self,
        query: str,
        top_k: int = None
    ) -> List[Dict[str, Any]]:
        """
        æ··åˆæœç´¢ (å‘é‡ + BM25ï¼ŒRRF èåˆ)
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        await self.initialize()
        
        top_k = top_k or config.default_top_k
        candidate_k = top_k * config.search_oversample
        
        # 1. å‘é‡æœç´¢
        vector_results: List[SearchResult] = []
        query_embedding = await self.embed_text(query)
        
        if query_embedding and self._qdrant:
            vector_results = await self._qdrant.search(
                query_embedding,
                top_k=candidate_k
            )
        
        # 2. BM25 æœç´¢
        bm25_results: List[SearchResult] = []
        if self._bm25 and self._doc_store:
            tokens = self._tokenize(query)
            if not tokens:
                tokens = [""]
            
            try:
                scores = self._bm25.get_scores(tokens)
                top_indices = sorted(
                    range(len(scores)),
                    key=lambda i: scores[i],
                    reverse=True
                )[:candidate_k]
                
                for idx in top_indices:
                    if scores[idx] > 0:
                        doc = self._doc_store[idx]
                        bm25_results.append(SearchResult(
                            document=doc,
                            score=scores[idx],
                            source="bm25",
                        ))
            except Exception as e:
                logger.error(f"BM25 æœç´¢å¤±è´¥: {e}")
        
        # 3. RRF èåˆ
        fused = self._rrf_fusion(vector_results, bm25_results)
        
        # 4. æ ¼å¼åŒ–è¾“å‡º (å…¼å®¹æ—§æ¥å£)
        results = []
        for item in fused[:top_k]:
            doc = item.document
            results.append({
                "id": doc.id,
                "content": doc.content,
                "file": doc.file_path,
                "metadata": doc.metadata,
                "score": item.score,
            })
        
        return results
    
    def _rrf_fusion(
        self,
        vector_results: List[SearchResult],
        bm25_results: List[SearchResult]
    ) -> List[SearchResult]:
        """RRF (Reciprocal Rank Fusion) èåˆ"""
        k = config.rrf_k
        fused: Dict[str, Dict] = {}
        
        # å‘é‡ç»“æœ
        for rank, result in enumerate(vector_results):
            doc_id = result.document.id
            if doc_id not in fused:
                fused[doc_id] = {"result": result, "score": 0}
            fused[doc_id]["score"] += config.rrf_weight_vector / (k + rank + 1)
        
        # BM25 ç»“æœ
        for rank, result in enumerate(bm25_results):
            doc_id = result.document.id
            if doc_id not in fused:
                fused[doc_id] = {"result": result, "score": 0}
            fused[doc_id]["score"] += config.rrf_weight_bm25 / (k + rank + 1)
        
        # æ’åº
        sorted_items = sorted(
            fused.values(),
            key=lambda x: x["score"],
            reverse=True
        )
        
        return [
            SearchResult(
                document=item["result"].document,
                score=item["score"],
                source="hybrid",
            )
            for item in sorted_items
        ]
    
    def get_documents_by_file(self, file_path: str) -> List[Dict[str, Any]]:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„è·å–æ–‡æ¡£ (å…¼å®¹æ—§æ¥å£)"""
        docs = [
            doc for doc in self._doc_store
            if doc.file_path == file_path
        ]
        
        result = []
        for doc in sorted(docs, key=lambda d: d.metadata.get("start_line", 0)):
            result.append({
                "id": doc.id,
                "content": doc.content,
                "file": doc.file_path,
                "metadata": doc.metadata,
                "score": 1.0,
            })
        
        return result
    
    @property
    def indexed_files(self) -> Set[str]:
        """å·²ç´¢å¼•çš„æ–‡ä»¶"""
        return self._indexed_files


# ============================================================
# ç®¡ç†å™¨ - LRU Cache + è¿‡æœŸæ¸…ç†
# ============================================================

class SessionEntry:
    """Session æ¡ç›® - åŒ…å«å­˜å‚¨å®ä¾‹å’Œè®¿é—®æ—¶é—´"""
    __slots__ = ('store', 'last_access', 'created_at')
    
    def __init__(self, store: VectorStore):
        self.store = store
        self.last_access = time.time()
        self.created_at = time.time()
    
    def touch(self) -> None:
        """æ›´æ–°è®¿é—®æ—¶é—´"""
        self.last_access = time.time()


class VectorStoreManager:
    """
    å‘é‡å­˜å‚¨ç®¡ç†å™¨ - LRU Cache å®ç°
    
    ç‰¹æ€§:
    1. LRU æ·˜æ±° - è¶…è¿‡ max_count æ—¶æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„å†…å­˜ä¸­çš„ session
    2. ä»“åº“æ•°æ®æ°¸ä¹…å­˜å‚¨ - ä¸æ¸…ç†ä»“åº“ç´¢å¼•å’ŒæŠ¥å‘Š
    3. çº¿ç¨‹å®‰å…¨ - ä½¿ç”¨ asyncio.Lock
    """
    
    def __init__(self, max_count: int = None):
        self._max_count = max_count or config.session_max_count
        self._sessions: Dict[str, SessionEntry] = {}
        self._lock = asyncio.Lock()
    
    def get_store(self, session_id: str) -> VectorStore:
        """
        è·å–æˆ–åˆ›å»ºå­˜å‚¨å®ä¾‹ (åŒæ­¥æ¥å£ï¼Œå…¼å®¹ç°æœ‰ä»£ç )
        
        ä¼šè§¦å‘ LRU æ·˜æ±°æ£€æŸ¥
        """
        if session_id in self._sessions:
            entry = self._sessions[session_id]
            entry.touch()
            # ç§»åŠ¨åˆ°æœ€åï¼ˆæ¨¡æ‹Ÿ LRUï¼‰
            self._sessions.pop(session_id)
            self._sessions[session_id] = entry
            return entry.store
        
        # åˆ›å»ºæ–° session
        store = VectorStore(session_id)
        entry = SessionEntry(store)
        self._sessions[session_id] = entry
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ LRU æ·˜æ±°ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
        if len(self._sessions) > self._max_count:
            asyncio.create_task(self._evict_lru())
        
        logger.info(f"ğŸ“¦ Session åˆ›å»º: {session_id} (æ€»æ•°: {len(self._sessions)})")
        return store
    
    async def _evict_lru(self) -> None:
        """æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„ session"""
        async with self._lock:
            while len(self._sessions) > self._max_count:
                # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„
                oldest_id = min(
                    self._sessions.keys(),
                    key=lambda k: self._sessions[k].last_access
                )
                entry = self._sessions.pop(oldest_id)
                await entry.store.close()
                logger.info(f"ğŸ—‘ï¸ LRU æ·˜æ±°: {oldest_id}")
    
    async def close_session(self, session_id: str) -> None:
        """å…³é—­æŒ‡å®š session"""
        async with self._lock:
            if session_id in self._sessions:
                entry = self._sessions.pop(session_id)
                await entry.store.close()
                logger.info(f"ğŸ”’ Session å…³é—­: {session_id}")
    
    async def close_all(self) -> None:
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        async with self._lock:
            for session_id, entry in list(self._sessions.items()):
                await entry.store.close()
            self._sessions.clear()
            logger.info("ğŸ”’ æ‰€æœ‰ Session å·²å…³é—­")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç®¡ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        now = time.time()
        sessions_info = []
        for sid, entry in self._sessions.items():
            sessions_info.append({
                "session_id": sid,
                "age_hours": round((now - entry.created_at) / 3600, 2),
                "idle_minutes": round((now - entry.last_access) / 60, 2),
            })
        
        return {
            "total_sessions": len(self._sessions),
            "max_sessions": self._max_count,
            "sessions": sorted(sessions_info, key=lambda x: x["idle_minutes"], reverse=True)
        }


# å…¨å±€ç®¡ç†å™¨
store_manager = VectorStoreManager()
