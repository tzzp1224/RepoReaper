# -*- coding: utf-8 -*-
# æ–‡ä»¶è·¯å¾„: app/services/vector_service.py

import asyncio
import chromadb
from chromadb.config import Settings as ChromaSettings
from app.core.config import settings
from app.utils.embedding import get_embedding_service, EmbeddingConfig
from rank_bm25 import BM25Okapi
from filelock import FileLock, Timeout
from dataclasses import dataclass

import re
import os
import json
import shutil
import pickle
import logging
import tempfile
import time

# === æ—¥å¿—é…ç½® ===
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class VectorServiceConfig:
    # --- åŸºç¡€é…ç½® ---
    DATA_DIR: str = "data"
    CACHE_VERSION: str = "1.0"
    
    # --- æ¨¡åž‹é…ç½® ---
    API_BASE_URL: str = "https://api.siliconflow.cn/v1"
    EMBEDDING_MODEL_NAME: str = "BAAI/bge-m3"
    EMBEDDING_BATCH_SIZE: int = 50       # æ‰¹é‡ Embedding çš„å¤§å°
    MAX_TEXT_LENGTH: int = 8000          # å•ä¸ªæ–‡æ¡£æœ€å¤§å­—ç¬¦æ•° (é˜²æ­¢ Token è¶…é™)

    # --- æ–‡æœ¬å¤„ç†é…ç½® ---
    # æ”¯æŒä¸­æ–‡çš„æ­£åˆ™ç¤ºä¾‹ï¼šr'[^a-zA-Z0-9_\.@\u4e00-\u9fa5]+'
    TOKENIZE_REGEX: str = r'[^a-zA-Z0-9_\.@]+'

    # --- å¹¶å‘æŽ§åˆ¶ ---
    LOCK_TIMEOUT_RESET: int = 10         # é‡ç½®æ“ä½œçš„é”ç­‰å¾…æ—¶é—´ (ç§’)
    LOCK_TIMEOUT_WRITE: int = 30         # å†™å…¥æ“ä½œçš„é”ç­‰å¾…æ—¶é—´ (ç§’)

    # --- æ··åˆæ£€ç´¢ (RRF) å‚æ•° ---
    RRF_K: int = 60                      # RRF ç®—æ³•ä¸­çš„å¹³æ»‘å¸¸æ•° k
    RRF_WEIGHT_VECTOR: float = 1.0       # å‘é‡æ£€ç´¢æƒé‡
    RRF_WEIGHT_BM25: float = 0.3         # å…³é”®å­—æ£€ç´¢æƒé‡
    SEARCH_OVERSAMPLE_FACTOR: int = 2    # åˆç­›å€çŽ‡ (TopK * N)
    DEFAULT_TOP_K: int = 3               # é»˜è®¤æœç´¢æ•°é‡

# å®žä¾‹åŒ–é…ç½® (åŽç»­ä»£ç ç»Ÿä¸€ä½¿ç”¨è¿™ä¸ªå®žä¾‹)
vector_config = VectorServiceConfig()

# === åˆå§‹åŒ– Embedding æœåŠ¡ (å¹¶å‘ä¼˜åŒ–ç‰ˆ) ===
embedding_config = EmbeddingConfig(
    api_base_url=vector_config.API_BASE_URL,
    model_name=vector_config.EMBEDDING_MODEL_NAME,
    batch_size=vector_config.EMBEDDING_BATCH_SIZE,
    max_text_length=vector_config.MAX_TEXT_LENGTH,
    max_concurrent_batches=5  # æœ€å¤§ 5 ä¸ªå¹¶å‘æ‰¹æ¬¡
)
embedding_service = get_embedding_service(embedding_config)

CHROMA_DIR = os.path.join(vector_config.DATA_DIR, "chroma_db")
CONTEXT_DIR = os.path.join(vector_config.DATA_DIR, "contexts")
# å…¨å±€æ–‡ä»¶é”
LOCK_FILE = os.path.join(vector_config.DATA_DIR, "vector_store.lock")

os.makedirs(CHROMA_DIR, exist_ok=True)
os.makedirs(CONTEXT_DIR, exist_ok=True)

# === å…¨å±€ Client ===
try:
    GLOBAL_CHROMA_CLIENT = chromadb.PersistentClient(path=CHROMA_DIR)
except Exception as e:
    logger.critical(f"ChromaDB Init Error: {e}", exc_info=True)
    GLOBAL_CHROMA_CLIENT = None


class VectorStore:
    def __init__(self, session_id: str):
        self.session_id = self._sanitize_session_id(session_id)
        
        self.chroma_client = GLOBAL_CHROMA_CLIENT
        self.collection_name = f"repo_{self.session_id}"
        
        # è¯»æ“ä½œé€šå¸¸ä¸éœ€è¦å¼ºé”ï¼ŒChroma å†…éƒ¨æœ‰å¤„ç†
        self.collection = self.chroma_client.get_or_create_collection(name=self.collection_name)
        
        self.context_file = os.path.join(CONTEXT_DIR, f"{self.session_id}.json")
        self.bm25_cache_file = os.path.join(CONTEXT_DIR, f"{self.session_id}_bm25.pkl")
        
        self.repo_url = None
        self.indexed_files = set() 
        self.doc_store = [] 
        self.bm25 = None
        
        self._load_local_state()

    def _sanitize_session_id(self, session_id: str) -> str:
        """é˜²æ­¢è·¯å¾„æ³¨å…¥"""
        clean_id = re.sub(r'[^a-zA-Z0-9_-]', '', session_id)
        if not clean_id: raise ValueError("Invalid session_id")
        return clean_id

    def _load_local_state(self):
        """åŠ è½½çŠ¶æ€ (Pickle Cache ä¼˜å…ˆ)"""
        # åŠ è½½ Context JSON
        if os.path.exists(self.context_file):
            try:
                with open(self.context_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.repo_url = data.get("repo_url")
                    self.global_context = data.get("global_context", {})
            except Exception as e:
                logger.error(f"Context Load Error: {e}")
                self.global_context = {}
        else:
            self.global_context = {}

        # å°è¯•åŠ è½½ Pickle ç¼“å­˜
        cache_loaded = False
        if os.path.exists(self.bm25_cache_file):
            try:
                with open(self.bm25_cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                    # Bug Fix: å¢žåŠ ç‰ˆæœ¬æ ¡éªŒ
                    if isinstance(cache_data, dict) and cache_data.get('version') == vector_config.CACHE_VERSION:
                        self.bm25 = cache_data.get('bm25')
                        self.doc_store = cache_data.get('doc_store', [])
                        self.indexed_files = cache_data.get('indexed_files', set())
                        cache_loaded = True
                    else:
                        logger.warning(f"Cache version mismatch or invalid for {self.session_id}")
            except Exception as e:
                logger.warning(f"Cache corrupted ({e}), rebuilding...")
                os.remove(self.bm25_cache_file)

        # ç¼“å­˜æœªå‘½ä¸­ï¼šä»Ž DB é‡å»º (Slow Path)
        if not cache_loaded:
            logger.info(f"Rebuilding index from DB for {self.session_id}...")
            try:
                existing_data = self.collection.get()
                if existing_data and existing_data['ids']:
                    self.doc_store = []
                    self.indexed_files = set()
                    for i, doc_id in enumerate(existing_data['ids']):
                        content = existing_data['documents'][i]
                        meta = existing_data['metadatas'][i]
                        self.indexed_files.add(meta['file'])
                        self.doc_store.append({
                            "id": doc_id,
                            "content": content,
                            "metadata": meta
                        })
                    
                    tokenized_corpus = [self._tokenize(doc['content']) for doc in self.doc_store]
                    if tokenized_corpus:
                        self.bm25 = BM25Okapi(tokenized_corpus)
                    
                    self._save_bm25_cache()
            except Exception as e:
                logger.error(f"DB Rebuild Error: {e}")

    def _save_bm25_cache(self):
        """åŽŸå­å†™å…¥ç¼“å­˜"""
        if not self.doc_store: return
        try:
            fd, tmp_path = tempfile.mkstemp(dir=CONTEXT_DIR)
            with os.fdopen(fd, 'wb') as f:
                pickle.dump({
                    'version': vector_config.CACHE_VERSION,
                    'bm25': self.bm25, 
                    'doc_store': self.doc_store,
                    'indexed_files': self.indexed_files
                }, f)
            
            if os.path.exists(self.bm25_cache_file):
                os.remove(self.bm25_cache_file)
            os.rename(tmp_path, self.bm25_cache_file)
        except Exception as e:
            logger.error(f"Save Cache Error: {e}")
            if os.path.exists(tmp_path): os.remove(tmp_path)

    def save_context(self, repo_url, context_data):
        self.repo_url = repo_url
        self.global_context = context_data
        data = {"repo_url": repo_url, "global_context": context_data}
        try:
            with open(self.context_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Save Context Json Error: {e}")

    def reset_collection(self):
        # ä¸´ç•ŒåŒºï¼šå†™æ“ä½œå¿…é¡»åŠ é”
        lock = FileLock(LOCK_FILE, timeout=vector_config.LOCK_TIMEOUT_RESET)
        try:
            with lock:
                try:
                    self.chroma_client.delete_collection(name=self.collection_name)
                except ValueError: pass
                
                if os.path.exists(self.context_file): os.remove(self.context_file)
                if os.path.exists(self.bm25_cache_file): os.remove(self.bm25_cache_file)
                
                self.collection = self.chroma_client.get_or_create_collection(name=self.collection_name)
                self.bm25 = None
                self.doc_store = []
                self.repo_url = None
                self.indexed_files = set()
                self.global_context = {}
        except Timeout:
            logger.error("Reset Lock Timeout")
            raise

    async def embed_text(self, text):
        """èŽ·å–å•ä¸ªæ–‡æœ¬çš„ Embedding (ä½¿ç”¨ä¼˜åŒ–åŽçš„æœåŠ¡)"""
        return await embedding_service.embed_text(text)

    def _tokenize(self, text):
        return [t.lower() for t in re.split(vector_config.TOKENIZE_REGEX, text) if t.strip()]

    async def add_documents(self, documents, metadatas):
        if not documents: return
        
        ids = []
        
        # 1. æ‰¹é‡ Embedding (å¹¶å‘ä¼˜åŒ–ç‰ˆ - è‡ªåŠ¨åˆ†æ‰¹ã€å¹¶å‘ã€é‡è¯•)
        logger.info(f"ðŸ“Š å¼€å§‹ Embedding: {len(documents)} ä¸ªæ–‡æ¡£")
        embeddings = await embedding_service.embed_batch(documents, show_progress=True)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ embeddings
        valid_embeddings = [e for e in embeddings if e]
        if not valid_embeddings:
            logger.error("Embedding å…¨éƒ¨å¤±è´¥ï¼Œè·³è¿‡æ–‡æ¡£æ·»åŠ ")
            return

        # 2. å‡†å¤‡æ•°æ®
        new_doc_entries = []
        for i, doc in enumerate(documents):
            self.indexed_files.add(metadatas[i]['file'])
            doc_id = f"{metadatas[i]['file']}_{len(self.doc_store) + i}"
            ids.append(doc_id)
            new_doc_entries.append({
                "id": doc_id, "content": doc, "metadata": metadatas[i]
            })

        # 3. ä¸´ç•ŒåŒºï¼šåœ¨çº¿ç¨‹ä¸­æ‰§è¡Œå†™å…¥ï¼Œé¿å…é˜»å¡žäº‹ä»¶å¾ªçŽ¯
        def _write_to_db():
            lock = FileLock(LOCK_FILE, timeout=vector_config.LOCK_TIMEOUT_WRITE)
            try:
                with lock:
                    # ä½¿ç”¨å±€éƒ¨å˜é‡ï¼Œé˜²æ­¢å†™å…¥éƒ¨åˆ†å¤±è´¥å¯¼è‡´å†…å­˜è„æ•°æ®
                    # å…ˆå†™ DB
                    if embeddings:
                        self.collection.add(
                            documents=documents, embeddings=embeddings, 
                            metadatas=metadatas, ids=ids
                        )
                    
                    # å†æ›´æ–°å†…å­˜
                    self.doc_store.extend(new_doc_entries)
                    tokenized_corpus = [self._tokenize(d['content']) for d in self.doc_store]
                    self.bm25 = BM25Okapi(tokenized_corpus)
                    
                    # æœ€åŽå†™ç¼“å­˜
                    self._save_bm25_cache()
                    
            except Timeout:
                logger.error("Add Docs Lock Timeout")
                raise Exception("System busy, please try again.")
            except Exception as e:
                logger.critical(f"Critical Write Error: {e}")
                raise
        
        # ðŸ”§ ä½¿ç”¨ asyncio.to_thread é¿å…é˜»å¡žäº‹ä»¶å¾ªçŽ¯
        await asyncio.to_thread(_write_to_db)

    def get_documents_by_file(self, file_path):
        raw_docs = [doc for doc in self.doc_store if doc['metadata']['file'] == file_path]
        formatted_docs = []
        for d in raw_docs:
            formatted_docs.append({
                "id": d['id'], "content": d['content'],
                "file": d['metadata']['file'], "metadata": d['metadata'], "score": 1.0
            })
        return sorted(formatted_docs, key=lambda x: x['metadata'].get('start_line', 0))

    # === Search é€»è¾‘ ===
    async def search_hybrid(self, query: str, top_k: int = vector_config.DEFAULT_TOP_K) -> list:
        vector_results = []
        query_embedding = await self.embed_text(query)
        
        candidate_k = top_k * vector_config.SEARCH_OVERSAMPLE_FACTOR

        # 1. å‘é‡æœç´¢ (è¯»ç£ç›˜ï¼Œé€šå¸¸æ— éœ€é”ï¼Œæˆ–è€… Chroma å†…éƒ¨æœ‰è¯»é”)
        if query_embedding:
            try:
                chroma_res = self.collection.query(
                    query_embeddings=[query_embedding], n_results=candidate_k
                )
                if chroma_res['ids']:
                    ids = chroma_res['ids'][0]
                    docs = chroma_res['documents'][0]
                    metas = chroma_res['metadatas'][0]
                    for i in range(len(ids)):
                        vector_results.append({
                            "id": ids[i], "content": docs[i], 
                            "file": metas[i]['file'], "metadata": metas[i], "score": 0
                        })
            except Exception as e:
                logger.error(f"Chroma Search Error: {e}")

        # 2. BM25 æœç´¢ (è¯»å†…å­˜)
        bm25_results = []
        if self.bm25:
            tokenized_query = self._tokenize(query)
            # ç®€å•çš„é˜²é”™
            if not tokenized_query: tokenized_query = [""]
            
            try:
                doc_scores = self.bm25.get_scores(tokenized_query)
                top_n = min(len(doc_scores), candidate_k)
                # èŽ·å–å‰ N ä¸ªæœ€é«˜åˆ†çš„ç´¢å¼•
                top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:top_n]
                
                for idx in top_indices:
                    if doc_scores[idx] > 0:
                        item = self.doc_store[idx]
                        bm25_results.append({
                            "id": item["id"], "content": item["content"], 
                            "file": item["metadata"]["file"], "metadata": item["metadata"], "score": 0
                        })
            except Exception as e:
                logger.error(f"BM25 Search Error: {e}")

        # 3. RRF èžåˆ (Reciprocal Rank Fusion)
        k = vector_config.RRF_K
        fused_scores = {}

        for rank, item in enumerate(vector_results):
            doc_id = item['id']
            if doc_id not in fused_scores: fused_scores[doc_id] = {"item": item, "score": 0}
            # ä½¿ç”¨é…ç½®æƒé‡
            fused_scores[doc_id]["score"] += vector_config.RRF_WEIGHT_VECTOR * (1 / (k + rank + 1))
            
        for rank, item in enumerate(bm25_results):
            doc_id = item['id']
            if doc_id not in fused_scores: fused_scores[doc_id] = {"item": item, "score": 0}
            # ä½¿ç”¨é…ç½®æƒé‡
            fused_scores[doc_id]["score"] += vector_config.RRF_WEIGHT_BM25 * (1 / (k + rank + 1))

        sorted_results = sorted(fused_scores.values(), key=lambda x: x['score'], reverse=True)
        return [res['item'] for res in sorted_results[:top_k]]

class VectorStoreManager:
    def get_store(self, session_id: str) -> VectorStore:
        return VectorStore(session_id)

store_manager = VectorStoreManager()