# æ–‡ä»¶è·¯å¾„: app/services/auto_evaluation_service.py
"""
è‡ªåŠ¨è¯„ä¼°æœåŠ¡ - Phase 1
åœ¨åå°å¼‚æ­¥è¿›è¡Œè¯„ä¼°ï¼Œä¸é˜»å¡ç”¨æˆ·è¯·æ±‚

å·¥ä½œæµç¨‹:
  1. ç”¨æˆ·è°ƒç”¨ /chat æˆ– /analyze
  2. è·å¾—ç«‹å³å“åº”
  3. åå°å¼‚æ­¥æ‰§è¡Œè¯„ä¼°
  4. è¯„ä¼°ç»“æœå­˜å‚¨åˆ° evaluation/sft_data/
"""

import asyncio
import json
import os
from datetime import datetime
from typing import Optional
from dataclasses import dataclass

from evaluation.evaluation_framework import (
    EvaluationEngine, 
    EvaluationResult, 
    DataRoutingEngine,
    DataQualityTier
)
from evaluation.utils import is_chatty_query, has_code_indicators
from app.services.tracing_service import tracing_service


@dataclass
class EvaluationConfig:
    """
    è‡ªåŠ¨è¯„ä¼°é…ç½®
    
    æ•°æ®è·¯ç”±é˜ˆå€¼è¯´æ˜ï¼ˆä¸ data_router.py ä¸€è‡´ï¼‰:
    - score > 0.9  â†’ Gold   â†’ positive_samples.jsonl
    - score > 0.6  â†’ Silver â†’ positive_samples.jsonl  
    - score > 0.4  â†’ Bronze â†’ negative_samples.jsonl
    - score <= 0.4 â†’ Rejected â†’ ä¸å­˜å‚¨
    """
    enabled: bool = True                    # æ˜¯å¦å¯ç”¨è‡ªåŠ¨è¯„ä¼°
    use_ragas: bool = False                 # æ˜¯å¦ä½¿ç”¨ Ragas è¿›è¡Œ sanity check
    custom_weight: float = 0.7              # custom_eval çš„æƒé‡
    ragas_weight: float = 0.3               # ragas_eval çš„æƒé‡
    diff_threshold: float = 0.2             # å·®å¼‚é˜ˆå€¼ï¼ˆè¶…è¿‡åˆ™æ ‡è®° needs_reviewï¼‰
    min_quality_score: float = 0.4          # æœ€ä½è´¨é‡åˆ†æ•°ï¼ˆ<=0.4 æ‰æ‹’ç»ï¼‰
    async_evaluation: bool = True           # æ˜¯å¦å¼‚æ­¥æ‰§è¡Œï¼ˆæ¨è Trueï¼‰
    min_query_length: int = 10              # æœ€å° query é•¿åº¦
    min_answer_length: int = 100            # æœ€å° answer é•¿åº¦
    require_repo_url: bool = True           # æ˜¯å¦è¦æ±‚æœ‰ä»“åº“ URL
    require_code_in_context: bool = True    # æ˜¯å¦è¦æ±‚ä¸Šä¸‹æ–‡åŒ…å«ä»£ç 


class AutoEvaluationService:
    """è‡ªåŠ¨è¯„ä¼°æœåŠ¡"""
    
    def __init__(
        self,
        eval_engine: EvaluationEngine,
        data_router: DataRoutingEngine,
        config: EvaluationConfig = None
    ):
        self.eval_engine = eval_engine
        self.data_router = data_router
        self.config = config or EvaluationConfig()
        self.needs_review_queue: list = []  # éœ€è¦äººå·¥å®¡æŸ¥çš„æ ·æœ¬é˜Ÿåˆ—
        self._evaluated_keys: set = set()   # é˜²é‡å¤è¯„ä¼°ï¼ˆsession_id:query_hashï¼‰
        
        # è¢«è¿‡æ»¤æ•°æ®çš„è®°å½•æ–‡ä»¶
        self.skipped_samples_file = "evaluation/sft_data/skipped_samples.jsonl"
        os.makedirs(os.path.dirname(self.skipped_samples_file), exist_ok=True)
    
    def _record_skipped(self, reason: str, query: str, session_id: str, 
                        repo_url: str = "", context_len: int = 0, answer_len: int = 0) -> None:
        """è®°å½•è¢«è·³è¿‡çš„æ ·æœ¬ï¼ˆä¾›æ—¥ååˆ†æï¼‰"""
        record = {
            "timestamp": datetime.now().isoformat(),
            "reason": reason,
            "session_id": session_id,
            "query": query[:200] if query else "",
            "repo_url": repo_url,
            "context_length": context_len,
            "answer_length": answer_len
        }
        try:
            with open(self.skipped_samples_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"  âš ï¸ è®°å½•è·³è¿‡æ ·æœ¬å¤±è´¥: {e}")
    
    def _validate_input(
        self,
        query: str,
        retrieved_context: str,
        generated_answer: str,
        session_id: str,
        repo_url: str
    ) -> tuple[bool, Optional[str]]:
        """
        éªŒè¯è¾“å…¥æ˜¯å¦æ»¡è¶³è¯„ä¼°æ¡ä»¶
        
        Returns:
            (is_valid, skip_reason) - å¦‚æœæœ‰æ•ˆè¿”å› (True, None)ï¼Œå¦åˆ™è¿”å› (False, reason)
        """
        context_len = len(retrieved_context) if retrieved_context else 0
        answer_len = len(generated_answer) if generated_answer else 0
        
        # Query éªŒè¯
        if not query or not query.strip():
            self._record_skipped("query_empty", query or "", session_id, repo_url, context_len, answer_len)
            return False, "query ä¸ºç©º"
        
        if len(query.strip()) < self.config.min_query_length:
            self._record_skipped("query_too_short", query, session_id, repo_url, context_len, answer_len)
            return False, f"query å¤ªçŸ­ ({len(query)} < {self.config.min_query_length})"
        
        if is_chatty_query(query):
            self._record_skipped("chatty_query", query, session_id, repo_url, context_len, answer_len)
            return False, f"é—²èŠ/æ— æ•ˆ query: {query[:30]}"
        
        # Repo URL éªŒè¯
        if self.config.require_repo_url and not repo_url:
            self._record_skipped("missing_repo_url", query, session_id, repo_url, context_len, answer_len)
            return False, "ç¼ºå°‘ repo_url"
        
        # Answer éªŒè¯
        if not generated_answer or len(generated_answer.strip()) < self.config.min_answer_length:
            self._record_skipped("answer_too_short", query, session_id, repo_url, context_len, answer_len)
            return False, f"å›ç­”å¤ªçŸ­ ({answer_len} < {self.config.min_answer_length})"
        
        # Context éªŒè¯
        if self.config.require_code_in_context and not has_code_indicators(retrieved_context):
            self._record_skipped("no_code_in_context", query, session_id, repo_url, context_len, answer_len)
            return False, "ä¸Šä¸‹æ–‡ä¸­æœªæ£€æµ‹åˆ°ä»£ç "
        
        return True, None
    
    def _check_duplicate(self, query: str, session_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤è¯„ä¼°ï¼Œè¿”å› True è¡¨ç¤ºæ˜¯é‡å¤çš„"""
        import hashlib
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        eval_key = f"{session_id}:{query_hash}"
        
        if eval_key in self._evaluated_keys:
            return True
        
        self._evaluated_keys.add(eval_key)
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
        if len(self._evaluated_keys) > 1000:
            self._evaluated_keys = set(list(self._evaluated_keys)[-500:])
        
        return False
    
    async def auto_evaluate(
        self,
        query: str,
        retrieved_context: str,
        generated_answer: str,
        session_id: str = "auto",
        repo_url: str = "",
        language: str = "en"
    ) -> Optional[str]:
        """
        è‡ªåŠ¨è¯„ä¼°å•ä¸ªæŸ¥è¯¢-å›ç­”å¯¹
        
        Returns:
            è´¨é‡ç­‰çº§ (gold/silver/bronze/rejected/needs_review) æˆ– None
        """
        # è¾“å…¥éªŒè¯
        is_valid, skip_reason = self._validate_input(
            query, retrieved_context, generated_answer, session_id, repo_url
        )
        if not is_valid:
            print(f"  âš ï¸ [AutoEval] è·³è¿‡: {skip_reason}")
            return None
        
        # é˜²é‡å¤è¯„ä¼°
        if self._check_duplicate(query, session_id):
            print(f"  â­ï¸ [AutoEval] è·³è¿‡é‡å¤è¯„ä¼°: {query[:30]}...")
            return None
        
        start_time = datetime.now()
        
        try:
            # Step 1: è‡ªå®šä¹‰è¯„ä¼°
            print(f"ğŸ“Š [AutoEval] å¼€å§‹è¯„ä¼°: {query[:50]}...")
            
            custom_metrics = await self.eval_engine.evaluate_generation(
                query=query,
                retrieved_context=retrieved_context,
                generated_answer=generated_answer
            )
            custom_score = custom_metrics.overall_score()
            
            print(f"  âœ“ Custom Score: {custom_score:.3f}")
            print(f"    - Faithfulness: {custom_metrics.faithfulness:.3f}")
            print(f"    - Answer Relevance: {custom_metrics.answer_relevance:.3f}")
            print(f"    - Completeness: {custom_metrics.answer_completeness:.3f}")
            
            # Step 2: Ragas Sanity Check (å¦‚æœå¯ç”¨)
            ragas_score = None
            ragas_details = None
            
            if self.config.use_ragas:
                try:
                    ragas_score, ragas_details = await self._ragas_eval(
                        query=query,
                        context=retrieved_context,
                        answer=generated_answer
                    )
                    print(f"  âœ“ Ragas Score: {ragas_score:.3f}")
                    if ragas_details:
                        print(f"    - {ragas_details}")
                except Exception as e:
                    print(f"  âš ï¸ Ragas è¯„ä¼°å¤±è´¥: {e}")
                    # Ragas å¤±è´¥ä¸åº”è¯¥ä¸­æ–­ä¸»æµç¨‹
            
            # ============================================================
            # Step 3: æ··åˆè¯„ä¼° + å¼‚å¸¸æ£€æµ‹
            # ============================================================
            final_score, quality_status = self._compute_final_score(
                custom_score=custom_score,
                ragas_score=ragas_score
            )
            
            print(f"  âœ“ Final Score: {final_score:.3f} | Status: {quality_status}")
            
            # ============================================================
            # Step 4: æ„å»ºè¯„ä¼°ç»“æœå¹¶å­˜å‚¨
            # ============================================================
            eval_result = EvaluationResult(
                session_id=session_id,
                query=query,
                repo_url=repo_url,
                timestamp=start_time,
                language=language,
                generation_metrics=custom_metrics,
                notes=f"ragas_score={ragas_score:.3f}" if ragas_score else ""
            )
            
            # è®¾ç½®ç»¼åˆå¾—åˆ†
            eval_result.overall_score = final_score
            
            # æ ¹æ®çŠ¶æ€å’Œå¾—åˆ†ç¡®å®šè´¨é‡ç­‰çº§
            print(f"  [DEBUG] quality_status={quality_status}, final_score={final_score:.3f}, threshold={self.config.min_quality_score}")
            
            if quality_status == "needs_review":
                eval_result.data_quality_tier = DataQualityTier.BRONZE
                eval_result.notes += " | needs_review=true"
                # åŠ å…¥å®¡æŸ¥é˜Ÿåˆ—
                self.needs_review_queue.append({
                    "eval_result": eval_result,
                    "custom_score": custom_score,
                    "ragas_score": ragas_score,
                    "diff": abs(custom_score - (ragas_score or custom_score)),
                    "timestamp": start_time.isoformat()
                })
                print(f"  âš ï¸ éœ€è¦äººå·¥å®¡æŸ¥ (needs_review)ï¼Œæš‚å­˜é˜Ÿåˆ—")
                # åŒæ—¶ä¹Ÿè·¯ç”±åˆ°æ•°æ®å­˜å‚¨ï¼Œä¾¿äºåç»­åˆ†æ
                self.data_router.route_sample(eval_result)
            elif final_score > self.config.min_quality_score:
                # score > 0.4: è·¯ç”±åˆ° positive (>0.6) æˆ– negative (0.4-0.6)
                print(f"  âœ“ è·¯ç”±åˆ° data_router (score {final_score:.2f} > {self.config.min_quality_score})")
                self.data_router.route_sample(eval_result)
            else:
                # score <= 0.4: è´¨é‡å¤ªå·®ï¼Œç›´æ¥æ‹’ç»
                eval_result.data_quality_tier = DataQualityTier.REJECTED
                print(f"  âŒ è¯„åˆ†è¿‡ä½ ({final_score:.2f} <= {self.config.min_quality_score})ï¼Œæ‹’ç»å­˜å‚¨")
            
            # è®°å½•åˆ° tracing
            tracing_service.add_event("auto_evaluation_completed", {
                "query": query[:100],
                "custom_score": custom_score,
                "ragas_score": ragas_score,
                "final_score": final_score,
                "status": quality_status,
                "quality_tier": eval_result.data_quality_tier.value
            })
            
            print(f"  âœ… è¯„ä¼°å®Œæˆ\n")
            
            return eval_result.data_quality_tier.value
        
        except Exception as e:
            print(f"  âŒ è‡ªåŠ¨è¯„ä¼°å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    async def auto_evaluate_async(
        self,
        query: str,
        retrieved_context: str,
        generated_answer: str,
        session_id: str = "auto",
        repo_url: str = "",
        language: str = "en"
    ) -> None:
        """
        å¼‚æ­¥ç‰ˆæœ¬ - ä¸é˜»å¡ä¸»æµç¨‹
        
        åœ¨åå°æ‰§è¡Œè¯„ä¼°ï¼Œä¸ç­‰å¾…ç»“æœ
        """
        if not self.config.async_evaluation:
            # åŒæ­¥æ¨¡å¼ï¼ˆä¸æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒï¼‰
            await self.auto_evaluate(
                query=query,
                retrieved_context=retrieved_context,
                generated_answer=generated_answer,
                session_id=session_id,
                repo_url=repo_url,
                language=language
            )
        else:
            # å¼‚æ­¥æ¨¡å¼ï¼ˆæ¨èï¼‰- åœ¨åå°æ‰§è¡Œ
            asyncio.create_task(
                self._eval_task(
                    query=query,
                    retrieved_context=retrieved_context,
                    generated_answer=generated_answer,
                    session_id=session_id,
                    repo_url=repo_url,
                    language=language
                )
            )
    
    async def _eval_task(
        self,
        query: str,
        retrieved_context: str,
        generated_answer: str,
        session_id: str,
        repo_url: str,
        language: str
    ) -> None:
        """åå°è¯„ä¼°ä»»åŠ¡åŒ…è£…"""
        try:
            await asyncio.sleep(0.1)  # è®©ç”¨æˆ·è¯·æ±‚å…ˆè¿”å›
            await self.auto_evaluate(
                query=query,
                retrieved_context=retrieved_context,
                generated_answer=generated_answer,
                session_id=session_id,
                repo_url=repo_url,
                language=language
            )
        except Exception as e:
            print(f"âŒ Background eval task failed: {e}")
    
    def _compute_final_score(
        self,
        custom_score: float,
        ragas_score: Optional[float]
    ) -> tuple[float, str]:
        """
        è®¡ç®—æœ€ç»ˆå¾—åˆ†å’ŒçŠ¶æ€
        
        Returns:
            (final_score, status)
            status: "normal" / "needs_review" / "high_confidence"
        """
        
        if ragas_score is None:
            # æ²¡æœ‰ Ragas åˆ†æ•°ï¼Œç›´æ¥ç”¨ custom åˆ†æ•°
            return custom_score, "normal"
        
        # è®¡ç®—å·®å¼‚
        diff = abs(custom_score - ragas_score)
        
        # åˆ¤æ–­å¼‚å¸¸
        if diff > self.config.diff_threshold:
            # å·®å¼‚è¿‡å¤§ï¼Œæ ‡è®°ä¸ºéœ€è¦å®¡æŸ¥
            return custom_score, "needs_review"
        
        # æ··åˆè¯„åˆ†
        final_score = (
            self.config.custom_weight * custom_score +
            self.config.ragas_weight * ragas_score
        )
        
        # ä¸¤è€…éƒ½é«˜åˆ† â†’ é«˜ç½®ä¿¡åº¦
        if custom_score > 0.75 and ragas_score > 0.75:
            status = "high_confidence"
        else:
            status = "normal"
        
        return final_score, status
    
    async def _ragas_eval(
        self,
        query: str,
        context: str,
        answer: str
    ) -> tuple[Optional[float], Optional[str]]:
        """
        ä½¿ç”¨ Ragas è¿›è¡Œ sanity check
        
        Returns:
            (score, details)
        """
        try:
            from ragas.metrics import faithfulness, answer_relevancy
            from ragas import evaluate
            
            # æ„é€  Ragas æ•°æ®é›†
            dataset_dict = {
                "question": [query],
                "contexts": [[context]],
                "answer": [answer]
            }
            
            # æ‰§è¡Œè¯„ä¼°
            result = evaluate(
                dataset=dataset_dict,
                metrics=[faithfulness, answer_relevancy]
            )
            
            # æå–åˆ†æ•°
            faithfulness_score = result["faithfulness"][0] if "faithfulness" in result else 0.5
            relevancy_score = result["answer_relevancy"][0] if "answer_relevancy" in result else 0.5
            
            # å¹³å‡å¾—åˆ†
            ragas_score = (faithfulness_score + relevancy_score) / 2
            
            details = f"Ragas: faithfulness={faithfulness_score:.3f}, relevancy={relevancy_score:.3f}"
            
            return ragas_score, details
        
        except ImportError:
            print("âš ï¸ Ragas æœªå®‰è£…ï¼Œè·³è¿‡ sanity check")
            return None, None
        except Exception as e:
            print(f"âš ï¸ Ragas è¯„ä¼°å¼‚å¸¸: {e}")
            return None, None
    
    def get_review_queue(self) -> list:
        """è·å–éœ€è¦å®¡æŸ¥çš„æ ·æœ¬åˆ—è¡¨"""
        return self.needs_review_queue
    
    def clear_review_queue(self) -> None:
        """æ¸…ç©ºå®¡æŸ¥é˜Ÿåˆ—"""
        self.needs_review_queue.clear()
    
    def approve_sample(self, index: int) -> None:
        """äººå·¥æ‰¹å‡†æŸä¸ªæ ·æœ¬"""
        if 0 <= index < len(self.needs_review_queue):
            item = self.needs_review_queue[index]
            # ç›´æ¥å­˜å‚¨åˆ°è¯„ä¼°ç»“æœ
            self.data_router.route_sample(item["eval_result"])
            print(f"âœ… æ ·æœ¬ {index} å·²æ‰¹å‡†")
    
    def reject_sample(self, index: int) -> None:
        """äººå·¥æ‹’ç»æŸä¸ªæ ·æœ¬"""
        if 0 <= index < len(self.needs_review_queue):
            print(f"âŒ æ ·æœ¬ {index} å·²æ‹’ç»")
            self.needs_review_queue.pop(index)


# å…¨å±€å®ä¾‹
auto_eval_service: Optional[AutoEvaluationService] = None


def init_auto_evaluation_service(
    eval_engine: EvaluationEngine,
    data_router: DataRoutingEngine,
    config: EvaluationConfig = None
) -> AutoEvaluationService:
    """åˆå§‹åŒ–è‡ªåŠ¨è¯„ä¼°æœåŠ¡"""
    global auto_eval_service
    auto_eval_service = AutoEvaluationService(
        eval_engine=eval_engine,
        data_router=data_router,
        config=config
    )
    return auto_eval_service


def get_auto_evaluation_service() -> Optional[AutoEvaluationService]:
    """è·å–è‡ªåŠ¨è¯„ä¼°æœåŠ¡å®ä¾‹"""
    return auto_eval_service
