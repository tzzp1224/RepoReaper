# æ–‡ä»¶è·¯å¾„: app/services/agent_service.py
import json
import asyncio
import traceback
import re
import ast
import httpx
import time
from typing import Set, Tuple, List
from datetime import datetime
from app.core.config import settings, agent_config
from app.utils.llm_client import client
from app.utils.repo_lock import RepoLock
from app.services.github_service import get_repo_structure, get_file_content
from app.services.vector_service import store_manager
from app.services.chunking_service import UniversalChunker, ChunkingConfig
from app.services.tracing_service import tracing_service
from evaluation.evaluation_framework import EvaluationEngine, EvaluationResult, DataRoutingEngine

# === Helper: é²æ£’çš„ JSON æå– ===
def extract_json_from_text(text):
    try:
        text = re.sub(r"^```(json)?|```$", "", text.strip(), flags=re.MULTILINE).strip()
        return json.loads(text)
    except:
        pass
    match = re.search(r"\[.*\]", text, re.DOTALL)
    if match:
        try: return json.loads(match.group(0))
        except: pass
    return []

# === å¤šè¯­è¨€ç¬¦å·æå– ===
def _extract_symbols(content, file_path):
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹ï¼Œæ™ºèƒ½æå– Class å’Œ Function ç­¾åç”Ÿæˆåœ°å›¾ã€‚
    """
    ext = file_path.split('.')[-1].lower() if '.' in file_path else ""
    
    # 1. Python ä½¿ç”¨ AST (æœ€å‡†)
    if ext == 'py':
        return _extract_symbols_python(content)
    
    # 2. å…¶ä»–è¯­è¨€ä½¿ç”¨æ­£åˆ™ (Java, TS, JS, Go, C++)
    elif ext in ['java', 'ts', 'tsx', 'js', 'jsx', 'go', 'cpp', 'cs', 'rs']:
        return _extract_symbols_regex(content, ext)
        
    return []

def _extract_symbols_python(content):
    try:
        tree = ast.parse(content)
        symbols = []
        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                symbols.append(f"  [C] {node.name}")
                for sub in node.body:
                    if isinstance(sub, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        if not sub.name.startswith("_") or sub.name == "__init__":
                            symbols.append(f"    - {sub.name}")
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                symbols.append(f"  [F] {node.name}")
        return symbols
    except:
        return []
    
def _extract_symbols_regex(content, ext):
    """
    é’ˆå¯¹ç±» C è¯­è¨€çš„é€šç”¨æ­£åˆ™æå–ã€‚
    """
    symbols = []
    lines = content.split('\n')
    
    # å®šä¹‰å„è¯­è¨€çš„æ­£åˆ™æ¨¡å¼
    patterns = {
        'java': {
            'class': re.compile(r'(?:public|protected|private)?\s*(?:static|abstract)?\s*(?:class|interface|enum)\s+([a-zA-Z0-9_]+)'),
            'func': re.compile(r'(?:public|protected|private)\s+(?:static\s+)?[\w<>[\]]+\s+([a-zA-Z0-9_]+)\s*\(')
        },
        'ts': { 
            'class': re.compile(r'class\s+([a-zA-Z0-9_]+)'),
            'func': re.compile(r'(?:function\s+([a-zA-Z0-9_]+)|const\s+([a-zA-Z0-9_]+)\s*=\s*(?:async\s*)?\(|([a-zA-Z0-9_]+)\s*\([^)]*\)\s*[:\{])') 
        },
        'go': {
            'class': re.compile(r'type\s+([a-zA-Z0-9_]+)\s+(?:struct|interface)'),
            'func': re.compile(r'func\s+(?:(?:\(.*\)\s+)?([a-zA-Z0-9_]+)|([a-zA-Z0-9_]+)\()')
        }
    }
    
    lang_key = 'java' if ext in ['java', 'cs', 'cpp', 'rs'] else 'go' if ext == 'go' else 'ts'
    rules = patterns.get(lang_key, patterns['java'])
    
    count = 0 
    for line in lines:
        line = line.strip()
        # === æ­£åˆ™è§£æä¼˜åŒ– (è¿‡æ»¤æ›´å¤šå¹²æ‰°é¡¹) ===
        if not line or line.startswith(("//", "/*", "*", "#", "print", "console.")): continue
        if count > agent_config.max_symbols_per_file: break

        # åŒ¹é…ç±»
        c_match = rules['class'].search(line)
        if c_match:
            name = next((g for g in c_match.groups() if g), "Unknown")
            symbols.append(f"  [C] {name}")
            count += 1
            continue
            
        # åŒ¹é…æ–¹æ³•
        if line.endswith('{') or "=>" in line: 
            f_match = rules['func'].search(line)
            if f_match:
                name = next((g for g in f_match.groups() if g), None)
                # å¢å¼ºè¿‡æ»¤
                if name and len(name) > 2 and name not in ['if', 'for', 'switch', 'while', 'catch', 'return']:
                    symbols.append(f"    - {name}")
                    count += 1

    return symbols

async def generate_repo_map(repo_url, file_list, limit=agent_config.initial_map_limit) -> Tuple[str, Set[str]]:
    """
    ç”Ÿæˆå¢å¼ºç‰ˆä»“åº“åœ°å›¾ (å¤šè¯­è¨€ç‰ˆ)
    Returns:
        str: åœ°å›¾å­—ç¬¦ä¸²
        set: å·²åŒ…å«åœ¨åœ°å›¾ä¸­çš„æ–‡ä»¶è·¯å¾„é›†åˆ (ç”¨äºå¢é‡æ›´æ–°æŸ¥é‡)
    """
    # === æ‰©å±•é«˜ä¼˜å…ˆçº§æ–‡ä»¶åˆ—è¡¨ (ä½¿ç”¨é…ç½®) ===
    priority_files = [
        f for f in file_list 
        if f.endswith(agent_config.priority_exts) and 
        (f.count('/') <= 2 or any(k in f.lower() for k in agent_config.priority_keywords))
    ]
    
    # å»é‡å¹¶æˆªå–
    targets = sorted(list(set(priority_files)))[:limit]
    remaining = [f for f in file_list if f not in targets]
    
    repo_map_lines = []
    mapped_files_set = set(targets) # === è®°å½•å·²æ˜ å°„çš„æ–‡ä»¶ ===
    
    async def process_file(path):
        content = await get_file_content(repo_url, path)
        if not content: return f"{path} (Read Failed)"
        
        symbols = await asyncio.to_thread(_extract_symbols, content, path)
        
        if symbols:
            return f"{path}\n" + "\n".join(symbols)
        return path

    repo_map_lines.append(f"--- Key Files Structure (Top {len(targets)}) ---")
    
    tasks = [process_file(f) for f in targets]
    results = await asyncio.gather(*tasks)
    repo_map_lines.extend(results)
    
    if remaining:
        repo_map_lines.append("\n--- Other Files ---")
        if len(remaining) > 300:
            repo_map_lines.extend(remaining[:300])
            repo_map_lines.append(f"... ({len(remaining)-300} more files)")
        else:
            repo_map_lines.extend(remaining)
            
    return "\n".join(repo_map_lines), mapped_files_set


async def agent_stream(repo_url: str, session_id: str, language: str = "en", regenerate_only: bool = False):
    """
    ä¸»åˆ†ææµç¨‹ã€‚
    
    Args:
        repo_url: GitHub ä»“åº“ URL
        session_id: ä¼šè¯ ID
        language: æŠ¥å‘Šè¯­è¨€ (zh/en)
        regenerate_only: å¦‚æœä¸º Trueï¼Œè·³è¿‡ç´¢å¼•æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰æ•°æ®ç”Ÿæˆæ–°è¯­è¨€æŠ¥å‘Š
    """
    short_id = session_id[-6:] if session_id else "unknown"
    
    # === è¿½è¸ªåˆå§‹åŒ– ===
    trace_id = tracing_service.start_trace(
        trace_name="agent_analysis",
        session_id=session_id,
        metadata={"repo_url": repo_url, "language": language, "regenerate_only": regenerate_only}
    )
    start_time = time.time()
    
    # === æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç”¨æˆ·æ­£åœ¨åˆ†æåŒä¸€ä»“åº“ ===
    if not regenerate_only:
        if await RepoLock.is_locked(session_id):
            yield json.dumps({
                "step": "waiting", 
                "message": f"â³ Another user is analyzing this repository. Please wait..."
            })
    
    # === è·å–ä»“åº“é” (ä»…å†™æ“ä½œéœ€è¦) ===
    try:
        async with RepoLock.acquire(session_id):
            async for event in _agent_stream_inner(
                repo_url, session_id, language, regenerate_only, 
                short_id, trace_id, start_time
            ):
                yield event
    except TimeoutError as e:
        yield json.dumps({
            "step": "error",
            "message": f"âŒ {str(e)}. The repository is being analyzed by another user."
        })


async def _agent_stream_inner(
    repo_url: str, session_id: str, language: str, regenerate_only: bool,
    short_id: str, trace_id: str, start_time: float
):
    """
    å®é™…çš„åˆ†ææµç¨‹ (åœ¨é”ä¿æŠ¤ä¸‹æ‰§è¡Œ)
    """
    try:
        vector_db = store_manager.get_store(session_id)
        
        # è°ƒè¯•æ—¥å¿—ï¼šç¡®è®¤ session éš”ç¦»
        print(f"ğŸ” [DEBUG] session_id: {session_id}, collection: {vector_db.collection_name}, context_file: {vector_db._context_file}")
        
        # === regenerate_only æ¨¡å¼ï¼šè·³è¿‡ç´¢å¼•ï¼Œç›´æ¥ç”ŸæˆæŠ¥å‘Š ===
        if regenerate_only:
            yield json.dumps({"step": "init", "message": f"ğŸ”„ [Session: {short_id}] Regenerating report in {language}..."})
            await asyncio.sleep(0.3)
            
            # ä»å·²æœ‰ç´¢å¼•åŠ è½½ä¸Šä¸‹æ–‡
            context = vector_db.load_context()
            if not context:
                yield json.dumps({"step": "error", "message": "âŒ No existing index found. Please analyze the repository first."})
                return
            
            # æ­£ç¡®è¯»å– global_context å†…çš„å­—æ®µ
            global_ctx = context.get("global_context", {})
            file_tree_str = global_ctx.get("file_tree", "")
            context_summary = global_ctx.get("summary", "")
            visited_files = set()  # regenerate æ¨¡å¼ä¸éœ€è¦è¿™ä¸ªï¼Œä½†æŠ¥å‘Šç”Ÿæˆéœ€è¦å¼•ç”¨
            
            # éªŒè¯ä¸Šä¸‹æ–‡ä¸è¯·æ±‚çš„ä»“åº“åŒ¹é…
            stored_repo_url = context.get("repo_url", "")
            if stored_repo_url and repo_url not in stored_repo_url and stored_repo_url not in repo_url:
                print(f"âš ï¸ [WARNING] repo_url mismatch! Request: {repo_url}, Stored: {stored_repo_url}")
            
            yield json.dumps({"step": "generating", "message": f"ğŸ“ Generating report in {'Chinese' if language == 'zh' else 'English'}..."})
        else:
            # === æ­£å¸¸åˆ†ææ¨¡å¼ ===
            yield json.dumps({"step": "init", "message": f"ğŸš€ [Session: {short_id}] Connecting to GitHub..."})
            await asyncio.sleep(0.5)
            
            await vector_db.reset()  # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•
            
            chunker = UniversalChunker(config=ChunkingConfig(min_chunk_size=50))

            file_list = await get_repo_structure(repo_url)
            if not file_list:
                raise Exception("Repository is empty or unreadable.")

            yield json.dumps({"step": "fetched", "message": f"ğŸ“¦ Found {len(file_list)} files. Building Repo Map (AST Parsing)..."})        
            
            # === æ¥æ”¶ mapped_files ç”¨äºåç»­æŸ¥é‡ + è®¡æ—¶ ===
            map_start = time.time()
            file_tree_str, mapped_files = await generate_repo_map(repo_url, file_list, limit=agent_config.initial_map_limit)
            map_latency_ms = (time.time() - map_start) * 1000
            tracing_service.add_event("repo_map_generated", {"latency_ms": map_latency_ms, "files_mapped": len(mapped_files)})
            
            visited_files = set()
            context_summary = ""
            readme_file = next((f for f in file_list if f.lower().endswith("readme.md")), None)

            for round_idx in range(agent_config.max_rounds):
                yield json.dumps({"step": "thinking", "message": f"ğŸ•µï¸ [Round {round_idx+1}/{agent_config.max_rounds}] DeepSeek is analyzing Repo Map..."})
                
                system_prompt = "You are a Senior Software Architect. Your goal is to understand the codebase."
                user_content = f"""
                [Project Repo Map]
                (Contains file paths and key Class/Function signatures)
                {file_tree_str}
                
                [Files Already Read]
                {list(visited_files)}
                
                [Current Knowledge]
                {context_summary}
                
                [Task]
                Select 1-{agent_config.files_per_round} MOST CRITICAL files to read next to understand the core logic.
                Focus on files that seem to contain main logic based on the Repo Map symbols.
                
                [Constraint]
                Return ONLY a raw JSON list of strings. No markdown.
                Example: ["src/main.py", "app/auth.py"]
                """
                
                if not client:
                     yield json.dumps({"step": "error", "message": "âŒ LLM Client Not Initialized."})
                     return
                
                # === Token & Latency Tracing ===
                llm_start_time = time.time()
                plan_messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ]
                
                response = await client.chat.completions.create(
                    model=settings.default_model_name,
                    messages=plan_messages,
                    temperature=0.1,
                    timeout=settings.LLM_TIMEOUT 
                )
                
                llm_latency_ms = (time.time() - llm_start_time) * 1000
                raw_content = response.choices[0].message.content
                
                # è®°å½• Token ä½¿ç”¨é‡
                usage = getattr(response, 'usage', None)
                tracing_service.record_llm_generation(
                    model=settings.default_model_name,
                    prompt_messages=plan_messages,
                    generated_text=raw_content,
                    total_latency_ms=llm_latency_ms,
                    prompt_tokens=usage.prompt_tokens if usage else None,
                    completion_tokens=usage.completion_tokens if usage else None,
                    total_tokens=usage.total_tokens if usage else None,
                    is_streaming=False,
                    metadata={"step": "file_selection", "round": round_idx + 1}
                )
                target_files = extract_json_from_text(raw_content)

                valid_files = [f for f in target_files if f in file_list and f not in visited_files]

                if round_idx == 0 and readme_file and readme_file not in visited_files and readme_file not in valid_files:
                    valid_files.insert(0, readme_file)

                if not valid_files:
                    yield json.dumps({"step": "plan", "message": f"ğŸ›‘ [Round {round_idx+1}] Sufficient context gathered."})
                    break
                
                yield json.dumps({"step": "plan", "message": f"ğŸ‘‰ [Round {round_idx+1}] Selected: {valid_files}"})
                
                # === å¹¶å‘æ¨¡å‹ç¼ºé™·ä¼˜åŒ– (å¹¶è¡Œä¸‹è½½å¤„ç†) ===
                async def process_single_file(file_path):
                    try:
                        file_start = time.time()
                        
                        # ğŸ”§ å¼‚æ­¥ GitHub API (å·²ä¼˜åŒ–ä¸ºéé˜»å¡)
                        content = await get_file_content(repo_url, file_path)
                        if not content: 
                            tracing_service.add_event("file_read_failed", {"file": file_path})
                            return None

                        # 1. æ‘˜è¦ä¸ Context
                        lines = content.split('\n')[:50]
                        preview = "\n".join(lines)
                        file_knowledge = f"\n--- File: {file_path} ---\n{preview}\n"
                        
                        # 2. Repo Map å¢é‡æ›´æ–°ä¸æŸ¥é‡
                        new_map_entry = None
                        if file_path not in mapped_files:
                            symbols = await asyncio.to_thread(_extract_symbols, content, file_path)
                            if symbols:
                                new_map_entry = f"{file_path}\n" + "\n".join(symbols)

                        # 3. åˆ‡ç‰‡ä¸å…¥åº“
                        chunks = await asyncio.to_thread(chunker.chunk_file, content, file_path)
                        if chunks:
                            documents = [c["content"] for c in chunks]
                            metadatas = []
                            for c in chunks:
                                meta = c["metadata"]
                                metadatas.append({
                                    "file": meta["file"],
                                    "type": meta["type"],
                                    "name": meta.get("name", ""),
                                    "class": meta.get("class") or ""
                                })
                            if documents:
                                try:
                                    await vector_db.add_documents(documents, metadatas)
                                except Exception as e:
                                    print(f"âŒ ç´¢å¼•é”™è¯¯ {file_path}: {e}")
                                    # ä¸ä¸­æ–­ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
                                    return None
                        
                        file_latency_ms = (time.time() - file_start) * 1000
                        tracing_service.add_event("file_processed", {
                            "file": file_path,
                            "latency_ms": file_latency_ms,
                            "chunks_count": len(chunks) if chunks else 0
                        })

                        return {
                            "path": file_path,
                            "knowledge": file_knowledge,
                            "map_entry": new_map_entry
                        }
                    except Exception as e:
                        print(f"âŒ å¤„ç†æ–‡ä»¶é”™è¯¯ {file_path}: {e}")
                        return None

                # æç¤ºå¼€å§‹å¹¶å‘ä¸‹è½½
                yield json.dumps({"step": "download", "message": f"ğŸ“¥ Starting parallel download for {len(valid_files)} files..."})

                # å¯åŠ¨å¹¶å‘ä»»åŠ¡ (return_exceptions=True é˜²æ­¢å•ä¸ªå¤±è´¥å¯¼è‡´æ•´ä¸ªä¸­æ–­)
                tasks = [process_single_file(f) for f in valid_files]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # èšåˆç»“æœ
                download_count = 0
                for res in results:
                    if not res or isinstance(res, Exception): 
                        if isinstance(res, Exception):
                            print(f"âŒ Task å¼‚å¸¸: {res}")
                        continue
                    download_count += 1
                    visited_files.add(res["path"])
                    context_summary += res["knowledge"]
                    
                    # å¢é‡æ›´æ–° Map
                    if res["map_entry"]:
                        file_tree_str = f"{res['map_entry']}\n\n{file_tree_str}"
                        mapped_files.add(res["path"])
                
                # === ç¡¬ç¼–ç æˆªæ–­è§£è€¦ ===
                context_summary = context_summary[:agent_config.max_context_length]
                
                global_context_data = {
                    "file_tree": file_tree_str,
                    "summary": context_summary[:8000]
                }
                await vector_db.save_context(repo_url, global_context_data)
                
                yield json.dumps({"step": "indexing", "message": f"ğŸ§  [Round {round_idx+1}] Processed {download_count} files. Knowledge graph updated."})

            # Final Report (æ­£å¸¸åˆ†ææ¨¡å¼ä¸‹çš„æç¤º)
            yield json.dumps({"step": "generating", "message": "ğŸ“ Generating technical report..."})
        
        # === æŠ¥å‘Šç”Ÿæˆ (ä¸¤ç§æ¨¡å¼å…±ç”¨) ===
        
        # === P0: å‘é‡æ£€ç´¢è¡¥å……å…³é”®ä»£ç ç‰‡æ®µ ===
        yield json.dumps({"step": "enriching", "message": "ğŸ” Retrieving key code snippets..."})
        
        key_queries = [
            "main entry point initialization startup",
            "core business logic handler processor",
            "API routes endpoints controllers",
            "database models schema ORM",
            "authentication authorization middleware"
        ]
        
        retrieved_snippets = []
        try:
            await vector_db.initialize()
            for query in key_queries:
                results = await vector_db.search_hybrid(query, top_k=2)
                for r in results:
                    snippet = r.get("content", "")[:400]
                    file_path = r.get("file", "unknown")
                    if snippet and snippet not in [s.split("]")[1] if "]" in s else s for s in retrieved_snippets]:
                        retrieved_snippets.append(f"[{file_path}]\n{snippet}")
        except Exception as e:
            print(f"âš ï¸ å‘é‡æ£€ç´¢å¤±è´¥: {e}")
        
        code_snippets_section = "\n\n".join(retrieved_snippets[:8]) if retrieved_snippets else ""
        
        # === P1: ä¾èµ–æ–‡ä»¶è§£æ ===
        dep_files = ["requirements.txt", "pyproject.toml", "package.json", "go.mod", "Cargo.toml", "pom.xml", "build.gradle"]
        dependencies_info = ""
        
        # è·å– file_listï¼ˆregenerate_only æ¨¡å¼ä¸‹éœ€è¦é‡æ–°è·å–ï¼‰
        if regenerate_only:
            try:
                temp_file_list = await get_repo_structure(repo_url)
            except:
                temp_file_list = []
        else:
            temp_file_list = file_list if 'file_list' in dir() else []
        
        for dep_file in dep_files:
            matching = [f for f in temp_file_list if f.endswith(dep_file)]
            for f in matching[:1]:  # åªå–ç¬¬ä¸€ä¸ªåŒ¹é…
                try:
                    content = await get_file_content(repo_url, f)
                    if content:
                        dependencies_info += f"\n[{f}]\n{content[:800]}\n"
                except:
                    pass
        
        # æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡
        enhanced_context = f"""
        {context_summary[:12000]}
        
        [Key Code Snippets (Retrieved by Semantic Search)]
        {code_snippets_section}
        
        [Project Dependencies]
        {dependencies_info if dependencies_info else "No dependency file found."}
        """

        repo_map_injection = f"""
        [Project Repo Map (Structure)]
        {file_tree_str}
        """

        # === æ ¹æ®è¯­è¨€é€‰æ‹© Prompt ===
        if language == "zh":
            # --- ä¸­æ–‡ Prompt ---
            system_role = "ä½ æ˜¯ä¸€ä½åŠ¡å®çš„æŠ€æœ¯ä¸“å®¶ã€‚ç›®æ ‡æ˜¯ä¸ºå¼€å‘è€…åˆ›å»ºä¸€ä¸ª'3é¡µçº¸'æ¶æ„æ¦‚è§ˆï¼Œè®©ä»–ä»¬èƒ½åœ¨5åˆ†é’Ÿå†…çœ‹æ‡‚è¿™ä¸ªä»“åº“ã€‚é‡ç‚¹å…³æ³¨æ¶æ„å’Œæ•°æ®æµï¼Œä¸è¦çº ç»“ç»†èŠ‚ã€‚"
            analysis_user_content = f"""
            [è§’è‰²]
            ä½ æ˜¯ä¸€ä½åŠ¡å®çš„æŠ€æœ¯ä¸“å®¶ï¼ˆTech Leadï¼‰ã€‚
            
            [è¾“å…¥æ•°æ®]
            {repo_map_injection}

            åˆ†æçš„æ–‡ä»¶: {list(visited_files)}
            
            [ä»£ç çŸ¥è¯†åº“ä¸å…³é”®ç‰‡æ®µ]
            {enhanced_context}
            
            [ä¸¥æ ¼é™åˆ¶]
            1. **ä¸è¿›è¡Œä»£ç å®¡æŸ¥**: ä¸è¦åˆ—å‡º Bugã€ç¼ºå¤±åŠŸèƒ½æˆ–æ”¹è¿›å»ºè®®ã€‚
            2. **ä¸è¯„ä»·**: ä¸è¦è¯„ä»·ä»£ç è´¨é‡ï¼Œåªæè¿°å®ƒ**å¦‚ä½•å·¥ä½œ**ã€‚
            3. **è¯­è°ƒ**: ä¸“ä¸šã€ç»“æ„åŒ–ã€æè¿°æ€§ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
            4. **ä¸è¦åºŸè¯**: ä¸è¦å†™"å®‰å…¨æ€§"ã€"æœªæ¥è§„åˆ’"ç­‰æœªè¯·æ±‚çš„ç« èŠ‚ã€‚

            [è¾“å‡ºæ ¼å¼è¦æ±‚ (Markdown)]
            
            # é¡¹ç›®åˆ†ææŠ¥å‘Š

            ## 1. æ‰§è¡Œæ‘˜è¦ (Executive Summary)
            - **ç”¨é€”**: (è¿™ä¸ªé¡¹ç›®å…·ä½“è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ1-2å¥è¯)
            - **æ ¸å¿ƒåŠŸèƒ½**: (åˆ—å‡ºTop 3åŠŸèƒ½ç‚¹)
            - **æŠ€æœ¯æ ˆ**: (è¯­è¨€ã€æ¡†æ¶ã€æ•°æ®åº“ã€å…³é”®åº“)

            ## 2. ç³»ç»Ÿæ¶æ„ (Mermaid)
            åˆ›å»ºä¸€ä¸ª `graph TD` å›¾ã€‚
            - å±•ç¤ºé«˜å±‚ç»„ä»¶ (å¦‚ Client, API Server, Database, Worker, External Service)ã€‚
            - åœ¨è¿çº¿ä¸Šæ ‡æ³¨æ•°æ®æµ (å¦‚ "HTTP/JSON", "SQL")ã€‚
            - **é£æ ¼**: ä¿æŒæ¦‚å¿µæ¸…æ™°ç®€å•ã€‚
            - **é‡è¦**: æ‰€æœ‰èŠ‚ç‚¹æ–‡æœ¬å¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹ï¼Œä¾‹å¦‚: `A["ç”¨æˆ·ç•Œé¢"] --> B["APIæœåŠ¡"]`
            - **ç¤ºä¾‹**:
            ```mermaid
            graph TD
                A["å®¢æˆ·ç«¯"] -->|"HTTPè¯·æ±‚"| B["APIç½‘å…³"]
                B --> C["ä¸šåŠ¡æœåŠ¡"]
                C --> D[("æ•°æ®åº“")]
            ```

            ## 3. æ ¸å¿ƒé€»è¾‘åˆ†æ (Table)
            (æ€»ç»“å…³é”®æ¨¡å—ï¼Œä¸è¦åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ï¼Œåªåˆ—æœ€é‡è¦çš„)

            | ç»„ä»¶/æ–‡ä»¶ | èŒè´£ (å®ƒåšä»€ä¹ˆï¼Ÿ) | å…³é”®è®¾è®¡æ¨¡å¼/é€»è¾‘ |
            | :--- | :--- | :--- |
            | ä¾‹å¦‚ `auth_service.py` | å¤„ç†JWTé¢å‘ä¸éªŒè¯ | å•ä¾‹æ¨¡å¼, è·¯ç”±è£…é¥°å™¨ |
            | ... | ... | ... |

            ## 4. ğŸ”¬ æ ¸å¿ƒæ–¹æ³•æ·±åº¦è§£æ
            (ç²¾é€‰ 3-5 ä¸ªæœ€å…³é”®çš„ `.py` æ–‡ä»¶ã€‚é’ˆå¯¹æ¯ä¸ªæ–‡ä»¶ï¼Œåˆ—å‡ºé©±åŠ¨é€»è¾‘çš„ Top 2-3 ä¸ªæ–¹æ³•)

            ### 4.1 `[æ–‡ä»¶å]`
            * **`[æ–¹æ³•å]`**: [è§£é‡Šå®ƒåšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆé‡è¦ï¼Œä¸è¦è´´ä»£ç ]
            * **`[æ–¹æ³•å]`**: [è§£é‡Š...]

            ## 5. ä¸»è¦å·¥ä½œæµ (Mermaid)
            é€‰æ‹©**ä¸€ä¸ªæœ€é‡è¦**çš„ä¸šåŠ¡æµç¨‹ (Happy Path)ã€‚
            åˆ›å»ºä¸€ä¸ª `sequenceDiagram`ã€‚
            - å‚ä¸è€…åº”è¯¥æ˜¯é«˜å±‚æ¦‚å¿µ (å¦‚ User, API, DB)ï¼Œä¸è¦ç”¨å…·ä½“å˜é‡åã€‚
            - **é‡è¦**: å‚ä¸è€…åç§°ç”¨è‹±æ–‡ï¼Œæ¶ˆæ¯å†…å®¹å¯ä»¥ç”¨ä¸­æ–‡ä½†å¿…é¡»ç”¨åŒå¼•å·åŒ…è£¹ã€‚
            - **ç¤ºä¾‹**:
            ```mermaid
            sequenceDiagram
                participant User as ç”¨æˆ·
                participant API as APIæœåŠ¡
                participant DB as æ•°æ®åº“
                User->>API: "å‘èµ·è¯·æ±‚"
                API->>DB: "æŸ¥è¯¢æ•°æ®"
                DB-->>API: "è¿”å›ç»“æœ"
                API-->>User: "å“åº”æ•°æ®"
            ```
            
            ## 6. å¿«é€Ÿå¼€å§‹ (Quick Start)
            - **å‰ç½®æ¡ä»¶**: (å¦‚ Docker, Python 3.9+, .env é…ç½®)
            - **å…¥å£**: (å¦‚ä½•å¯åŠ¨ä¸»é€»è¾‘ï¼Ÿå¦‚ `python main.py`)
            """
        else:
            analysis_user_content = f"""
            [Role]
            You are a **Pragmatic Tech Lead**. Your goal is to create a **"3-Pages" Architecture Overview** for a developer who wants to understand this repo in 5 minutes.
            [Input Data]
            {repo_map_injection}

            Files analyzed: {list(visited_files)}
            
            [Code Knowledge & Key Snippets]
            {enhanced_context}
            
            [Strict Constraints]
            1. **NO Code Review**: Do NOT list bugs, issues, missing features, or recommendations.
            2. **NO Critique**: Do not judge the code quality. Focus on HOW it works.
            3. **Tone**: Professional, descriptive, and structural.
            4. **NO "FLUFF"**: Do NOT add unrequested sections like "Security", "Scalability", "Data Models", "Future Enhancements", etc.

            [Required Output Format (Markdown)]
            
            # Project Analysis Report

            ## 1. Executive Summary
            - **Purpose**: (What specific problem does this project solve? 1-2 sentences)
            - **Key Features**: (Bullet points of top 3 features)
            - **Tech Stack**: (List languages, frameworks, databases, and key libs)

            ## 2. System Architecture
            Create a `graph TD` diagram.
            - Show high-level components (e.g., Client, API Server, Database, Worker, External Service).
            - Label the edges with data flow (e.g., "HTTP/JSON", "SQL").
            - **Style**: Keep it simple and conceptual.

            ## 3. Core Logic Analysis
            (Create a Markdown Table to summarize key modules. Do not list every file, only the most important ones.)

            | Component/File | Responsibility (What does it do?) | Key Design Pattern / Logic |
            | :--- | :--- | :--- |
            | e.g. `auth_service.py` | Handles JWT issuance and verification | Singleton, Decorator for routes |
            | ... | ... | ... |

            ## 4. Core Methods Deep Dive
            (Select the 3-5 most critical `.py` files. For each, list the top 2-3 methods that drive the logic.)

            ### 4.1 `[Filename, e.g., agent_service.py]`
            * **`[Method Name]`**: [Explanation of what it does and why it matters. No code.]
            * **`[Method Name]`**: [Explanation...]

            ### 4.2 `[Filename, e.g., vector_service.py]`
            * **`[Method Name]`**: [Explanation...]
            * ...

            ## 5. Main Workflow (Mermaid)
            Select the **Single Most Important** business flow (The "Happy Path").
            Create a `sequenceDiagram`.
            - Participants should be high-level (e.g., User, API, DB), not specific variable names.
            
            ## 6. Quick Start Guide
            - **Prerequisites**: (e.g. Docker, Python 3.9+, .env file)
            - **Entry Point**: (How to run the main logic? e.g. `python main.py` or `uvicorn`)

            """
        
        # === å¢åŠ  timeout é˜²æ­¢é•¿æ–‡æœ¬ç”Ÿæˆæ—¶æ–­è¿ ===
        report_messages = [
            {"role": "system", "content": "You are a pragmatic Tech Lead. Focus on architecture and data flow, not implementation details."},
            {"role": "user", "content": analysis_user_content}
        ]
        
        stream_start_time = time.time()
        stream = await client.chat.completions.create(
            model=settings.default_model_name,
            messages=report_messages,
            stream=True,
            timeout=settings.LLM_TIMEOUT  # ä½¿ç”¨ç»Ÿä¸€é…ç½®
        )
        
        # === TTFT & Token Tracking ===
        first_token_received = False
        ttft_ms = None
        generated_text = ""
        completion_tokens_estimate = 0
        
        # === å¢åŠ  try-except æ•è·æµå¼ä¼ è¾“ä¸­æ–­ ===
        try:
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    
                    # è®°å½• TTFT (é¦– Token æ—¶é—´)
                    if not first_token_received:
                        ttft_ms = (time.time() - stream_start_time) * 1000
                        tracing_service.record_ttft(
                            ttft_ms=ttft_ms,
                            model=settings.default_model_name,
                            metadata={"step": "report_generation"}
                        )
                        first_token_received = True
                    
                    generated_text += content
                    completion_tokens_estimate += 1  # ç²—ç•¥ä¼°è®¡æ¯ä¸ª chunk çº¦ 1 token
                    yield json.dumps({"step": "report_chunk", "chunk": content})
        except (httpx.ReadError, httpx.ConnectError) as e:
            yield json.dumps({"step": "error", "message": f"âš ï¸ Network Timeout during generation: {str(e)}"})
            return
        
        # æµç»“æŸåè®°å½•å®Œæ•´çš„ LLM ç”Ÿæˆä¿¡æ¯
        total_latency_ms = (time.time() - stream_start_time) * 1000
        tracing_service.record_llm_generation(
            model=settings.default_model_name,
            prompt_messages=report_messages,
            generated_text=generated_text,
            ttft_ms=ttft_ms,
            total_latency_ms=total_latency_ms,
            completion_tokens=completion_tokens_estimate,
            is_streaming=True,
            metadata={"step": "report_generation", "generated_chars": len(generated_text)}
        )
        
        # === ä¿å­˜æŠ¥å‘Š (æŒ‰è¯­è¨€å­˜å‚¨ï¼Œå¼‚æ­¥é¿å…é˜»å¡) ===
        await vector_db.save_report(generated_text, language)

        yield json.dumps({"step": "finish", "message": "âœ… Analysis Complete!"})

    except Exception as e:
        # === å…¨å±€å¼‚å¸¸æ•è· ===
        import traceback
        traceback.print_exc()
        
        # æå–å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "401" in error_msg:
            ui_msg = "âŒ GitHub Token Invalid. Please check your settings."
        elif "403" in error_msg:
            ui_msg = "âŒ GitHub API Rate Limit Exceeded. Try again later or add a Token."
        elif "404" in error_msg:
            ui_msg = "âŒ Repository Not Found. Check the URL."
        elif "Timeout" in error_msg or "ConnectError" in error_msg:
            ui_msg = "âŒ Network Timeout. LLM or GitHub is not responding."
        else:
            ui_msg = f"ğŸ’¥ System Error: {error_msg}"
            
        yield json.dumps({"step": "error", "message": ui_msg})
        return # ç»ˆæ­¢æµ