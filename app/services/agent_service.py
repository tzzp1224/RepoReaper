# æ–‡ä»¶è·¯å¾„: app/services/agent_service.py
import json
import asyncio
import traceback
import re
import ast
import httpx
from typing import Set, Tuple, List
from app.core.config import settings
from app.utils.llm_client import client
from app.services.github_service import get_repo_structure, get_file_content
from app.services.vector_service import store_manager
from app.services.chunking_service import UniversalChunker

# === ç¡¬ç¼–ç é…ç½®è§£è€¦ ===
class AgentConfig:
    INITIAL_MAP_LIMIT = 15
    MAX_ROUNDS = 3
    MAX_CONTEXT_LENGTH = 15000
    LLM_TIMEOUT = 600
    FILES_PER_ROUND = 3
    # æ‰©å±•çš„ä¼˜å…ˆçº§åˆ—è¡¨
    PRIORITY_EXTS = ('.py', '.java', '.go', '.js', '.ts', '.tsx', '.cpp', '.cs', '.rs')
    PRIORITY_KEYWORDS = ['main', 'app', 'core', 'api', 'service', 'utils', 'controller', 'model', 'config']

# === Helper: é²æ£’çš„ JSON æå– ===
def extract_json_from_text(text):
    try:
        text = re.sub(r"^```(json)?|```$", "", text.strip(), flags=re.MULTILINE).strip()
        return json.loads(text)
    except:
        pass
    match = re.search(r"\[.*\]", text, re.DOTALL)
    if match:
        try: return json.loads(match.group(0))
        except: pass
    return []

# === å¤šè¯­è¨€ç¬¦å·æå– ===
def _extract_symbols(content, file_path):
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹ï¼Œæ™ºèƒ½æå– Class å’Œ Function ç­¾åç”Ÿæˆåœ°å›¾ã€‚
    """
    ext = file_path.split('.')[-1].lower() if '.' in file_path else ""
    
    # 1. Python ä½¿ç”¨ AST (æœ€å‡†)
    if ext == 'py':
        return _extract_symbols_python(content)
    
    # 2. å…¶ä»–è¯­è¨€ä½¿ç”¨æ­£åˆ™ (Java, TS, JS, Go, C++)
    elif ext in ['java', 'ts', 'tsx', 'js', 'jsx', 'go', 'cpp', 'cs', 'rs']:
        return _extract_symbols_regex(content, ext)
        
    return []

def _extract_symbols_python(content):
    try:
        tree = ast.parse(content)
        symbols = []
        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                symbols.append(f"  [C] {node.name}")
                for sub in node.body:
                    if isinstance(sub, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        if not sub.name.startswith("_") or sub.name == "__init__":
                            symbols.append(f"    - {sub.name}")
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                symbols.append(f"  [F] {node.name}")
        return symbols
    except:
        return []
    
def _extract_symbols_regex(content, ext):
    """
    é’ˆå¯¹ç±» C è¯­è¨€çš„é€šç”¨æ­£åˆ™æå–ã€‚
    """
    symbols = []
    lines = content.split('\n')
    
    # å®šä¹‰å„è¯­è¨€çš„æ­£åˆ™æ¨¡å¼
    patterns = {
        'java': {
            'class': re.compile(r'(?:public|protected|private)?\s*(?:static|abstract)?\s*(?:class|interface|enum)\s+([a-zA-Z0-9_]+)'),
            'func': re.compile(r'(?:public|protected|private)\s+(?:static\s+)?[\w<>[\]]+\s+([a-zA-Z0-9_]+)\s*\(')
        },
        'ts': { 
            'class': re.compile(r'class\s+([a-zA-Z0-9_]+)'),
            'func': re.compile(r'(?:function\s+([a-zA-Z0-9_]+)|const\s+([a-zA-Z0-9_]+)\s*=\s*(?:async\s*)?\(|([a-zA-Z0-9_]+)\s*\([^)]*\)\s*[:\{])') 
        },
        'go': {
            'class': re.compile(r'type\s+([a-zA-Z0-9_]+)\s+(?:struct|interface)'),
            'func': re.compile(r'func\s+(?:(?:\(.*\)\s+)?([a-zA-Z0-9_]+)|([a-zA-Z0-9_]+)\()')
        }
    }
    
    lang_key = 'java' if ext in ['java', 'cs', 'cpp', 'rs'] else 'go' if ext == 'go' else 'ts'
    rules = patterns.get(lang_key, patterns['java'])
    
    count = 0 
    for line in lines:
        line = line.strip()
        # === æ­£åˆ™è§£æä¼˜åŒ– (è¿‡æ»¤æ›´å¤šå¹²æ‰°é¡¹) ===
        if not line or line.startswith(("//", "/*", "*", "#", "print", "console.")): continue
        if count > 30: break # å•æ–‡ä»¶é™åˆ¶

        # åŒ¹é…ç±»
        c_match = rules['class'].search(line)
        if c_match:
            name = next((g for g in c_match.groups() if g), "Unknown")
            symbols.append(f"  [C] {name}")
            count += 1
            continue
            
        # åŒ¹é…æ–¹æ³•
        if line.endswith('{') or "=>" in line: 
            f_match = rules['func'].search(line)
            if f_match:
                name = next((g for g in f_match.groups() if g), None)
                # å¢å¼ºè¿‡æ»¤
                if name and len(name) > 2 and name not in ['if', 'for', 'switch', 'while', 'catch', 'return']:
                    symbols.append(f"    - {name}")
                    count += 1

    return symbols

async def generate_repo_map(repo_url, file_list, limit=AgentConfig.INITIAL_MAP_LIMIT) -> Tuple[str, Set[str]]:
    """
    ç”Ÿæˆå¢å¼ºç‰ˆä»“åº“åœ°å›¾ (å¤šè¯­è¨€ç‰ˆ)
    Returns:
        str: åœ°å›¾å­—ç¬¦ä¸²
        set: å·²åŒ…å«åœ¨åœ°å›¾ä¸­çš„æ–‡ä»¶è·¯å¾„é›†åˆ (ç”¨äºå¢é‡æ›´æ–°æŸ¥é‡)
    """
    # === æ‰©å±•é«˜ä¼˜å…ˆçº§æ–‡ä»¶åˆ—è¡¨ (ä½¿ç”¨é…ç½®) ===
    priority_files = [
        f for f in file_list 
        if f.endswith(AgentConfig.PRIORITY_EXTS) and 
        (f.count('/') <= 2 or any(k in f.lower() for k in AgentConfig.PRIORITY_KEYWORDS))
    ]
    
    # å»é‡å¹¶æˆªå–
    targets = sorted(list(set(priority_files)))[:limit]
    remaining = [f for f in file_list if f not in targets]
    
    repo_map_lines = []
    mapped_files_set = set(targets) # === è®°å½•å·²æ˜ å°„çš„æ–‡ä»¶ ===
    
    async def process_file(path):
        content = await asyncio.to_thread(get_file_content, repo_url, path)
        if not content: return f"{path} (Read Failed)"
        
        symbols = await asyncio.to_thread(_extract_symbols, content, path)
        
        if symbols:
            return f"{path}\n" + "\n".join(symbols)
        return path

    repo_map_lines.append(f"--- Key Files Structure (Top {len(targets)}) ---")
    
    tasks = [process_file(f) for f in targets]
    results = await asyncio.gather(*tasks)
    repo_map_lines.extend(results)
    
    if remaining:
        repo_map_lines.append("\n--- Other Files ---")
        if len(remaining) > 300:
            repo_map_lines.extend(remaining[:300])
            repo_map_lines.append(f"... ({len(remaining)-300} more files)")
        else:
            repo_map_lines.extend(remaining)
            
    return "\n".join(repo_map_lines), mapped_files_set


async def agent_stream(repo_url: str, session_id: str, language: str = "en"):
    short_id = session_id[-6:] if session_id else "unknown"
    yield json.dumps({"step": "init", "message": f"ğŸš€ [Session: {short_id}] Connecting to GitHub..."})
    await asyncio.sleep(0.5)
    
    try:
        vector_db = store_manager.get_store(session_id)
        vector_db.reset_collection() 
        
        chunker = UniversalChunker(min_chunk_size=50)

        file_list = await asyncio.to_thread(get_repo_structure, repo_url)
        if not file_list:
            raise Exception("Repository is empty or unreadable.")

        yield json.dumps({"step": "fetched", "message": f"ğŸ“¦ Found {len(file_list)} files. Building Repo Map (AST Parsing)..."})        
        
        # === æ¥æ”¶ mapped_files ç”¨äºåç»­æŸ¥é‡ ===
        file_tree_str, mapped_files = await generate_repo_map(repo_url, file_list, limit=AgentConfig.INITIAL_MAP_LIMIT)
        
        visited_files = set()
        context_summary = ""
        readme_file = next((f for f in file_list if f.lower().endswith("readme.md")), None)

        for round_idx in range(AgentConfig.MAX_ROUNDS):
            yield json.dumps({"step": "thinking", "message": f"ğŸ•µï¸ [Round {round_idx+1}/{AgentConfig.MAX_ROUNDS}] DeepSeek is analyzing Repo Map..."})
            
            system_prompt = "You are a Senior Software Architect. Your goal is to understand the codebase."
            user_content = f"""
            [Project Repo Map]
            (Contains file paths and key Class/Function signatures)
            {file_tree_str}
            
            [Files Already Read]
            {list(visited_files)}
            
            [Current Knowledge]
            {context_summary}
            
            [Task]
            Select 1-{AgentConfig.FILES_PER_ROUND} MOST CRITICAL files to read next to understand the core logic.
            Focus on files that seem to contain main logic based on the Repo Map symbols.
            
            [Constraint]
            Return ONLY a raw JSON list of strings. No markdown.
            Example: ["src/main.py", "app/auth.py"]
            """
            
            if not client:
                 yield json.dumps({"step": "error", "message": "âŒ LLM Client Not Initialized."})
                 return
            
            response = await client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.1,
                timeout=AgentConfig.LLM_TIMEOUT 
            )
            
            raw_content = response.choices[0].message.content
            target_files = extract_json_from_text(raw_content)

            valid_files = [f for f in target_files if f in file_list and f not in visited_files]

            if round_idx == 0 and readme_file and readme_file not in visited_files and readme_file not in valid_files:
                valid_files.insert(0, readme_file)

            if not valid_files:
                yield json.dumps({"step": "plan", "message": f"ğŸ›‘ [Round {round_idx+1}] Sufficient context gathered."})
                break
            
            yield json.dumps({"step": "plan", "message": f"ğŸ‘‰ [Round {round_idx+1}] Selected: {valid_files}"})
            
            # === å¹¶å‘æ¨¡å‹ç¼ºé™·ä¼˜åŒ– (å¹¶è¡Œä¸‹è½½å¤„ç†) ===
            async def process_single_file(file_path):

                # å¦‚æœéå¸¸éœ€è¦åœ¨UIæ˜¾ç¤ºä¸‹è½½è¿›åº¦ï¼Œåªèƒ½åœ¨å¤–éƒ¨æ¨¡æ‹Ÿï¼Œæˆ–è€…å¼•å…¥Queueï¼Œä½†åœ¨ gather ä¸­æœ€ç®€å•çš„åŠæ³•æ˜¯å»æ‰å®ƒ
                content = get_file_content(repo_url, file_path)
                if not content: return None

                # 1. æ‘˜è¦ä¸ Context
                lines = content.split('\n')[:50]
                preview = "\n".join(lines)
                file_knowledge = f"\n--- File: {file_path} ---\n{preview}\n"
                
                # 2. Repo Map å¢é‡æ›´æ–°ä¸æŸ¥é‡
                new_map_entry = None
                if file_path not in mapped_files:
                    symbols = await asyncio.to_thread(_extract_symbols, content, file_path)
                    if symbols:
                        new_map_entry = f"{file_path}\n" + "\n".join(symbols)

                # 3. åˆ‡ç‰‡ä¸å…¥åº“
                chunks = await asyncio.to_thread(chunker.chunk_file, content, file_path)
                if chunks:
                    documents = [c["content"] for c in chunks]
                    metadatas = []
                    for c in chunks:
                        meta = c["metadata"]
                        metadatas.append({
                            "file": meta["file"],
                            "type": meta["type"],
                            "name": meta.get("name", ""),
                            "class": meta.get("class") or ""
                        })
                    if documents:
                        await vector_db.add_documents(documents, metadatas)

                return {
                    "path": file_path,
                    "knowledge": file_knowledge,
                    "map_entry": new_map_entry
                }

            # æç¤ºå¼€å§‹å¹¶å‘ä¸‹è½½
            yield json.dumps({"step": "download", "message": f"ğŸ“¥ Starting parallel download for {len(valid_files)} files..."})

            # å¯åŠ¨å¹¶å‘ä»»åŠ¡
            tasks = [process_single_file(f) for f in valid_files]
            results = await asyncio.gather(*tasks)

            # èšåˆç»“æœ
            download_count = 0
            for res in results:
                if not res: continue
                download_count += 1
                visited_files.add(res["path"])
                context_summary += res["knowledge"]
                
                # å¢é‡æ›´æ–° Map
                if res["map_entry"]:
                    file_tree_str = f"{res['map_entry']}\n\n{file_tree_str}"
                    mapped_files.add(res["path"])
            
            # === ç¡¬ç¼–ç æˆªæ–­è§£è€¦ ===
            context_summary = context_summary[:AgentConfig.MAX_CONTEXT_LENGTH]
            
            global_context_data = {
                "file_tree": file_tree_str,
                "summary": context_summary[:8000]
            }
            vector_db.save_context(repo_url, global_context_data)
            
            yield json.dumps({"step": "indexing", "message": f"ğŸ§  [Round {round_idx+1}] Processed {download_count} files. Knowledge graph updated."})

        # Final Report
        yield json.dumps({"step": "generating", "message": "ğŸ“ Generating technical report..."})
        

        repo_map_injection = f"""
        [Project Repo Map (Structure)]
        {file_tree_str}
        """

        # === æ ¹æ®è¯­è¨€é€‰æ‹© Prompt ===
        if language == "zh":
            # --- ä¸­æ–‡ Prompt ---
            system_role = "ä½ æ˜¯ä¸€ä½åŠ¡å®çš„æŠ€æœ¯ä¸“å®¶ã€‚ç›®æ ‡æ˜¯ä¸ºå¼€å‘è€…åˆ›å»ºä¸€ä¸ª'3é¡µçº¸'æ¶æ„æ¦‚è§ˆï¼Œè®©ä»–ä»¬èƒ½åœ¨5åˆ†é’Ÿå†…çœ‹æ‡‚è¿™ä¸ªä»“åº“ã€‚é‡ç‚¹å…³æ³¨æ¶æ„å’Œæ•°æ®æµï¼Œä¸è¦çº ç»“ç»†èŠ‚ã€‚"
            analysis_user_content = f"""
            [è§’è‰²]
            ä½ æ˜¯ä¸€ä½åŠ¡å®çš„æŠ€æœ¯ä¸“å®¶ï¼ˆTech Leadï¼‰ã€‚
            
            [è¾“å…¥æ•°æ®]
            {repo_map_injection}  <-- æ’å…¥ Repo Map

            åˆ†æçš„æ–‡ä»¶: {list(visited_files)}
            ä»£ç çŸ¥è¯†åº“: 
            {context_summary[:15000]}
            
            [ä¸¥æ ¼é™åˆ¶]
            1. **ä¸è¿›è¡Œä»£ç å®¡æŸ¥**: ä¸è¦åˆ—å‡º Bugã€ç¼ºå¤±åŠŸèƒ½æˆ–æ”¹è¿›å»ºè®®ã€‚
            2. **ä¸è¯„ä»·**: ä¸è¦è¯„ä»·ä»£ç è´¨é‡ï¼Œåªæè¿°å®ƒ**å¦‚ä½•å·¥ä½œ**ã€‚
            3. **è¯­è°ƒ**: ä¸“ä¸šã€ç»“æ„åŒ–ã€æè¿°æ€§ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
            4. **ä¸è¦åºŸè¯**: ä¸è¦å†™"å®‰å…¨æ€§"ã€"æœªæ¥è§„åˆ’"ç­‰æœªè¯·æ±‚çš„ç« èŠ‚ã€‚

            [è¾“å‡ºæ ¼å¼è¦æ±‚ (Markdown)]
            
            # é¡¹ç›®åˆ†ææŠ¥å‘Š

            ## 1. æ‰§è¡Œæ‘˜è¦ (Executive Summary)
            - **ç”¨é€”**: (è¿™ä¸ªé¡¹ç›®å…·ä½“è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ1-2å¥è¯)
            - **æ ¸å¿ƒåŠŸèƒ½**: (åˆ—å‡ºTop 3åŠŸèƒ½ç‚¹)
            - **æŠ€æœ¯æ ˆ**: (è¯­è¨€ã€æ¡†æ¶ã€æ•°æ®åº“ã€å…³é”®åº“)

            ## 2. ç³»ç»Ÿæ¶æ„ (Mermaid)
            åˆ›å»ºä¸€ä¸ª `graph TD` å›¾ã€‚
            - å±•ç¤ºé«˜å±‚ç»„ä»¶ (å¦‚ Client, API Server, Database, Worker, External Service)ã€‚
            - åœ¨è¿çº¿ä¸Šæ ‡æ³¨æ•°æ®æµ (å¦‚ "HTTP/JSON", "SQL")ã€‚
            - **é£æ ¼**: ä¿æŒæ¦‚å¿µæ¸…æ™°ç®€å•ã€‚

            ## 3. æ ¸å¿ƒé€»è¾‘åˆ†æ (Table)
            (æ€»ç»“å…³é”®æ¨¡å—ï¼Œä¸è¦åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ï¼Œåªåˆ—æœ€é‡è¦çš„)

            | ç»„ä»¶/æ–‡ä»¶ | èŒè´£ (å®ƒåšä»€ä¹ˆï¼Ÿ) | å…³é”®è®¾è®¡æ¨¡å¼/é€»è¾‘ |
            | :--- | :--- | :--- |
            | ä¾‹å¦‚ `auth_service.py` | å¤„ç†JWTé¢å‘ä¸éªŒè¯ | å•ä¾‹æ¨¡å¼, è·¯ç”±è£…é¥°å™¨ |
            | ... | ... | ... |

            ## 4. ğŸ”¬ æ ¸å¿ƒæ–¹æ³•æ·±åº¦è§£æ
            (ç²¾é€‰ 3-5 ä¸ªæœ€å…³é”®çš„ `.py` æ–‡ä»¶ã€‚é’ˆå¯¹æ¯ä¸ªæ–‡ä»¶ï¼Œåˆ—å‡ºé©±åŠ¨é€»è¾‘çš„ Top 2-3 ä¸ªæ–¹æ³•)

            ### 4.1 `[æ–‡ä»¶å]`
            * **`[æ–¹æ³•å]`**: [è§£é‡Šå®ƒåšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆé‡è¦ï¼Œä¸è¦è´´ä»£ç ]
            * **`[æ–¹æ³•å]`**: [è§£é‡Š...]

            ## 5. ä¸»è¦å·¥ä½œæµ (Mermaid)
            é€‰æ‹©**ä¸€ä¸ªæœ€é‡è¦**çš„ä¸šåŠ¡æµç¨‹ (Happy Path)ã€‚
            åˆ›å»ºä¸€ä¸ª `sequenceDiagram`ã€‚
            - å‚ä¸è€…åº”è¯¥æ˜¯é«˜å±‚æ¦‚å¿µ (å¦‚ User, API, DB)ï¼Œä¸è¦ç”¨å…·ä½“å˜é‡åã€‚
            
            ## 6. å¿«é€Ÿå¼€å§‹ (Quick Start)
            - **å‰ç½®æ¡ä»¶**: (å¦‚ Docker, Python 3.9+, .env é…ç½®)
            - **å…¥å£**: (å¦‚ä½•å¯åŠ¨ä¸»é€»è¾‘ï¼Ÿå¦‚ `python main.py`)
            """
        else:
            analysis_user_content = f"""
            [Role]
            You are a **Pragmatic Tech Lead**. Your goal is to create a **"3-Pages" Architecture Overview** for a developer who wants to understand this repo in 5 minutes.
            [Input Data]
            {repo_map_injection}  <-- Injecting Repo Map

            Files analyzed: {list(visited_files)}
            Code Knowledge: 
            {context_summary[:15000]}  # ç¨å¾®å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒDeepSeek å¤„ç†å¾—æ¥
            
            [Strict Constraints]
            1. **NO Code Review**: Do NOT list bugs, issues, missing features, or recommendations.
            2. **NO Critique**: Do not judge the code quality. Focus on HOW it works.
            3. **Tone**: Professional, descriptive, and structural.
            4. **NO "FLUFF"**: Do NOT add unrequested sections like "Security", "Scalability", "Data Models", "Future Enhancements", etc.

            [Required Output Format (Markdown)]
            
            # Project Analysis Report

            ## 1. Executive Summary
            - **Purpose**: (What specific problem does this project solve? 1-2 sentences)
            - **Key Features**: (Bullet points of top 3 features)
            - **Tech Stack**: (List languages, frameworks, databases, and key libs)

            ## 2. System Architecture
            Create a `graph TD` diagram.
            - Show high-level components (e.g., Client, API Server, Database, Worker, External Service).
            - Label the edges with data flow (e.g., "HTTP/JSON", "SQL").
            - **Style**: Keep it simple and conceptual.

            ## 3. Core Logic Analysis
            (Create a Markdown Table to summarize key modules. Do not list every file, only the most important ones.)

            | Component/File | Responsibility (What does it do?) | Key Design Pattern / Logic |
            | :--- | :--- | :--- |
            | e.g. `auth_service.py` | Handles JWT issuance and verification | Singleton, Decorator for routes |
            | ... | ... | ... |

            ## 4. Core Methods Deep Dive
            (Select the 3-5 most critical `.py` files. For each, list the top 2-3 methods that drive the logic.)

            ### 4.1 `[Filename, e.g., agent_service.py]`
            * **`[Method Name]`**: [Explanation of what it does and why it matters. No code.]
            * **`[Method Name]`**: [Explanation...]

            ### 4.2 `[Filename, e.g., vector_service.py]`
            * **`[Method Name]`**: [Explanation...]
            * ...

            ## 5. Main Workflow (Mermaid)
            Select the **Single Most Important** business flow (The "Happy Path").
            Create a `sequenceDiagram`.
            - Participants should be high-level (e.g., User, API, DB), not specific variable names.
            
            ## 6. Quick Start Guide
            - **Prerequisites**: (e.g. Docker, Python 3.9+, .env file)
            - **Entry Point**: (How to run the main logic? e.g. `python main.py` or `uvicorn`)

            """
        
        # === å¢åŠ  timeout é˜²æ­¢é•¿æ–‡æœ¬ç”Ÿæˆæ—¶æ–­è¿ ===
        stream = await client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a pragmatic Tech Lead. Focus on architecture and data flow, not implementation details."},
                {"role": "user", "content": analysis_user_content}
            ],
            stream=True,
            timeout=AgentConfig.LLM_TIMEOUT  # ä½¿ç”¨ Config
        )
        
        # === å¢åŠ  try-except æ•è·æµå¼ä¼ è¾“ä¸­æ–­ ===
        try:
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield json.dumps({"step": "report_chunk", "chunk": chunk.choices[0].delta.content})
        except (httpx.ReadError, httpx.ConnectError) as e:
            yield json.dumps({"step": "error", "message": f"âš ï¸ Network Timeout during generation: {str(e)}"})
            return

        yield json.dumps({"step": "finish", "message": "âœ… Analysis Complete!"})

    except Exception as e:
        # === å…¨å±€å¼‚å¸¸æ•è· ===
        import traceback
        traceback.print_exc()
        
        # æå–å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        if "401" in error_msg:
            ui_msg = "âŒ GitHub Token Invalid. Please check your settings."
        elif "403" in error_msg:
            ui_msg = "âŒ GitHub API Rate Limit Exceeded. Try again later or add a Token."
        elif "404" in error_msg:
            ui_msg = "âŒ Repository Not Found. Check the URL."
        elif "Timeout" in error_msg or "ConnectError" in error_msg:
            ui_msg = "âŒ Network Timeout. LLM or GitHub is not responding."
        else:
            ui_msg = f"ğŸ’¥ System Error: {error_msg}"
            
        yield json.dumps({"step": "error", "message": ui_msg})
        return # ç»ˆæ­¢æµ