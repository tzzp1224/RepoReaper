# æ–‡ä»¶è·¯å¾„: app/services/agent_service.py
import json
import asyncio
import traceback
import re
import ast
from app.core.config import settings
from app.utils.llm_client import client
from app.services.github_service import get_repo_structure, get_file_content
from app.services.vector_service import store_manager
from app.services.chunking_service import PythonASTChunker

# === Helper: é²æ£’çš„ JSON æå– ===
def extract_json_from_text(text):
    try:
        text = re.sub(r"^```(json)?|```$", "", text.strip(), flags=re.MULTILINE).strip()
        return json.loads(text)
    except:
        pass
    match = re.search(r"\[.*\]", text, re.DOTALL)
    if match:
        try: return json.loads(match.group(0))
        except: pass
    return []

# === ä¼˜åŒ– 1ï¼šåŸºäº AST çš„ Repo Map ç”Ÿæˆ ===
def _extract_symbols(content):
    """
    ä»ä»£ç å†…å®¹ä¸­æå– Class å’Œ Function çš„ç­¾åï¼Œç”Ÿæˆç²¾ç®€åœ°å›¾ã€‚
    """
    try:
        tree = ast.parse(content)
        symbols = []
        for node in tree.body:
            if isinstance(node, ast.ClassDef):
                symbols.append(f"  [C] {node.name}")
                # æå–ç±»é‡Œé¢çš„æ–¹æ³•ï¼ˆå¯é€‰ï¼Œä¸ºäº†ä¸å å¤ªå¤š Tokenï¼Œåªæå– __init__ æˆ–å…¬å…±æ–¹æ³•ï¼‰
                for sub in node.body:
                    if isinstance(sub, (ast.FunctionDef, ast.AsyncFunctionDef)):
                        if not sub.name.startswith("_") or sub.name == "__init__":
                            symbols.append(f"    - {sub.name}")
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                symbols.append(f"  [F] {node.name}")
        return symbols
    except:
        return []

async def generate_repo_map(repo_url, file_list, limit=20):
    """
    ç”Ÿæˆå¢å¼ºç‰ˆä»“åº“åœ°å›¾ï¼š
    1. å¯¹ file_list è¿›è¡Œæ’åºï¼Œä¼˜å…ˆçœ‹æ ¹ç›®å½•å’Œ core/app/src ç›®å½•ã€‚
    2. å¼‚æ­¥å¹¶å‘ä¸‹è½½å‰ limit ä¸ªæ–‡ä»¶çš„å†…å®¹ã€‚
    3. è§£æ ASTï¼Œæå–ç±»/å‡½æ•°åã€‚
    4. ç»„åˆæˆ Repo Map å­—ç¬¦ä¸²ã€‚
    """
    # ç­›é€‰é«˜ä¼˜å…ˆçº§çš„ Python æ–‡ä»¶
    priority_files = [
        f for f in file_list 
        if f.endswith('.py') and 
        (f.count('/') <= 1 or any(k in f for k in ['main', 'app', 'core', 'api', 'service', 'utils']))
    ]
    # æˆªå–å‰ N ä¸ªï¼Œé¿å…ä¸‹è½½å¤ªå¤š
    targets = priority_files[:limit]
    remaining = [f for f in file_list if f not in targets]
    
    repo_map_lines = []
    
    # å¼‚æ­¥ä¸‹è½½å¹¶è§£æ
    async def process_file(path):
        content = await asyncio.to_thread(get_file_content, repo_url, path)
        if not content: return f"{path} (Read Failed)"
        symbols = await asyncio.to_thread(_extract_symbols, content)
        if symbols:
            return f"{path}\n" + "\n".join(symbols)
        return path

    # æç¤ºä¿¡æ¯
    repo_map_lines.append(f"--- Key Files Structure (Top {len(targets)}) ---")
    
    # å¹¶å‘æ‰§è¡Œ (åŠ å¿«é€Ÿåº¦)
    tasks = [process_file(f) for f in targets]
    results = await asyncio.gather(*tasks)
    repo_map_lines.extend(results)
    
    # è¿½åŠ å‰©ä½™æ–‡ä»¶ï¼ˆä»…è·¯å¾„ï¼‰
    if remaining:
        repo_map_lines.append("\n--- Other Files ---")
        # å¦‚æœå‰©ä½™å¤ªå¤šï¼Œåšæˆªæ–­
        if len(remaining) > 300:
            repo_map_lines.extend(remaining[:300])
            repo_map_lines.append(f"... ({len(remaining)-300} more files)")
        else:
            repo_map_lines.extend(remaining)
            
    return "\n".join(repo_map_lines)


async def agent_stream(repo_url: str, session_id: str, language: str = "en"):
    short_id = session_id[-6:] if session_id else "unknown"
    yield json.dumps({"step": "init", "message": f"ğŸš€ [Session: {short_id}] Connecting to GitHub..."})
    await asyncio.sleep(0.5)
    
    try:
        vector_db = store_manager.get_store(session_id)
        vector_db.reset_collection() 
        vector_db.repo_url = repo_url
        
        chunker = PythonASTChunker(min_chunk_size=50)

        file_list = await asyncio.to_thread(get_repo_structure, repo_url)
        if not file_list:
            yield json.dumps({"step": "error", "message": "âŒ Failed to fetch file list. Check URL or Token."})
            return

        yield json.dumps({"step": "fetched", "message": f"ğŸ“¦ Found {len(file_list)} files. Building Repo Map (AST Parsing)..."})        
        # === ä½¿ç”¨æ–°çš„ Repo Map ç”Ÿæˆé€»è¾‘ ===
        # è¿™ä¼šæ¯”ä¹‹å‰ç¨æ…¢ä¸€ç‚¹ç‚¹ï¼ˆå› ä¸ºè¦ä¸‹è½½åå‡ ä¸ªæ–‡ä»¶ï¼‰ï¼Œä½†å¯¹ Agent æ™ºå•†æå‡å·¨å¤§
        file_tree_str = await generate_repo_map(repo_url, file_list, limit=15)
        
        MAX_ROUNDS = 3
        visited_files = set()
        context_summary = ""
        readme_file = next((f for f in file_list if f.lower().endswith("readme.md")), None)

        for round_idx in range(MAX_ROUNDS):
            yield json.dumps({"step": "thinking", "message": f"ğŸ•µï¸ [Round {round_idx+1}/{MAX_ROUNDS}] DeepSeek is analyzing Repo Map..."})
            
            # === DeepSeek English Prompt Strategy ===
            system_prompt = "You are a Senior Software Architect. Your goal is to understand the codebase."
            user_content = f"""
            [Project Repo Map]
            (Contains file paths and key Class/Function signatures)
            {file_tree_str}
            
            [Files Already Read]
            {list(visited_files)}
            
            [Current Knowledge]
            {context_summary}
            
            [Task]
            Select 1-3 MOST CRITICAL files to read next to understand the core logic.
            Focus on files that seem to contain main logic based on the Repo Map symbols.
            
            [Constraint]
            Return ONLY a raw JSON list of strings. No markdown.
            Example: ["src/main.py", "app/auth.py"]
            """
            
            if not client:
                 yield json.dumps({"step": "error", "message": "âŒ LLM Client Not Initialized."})
                 return
            
            response = await client.chat.completions.create(
                model=settings.MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.1 
            )
            
            raw_content = response.choices[0].message.content
            target_files = extract_json_from_text(raw_content)

            valid_files = [f for f in target_files if f in file_list and f not in visited_files]

            if round_idx == 0 and readme_file and readme_file not in visited_files and readme_file not in valid_files:
                valid_files.insert(0, readme_file)

            if not valid_files:
                yield json.dumps({"step": "plan", "message": f"ğŸ›‘ [Round {round_idx+1}] Sufficient context gathered."})
                break
            
            yield json.dumps({"step": "plan", "message": f"ğŸ‘‰ [Round {round_idx+1}] Selected: {valid_files}"})
            
            new_knowledge = ""
            for i, file_path in enumerate(valid_files):
                yield json.dumps({"step": "download", "message": f"ğŸ“¥ Reading: {file_path}..."})
                
                content = get_file_content(repo_url, file_path)
                if not content: continue
                visited_files.add(file_path)
                
                # Preview logic
                lines = content.split('\n')[:50]
                preview = "\n".join(lines)
                new_knowledge += f"\n--- File: {file_path} ---\n{preview}\n"

                chunks = await asyncio.to_thread(chunker.chunk_file, content, file_path)
                if not chunks: continue
                
                documents = [c["content"] for c in chunks]
                metadatas = []
                for c in chunks:
                    meta = c["metadata"]
                    metadatas.append({
                        "file": meta["file"],
                        "type": meta["type"],
                        "name": meta.get("name", ""),
                        "class": meta.get("class") or ""
                    })

                if documents:
                    await vector_db.add_documents(documents, metadatas)
            
            context_summary += new_knowledge
            
            # Save global context
            vector_db.global_context = {
                "file_tree": file_tree_str,
                "summary": context_summary[:8000] 
            }
            yield json.dumps({"step": "indexing", "message": f"ğŸ§  [Round {round_idx+1}] Knowledge graph updated."})

        # Final Report
        yield json.dumps({"step": "generating", "message": "ğŸ“ Generating technical report..."})
        
        # === æ ¹æ®è¯­è¨€é€‰æ‹© Prompt ===
        if language == "zh":
            # --- ä¸­æ–‡ Prompt ---
            system_role = "ä½ æ˜¯ä¸€ä½åŠ¡å®çš„æŠ€æœ¯ä¸“å®¶ã€‚ç›®æ ‡æ˜¯ä¸ºå¼€å‘è€…åˆ›å»ºä¸€ä¸ª'3é¡µçº¸'æ¶æ„æ¦‚è§ˆï¼Œè®©ä»–ä»¬èƒ½åœ¨5åˆ†é’Ÿå†…çœ‹æ‡‚è¿™ä¸ªä»“åº“ã€‚é‡ç‚¹å…³æ³¨æ¶æ„å’Œæ•°æ®æµï¼Œä¸è¦çº ç»“ç»†èŠ‚ã€‚"
            analysis_user_content = f"""
            [è§’è‰²]
            ä½ æ˜¯ä¸€ä½åŠ¡å®çš„æŠ€æœ¯ä¸“å®¶ï¼ˆTech Leadï¼‰ã€‚
            
            [è¾“å…¥æ•°æ®]
            åˆ†æçš„æ–‡ä»¶: {list(visited_files)}
            ä»£ç çŸ¥è¯†åº“: 
            {context_summary[:15000]}
            
            [ä¸¥æ ¼é™åˆ¶]
            1. **ä¸è¿›è¡Œä»£ç å®¡æŸ¥**: ä¸è¦åˆ—å‡º Bugã€ç¼ºå¤±åŠŸèƒ½æˆ–æ”¹è¿›å»ºè®®ã€‚
            2. **ä¸è¯„ä»·**: ä¸è¦è¯„ä»·ä»£ç è´¨é‡ï¼Œåªæè¿°å®ƒ**å¦‚ä½•å·¥ä½œ**ã€‚
            3. **è¯­è°ƒ**: ä¸“ä¸šã€ç»“æ„åŒ–ã€æè¿°æ€§ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
            4. **ä¸è¦åºŸè¯**: ä¸è¦å†™"å®‰å…¨æ€§"ã€"æœªæ¥è§„åˆ’"ç­‰æœªè¯·æ±‚çš„ç« èŠ‚ã€‚

            [è¾“å‡ºæ ¼å¼è¦æ±‚ (Markdown)]
            
            # é¡¹ç›®åˆ†ææŠ¥å‘Š

            ## 1. æ‰§è¡Œæ‘˜è¦ (Executive Summary)
            - **ç”¨é€”**: (è¿™ä¸ªé¡¹ç›®å…·ä½“è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ1-2å¥è¯)
            - **æ ¸å¿ƒåŠŸèƒ½**: (åˆ—å‡ºTop 3åŠŸèƒ½ç‚¹)
            - **æŠ€æœ¯æ ˆ**: (è¯­è¨€ã€æ¡†æ¶ã€æ•°æ®åº“ã€å…³é”®åº“)

            ## 2. ç³»ç»Ÿæ¶æ„ (Mermaid)
            åˆ›å»ºä¸€ä¸ª `graph TD` å›¾ã€‚
            - å±•ç¤ºé«˜å±‚ç»„ä»¶ (å¦‚ Client, API Server, Database, Worker, External Service)ã€‚
            - åœ¨è¿çº¿ä¸Šæ ‡æ³¨æ•°æ®æµ (å¦‚ "HTTP/JSON", "SQL")ã€‚
            - **é£æ ¼**: ä¿æŒæ¦‚å¿µæ¸…æ™°ç®€å•ã€‚

            ## 3. æ ¸å¿ƒé€»è¾‘åˆ†æ (Table)
            (æ€»ç»“å…³é”®æ¨¡å—ï¼Œä¸è¦åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ï¼Œåªåˆ—æœ€é‡è¦çš„)

            | ç»„ä»¶/æ–‡ä»¶ | èŒè´£ (å®ƒåšä»€ä¹ˆï¼Ÿ) | å…³é”®è®¾è®¡æ¨¡å¼/é€»è¾‘ |
            | :--- | :--- | :--- |
            | ä¾‹å¦‚ `auth_service.py` | å¤„ç†JWTé¢å‘ä¸éªŒè¯ | å•ä¾‹æ¨¡å¼, è·¯ç”±è£…é¥°å™¨ |
            | ... | ... | ... |

            ## 4. ğŸ”¬ æ ¸å¿ƒæ–¹æ³•æ·±åº¦è§£æ
            (ç²¾é€‰ 3-5 ä¸ªæœ€å…³é”®çš„ `.py` æ–‡ä»¶ã€‚é’ˆå¯¹æ¯ä¸ªæ–‡ä»¶ï¼Œåˆ—å‡ºé©±åŠ¨é€»è¾‘çš„ Top 2-3 ä¸ªæ–¹æ³•)

            ### 4.1 `[æ–‡ä»¶å]`
            * **`[æ–¹æ³•å]`**: [è§£é‡Šå®ƒåšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆé‡è¦ï¼Œä¸è¦è´´ä»£ç ]
            * **`[æ–¹æ³•å]`**: [è§£é‡Š...]

            ## 5. ä¸»è¦å·¥ä½œæµ (Mermaid)
            é€‰æ‹©**ä¸€ä¸ªæœ€é‡è¦**çš„ä¸šåŠ¡æµç¨‹ (Happy Path)ã€‚
            åˆ›å»ºä¸€ä¸ª `sequenceDiagram`ã€‚
            - å‚ä¸è€…åº”è¯¥æ˜¯é«˜å±‚æ¦‚å¿µ (å¦‚ User, API, DB)ï¼Œä¸è¦ç”¨å…·ä½“å˜é‡åã€‚
            
            ## 6. å¿«é€Ÿå¼€å§‹ (Quick Start)
            - **å‰ç½®æ¡ä»¶**: (å¦‚ Docker, Python 3.9+, .env é…ç½®)
            - **å…¥å£**: (å¦‚ä½•å¯åŠ¨ä¸»é€»è¾‘ï¼Ÿå¦‚ `python main.py`)
            """
        else:
            analysis_user_content = f"""
            [Role]
            You are a **Pragmatic Tech Lead**. Your goal is to create a **"3-Pages" Architecture Overview** for a developer who wants to understand this repo in 5 minutes.
            [Input Data]
            Files analyzed: {list(visited_files)}
            Code Knowledge: 
            {context_summary[:15000]}  # ç¨å¾®å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦ï¼ŒDeepSeek å¤„ç†å¾—æ¥
            
            [Strict Constraints]
            1. **NO Code Review**: Do NOT list bugs, issues, missing features, or recommendations.
            2. **NO Critique**: Do not judge the code quality. Focus on HOW it works.
            3. **Tone**: Professional, descriptive, and structural.
            4. **NO "FLUFF"**: Do NOT add unrequested sections like "Security", "Scalability", "Data Models", "Future Enhancements", etc.

            [Required Output Format (Markdown)]
            
            # Project Analysis Report

            ## 1. Executive Summary
            - **Purpose**: (What specific problem does this project solve? 1-2 sentences)
            - **Key Features**: (Bullet points of top 3 features)
            - **Tech Stack**: (List languages, frameworks, databases, and key libs)

            ## 2. System Architecture
            Create a `graph TD` diagram.
            - Show high-level components (e.g., Client, API Server, Database, Worker, External Service).
            - Label the edges with data flow (e.g., "HTTP/JSON", "SQL").
            - **Style**: Keep it simple and conceptual.

            ## 3. Core Logic Analysis
            (Create a Markdown Table to summarize key modules. Do not list every file, only the most important ones.)

            | Component/File | Responsibility (What does it do?) | Key Design Pattern / Logic |
            | :--- | :--- | :--- |
            | e.g. `auth_service.py` | Handles JWT issuance and verification | Singleton, Decorator for routes |
            | ... | ... | ... |

            ## 4. Core Methods Deep Dive
            (Select the 3-5 most critical `.py` files. For each, list the top 2-3 methods that drive the logic.)

            ### 4.1 `[Filename, e.g., agent_service.py]`
            * **`[Method Name]`**: [Explanation of what it does and why it matters. No code.]
            * **`[Method Name]`**: [Explanation...]

            ### 4.2 `[Filename, e.g., vector_service.py]`
            * **`[Method Name]`**: [Explanation...]
            * ...

            ## 5. Main Workflow (Mermaid)
            Select the **Single Most Important** business flow (The "Happy Path").
            Create a `sequenceDiagram`.
            - Participants should be high-level (e.g., User, API, DB), not specific variable names.
            
            ## 6. Quick Start Guide
            - **Prerequisites**: (e.g. Docker, Python 3.9+, .env file)
            - **Entry Point**: (How to run the main logic? e.g. `python main.py` or `uvicorn`)

            """
        
        # === FIX: å¢åŠ  timeout é˜²æ­¢é•¿æ–‡æœ¬ç”Ÿæˆæ—¶æ–­è¿ ===
        stream = await client.chat.completions.create(
            model=settings.MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are a pragmatic Tech Lead. Focus on architecture and data flow, not implementation details."},
                {"role": "user", "content": analysis_user_content}
            ],
            stream=True,
            timeout=600  # <--- æ ¸å¿ƒä¿®å¤ï¼šè®¾ç½® 600ç§’ (10åˆ†é’Ÿ) è¶…æ—¶ï¼Œè§£å†³ httpx.ReadError
        )
        
        # === FIX: å¢åŠ  try-except æ•è·æµå¼ä¼ è¾“ä¸­æ–­ ===
        try:
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield json.dumps({"step": "report_chunk", "chunk": chunk.choices[0].delta.content})
        except (httpx.ReadError, httpx.ConnectError) as e:
            yield json.dumps({"step": "error", "message": f"âš ï¸ Network Timeout during generation: {str(e)}"})
            return

        yield json.dumps({"step": "finish", "message": "âœ… Analysis Complete!"})

    except Exception as e:
        traceback.print_exc()
        yield json.dumps({"step": "error", "message": f"ğŸ’¥ System Error: {str(e)}"})