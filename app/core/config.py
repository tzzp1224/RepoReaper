# æ–‡ä»¶è·¯å¾„: app/core/config.py
"""
åº”ç”¨é…ç½®æ¨¡å— - ç»Ÿä¸€é…ç½®ä¸­å¿ƒ

æ”¯æŒå¤š LLM ä¾›åº”å•†é…ç½®:
- OpenAI (GPT-4, GPT-4o ç­‰)
- DeepSeek (deepseek-chat ç­‰)
- Anthropic (Claude ç³»åˆ—)
- Google Gemini (gemini-3-flash-preview ç­‰)
"""
import os
from dataclasses import dataclass, field
from typing import Optional, Tuple
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()


# ============================================================
# Agent åˆ†æé…ç½®
# ============================================================

@dataclass
class AgentAnalysisConfig:
    """Agent åˆ†æå¼•æ“é…ç½®"""
    # Repo Map é…ç½®
    initial_map_limit: int = 25           # åˆå§‹ Repo Map æ–‡ä»¶æ•°é‡ (æé«˜ç²¾åº¦)
    max_symbols_per_file: int = 40        # æ¯æ–‡ä»¶æœ€å¤§ç¬¦å·æ•° (æé«˜ç²¾åº¦)
    
    # åˆ†æè½®æ¬¡é…ç½®
    max_rounds: int = 4                   # æœ€å¤§åˆ†æè½®æ•° (æé«˜ç²¾åº¦ï¼Œå› ä¸ºæŠ¥å‘Šå¯å¤ç”¨)
    files_per_round: int = 5              # æ¯è½®é€‰æ‹©æ–‡ä»¶æ•° (æé«˜ç²¾åº¦)
    max_context_length: int = 20000       # ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦ (æé«˜ç²¾åº¦)
    
    # ä¼˜å…ˆçº§é…ç½®
    priority_exts: Tuple[str, ...] = (
        '.py', '.java', '.go', '.js', '.ts', '.tsx', '.cpp', '.cs', '.rs'
    )
    priority_keywords: Tuple[str, ...] = (
        'main', 'app', 'core', 'api', 'service', 'utils', 'controller', 'model', 'config'
    )


# ============================================================
# å‘é‡æœåŠ¡é…ç½®
# ============================================================

@dataclass
class VectorServiceConfig:
    """å‘é‡æœåŠ¡é…ç½®"""
    # æ•°æ®ç›®å½•
    data_dir: str = "data"
    context_dir: str = "data/contexts"
    cache_version: str = "2.0"
    
    # Embedding é…ç½®
    embedding_api_url: str = "https://api.siliconflow.cn/v1"
    embedding_model: str = "BAAI/bge-m3"
    embedding_batch_size: int = 50
    embedding_max_length: int = 8000
    embedding_concurrency: int = 5
    embedding_dimensions: int = 1024
    
    # BM25 é…ç½®
    tokenize_regex: str = r'[^a-zA-Z0-9_\.@\u4e00-\u9fa5]+'
    
    # æ··åˆæœç´¢ RRF å‚æ•°
    rrf_k: int = 60
    rrf_weight_vector: float = 1.0
    rrf_weight_bm25: float = 0.3
    search_oversample: int = 2
    default_top_k: int = 3
    
    # Session LRU ç¼“å­˜é…ç½®
    session_max_count: int = 100          # å†…å­˜ä¸­æœ€å¤§ session æ•°


# ============================================================
# å¯¹è¯è®°å¿†é…ç½®
# ============================================================

@dataclass
class ConversationConfig:
    """å¯¹è¯è®°å¿†é…ç½®"""
    # æ»‘åŠ¨çª—å£
    max_recent_turns: int = 10             # ä¿ç•™æœ€è¿‘ N è½®å¯¹è¯
    max_context_tokens: int = 8000        # æœ€å¤§ä¸Šä¸‹æ–‡ token æ•°
    summary_threshold: int = 15           # è¶…è¿‡ N è½®å¼€å§‹å‹ç¼©
    # å¯¹è¯è®°å¿†æ˜¯çº¯å†…å­˜å­˜å‚¨ï¼ŒæœåŠ¡é‡å¯è‡ªåŠ¨æ¸…ç©ºï¼Œæ— éœ€å®šæ—¶æ¸…ç†


# ============================================================
# Qdrant é…ç½®
# ============================================================

@dataclass
class QdrantServiceConfig:
    """
    Qdrant å‘é‡æ•°æ®åº“é…ç½®
    
    æ”¯æŒä¸‰ç§æ¨¡å¼ (é€šè¿‡ç¯å¢ƒå˜é‡ QDRANT_MODE åˆ‡æ¢):
    - local: æœ¬åœ°åµŒå…¥å¼å­˜å‚¨ (å¼€å‘ç¯å¢ƒ, å• Worker)
    - server: Qdrant Server Docker (ç”Ÿäº§ç¯å¢ƒ, å¤š Worker)
    - cloud: Qdrant Cloud æ‰˜ç®¡æœåŠ¡
    
    ç¯å¢ƒå˜é‡:
    - QDRANT_MODE: "local" | "server" | "cloud"
    - QDRANT_URL: æœåŠ¡å™¨ URL (server/cloud æ¨¡å¼)
    - QDRANT_API_KEY: API å¯†é’¥ (cloud æ¨¡å¼å¿…éœ€)
    - QDRANT_LOCAL_PATH: æœ¬åœ°å­˜å‚¨è·¯å¾„ (local æ¨¡å¼)
    """
    mode: str = os.getenv("QDRANT_MODE", "local")
    url: str = os.getenv("QDRANT_URL", "")
    host: str = os.getenv("QDRANT_HOST", "localhost")
    port: int = int(os.getenv("QDRANT_PORT", "6333"))
    grpc_port: int = int(os.getenv("QDRANT_GRPC_PORT", "6334"))
    prefer_grpc: bool = True
    api_key: str = os.getenv("QDRANT_API_KEY", "")
    
    local_path: str = os.getenv("QDRANT_LOCAL_PATH", "data/qdrant_db")
    
    vector_size: int = 1024               # BGE-M3 ç»´åº¦
    hnsw_m: int = 16
    hnsw_ef_construct: int = 100
    batch_size: int = 100
    timeout: float = 30.0


# ============================================================
# LLM ä¾›åº”å•†é…ç½®
# ============================================================


class Settings:
    """åº”ç”¨é…ç½®ç±»"""
    
    # --- LLM ä¾›åº”å•†é€‰æ‹© ---
    # æ”¯æŒ: "openai", "deepseek", "anthropic", "gemini"
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "deepseek")
    
    # --- API Keys (æ ¹æ®é€‰æ‹©çš„ä¾›åº”å•†é…ç½®å¯¹åº”çš„ Key) ---
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
    
    # OpenAI
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # å¯é€‰è‡ªå®šä¹‰ç«¯ç‚¹
    
    # DeepSeek
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
    DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
    
    # Anthropic (Claude)
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    
    # Google Gemini
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    GEMINI_BASE_URL = os.getenv("GEMINI_BASE_URL")  # å¯é€‰ OpenAI å…¼å®¹ç«¯ç‚¹
    
    # SiliconFlow (Embedding)
    SILICON_API_KEY = os.getenv("SILICON_API_KEY")
    
    # --- æ¨¡å‹é…ç½® ---
    # å¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨å„ä¾›åº”å•†çš„é»˜è®¤æ¨¡å‹
    MODEL_NAME = os.getenv("MODEL_NAME")
    
    # --- æœåŠ¡é…ç½® ---
    HOST = os.getenv("HOST", "127.0.0.1")
    PORT = int(os.getenv("PORT", 8000))
    
    # --- LLM é»˜è®¤å‚æ•° ---
    LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.1"))
    LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "4096"))
    LLM_TIMEOUT = int(os.getenv("LLM_TIMEOUT", "600"))
    
    @property
    def current_api_key(self) -> Optional[str]:
        """è·å–å½“å‰é€‰æ‹©çš„ä¾›åº”å•†çš„ API Key"""
        key_mapping = {
            "openai": self.OPENAI_API_KEY,
            "deepseek": self.DEEPSEEK_API_KEY,
            "anthropic": self.ANTHROPIC_API_KEY,
            "gemini": self.GEMINI_API_KEY,
        }
        return key_mapping.get(self.LLM_PROVIDER.lower())
    
    @property
    def current_base_url(self) -> Optional[str]:
        """è·å–å½“å‰é€‰æ‹©çš„ä¾›åº”å•†çš„ Base URL"""
        url_mapping = {
            "openai": self.OPENAI_BASE_URL,
            "deepseek": self.DEEPSEEK_BASE_URL,
            "anthropic": None,
            "gemini": self.GEMINI_BASE_URL,
        }
        return url_mapping.get(self.LLM_PROVIDER.lower())
    
    @property
    def default_model_name(self) -> str:
        """è·å–å½“å‰ä¾›åº”å•†çš„é»˜è®¤æ¨¡å‹åç§°"""
        defaults = {
            "openai": "gpt-4o-mini",
            "deepseek": "deepseek-chat",
            "anthropic": "claude-3-5-sonnet-20241022",
            "gemini": "gemini-3-flash-preview",
        }
        return self.MODEL_NAME or defaults.get(self.LLM_PROVIDER.lower(), "default")

    def validate(self):
        """å¯åŠ¨æ—¶æ£€æŸ¥å¿…è¦çš„é…ç½®æ˜¯å¦å­˜åœ¨"""
        provider = self.LLM_PROVIDER.lower()
        print(f"ğŸ”§ LLM Provider: {provider.upper()}")
        
        # 1. æ£€æŸ¥é€‰æ‹©çš„ä¾›åº”å•†çš„ API Key
        if not self.current_api_key:
            key_name = f"{provider.upper()}_API_KEY"
            raise ValueError(
                f"âŒ é”™è¯¯: ç¼ºå°‘ {key_name}ã€‚\n"
                f"   å½“å‰é€‰æ‹©çš„ LLM ä¾›åº”å•†æ˜¯: {provider}\n"
                f"   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® {key_name}ï¼Œæˆ–æ›´æ”¹ LLM_PROVIDER ä¸ºå…¶ä»–ä¾›åº”å•†ã€‚"
            )
        
        # 2. æ£€æŸ¥ SiliconCloud Key (Embedding åŠŸèƒ½)
        if not self.SILICON_API_KEY:
            print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° SILICON_API_KEYï¼Œå‘é‡æ£€ç´¢åŠŸèƒ½å¯èƒ½æ— æ³•å·¥ä½œã€‚")
            
        # 3. æ£€æŸ¥ GitHub Token (å¯é€‰ä½†å»ºè®®)
        if not self.GITHUB_TOKEN:
            print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° GITHUB_TOKENï¼ŒGitHub API è¯·æ±‚å°†å—åˆ°æ¯å°æ—¶ 60 æ¬¡çš„ä¸¥æ ¼é™åˆ¶ã€‚")
        
        print(f"âœ… é…ç½®éªŒè¯é€šè¿‡ (Model: {self.default_model_name})")


# ============================================================
# å…¨å±€é…ç½®å®ä¾‹
# ============================================================

# LLM è®¾ç½®
settings = Settings()
settings.validate()

# å­ç³»ç»Ÿé…ç½®
agent_config = AgentAnalysisConfig()
vector_config = VectorServiceConfig()
conversation_config = ConversationConfig()
qdrant_config = QdrantServiceConfig()