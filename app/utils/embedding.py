# -*- coding: utf-8 -*-
"""
Embedding æœåŠ¡ - å¹¶å‘ä¼˜åŒ–ç‰ˆ

ç‰¹æ€§:
1. å¹¶å‘æ‰¹é‡è¯·æ±‚ - ä½¿ç”¨ asyncio.gather å¹¶è¡Œå¤„ç†å¤šä¸ªæ‰¹æ¬¡
2. ä¿¡å·é‡æ§åˆ¶ - é™åˆ¶æœ€å¤§å¹¶å‘æ•°ï¼Œé¿å… API é™æµ
3. é‡è¯•æœºåˆ¶ - ä½¿ç”¨ tenacity å¤„ç†ä¸´æ—¶æ€§é”™è¯¯
4. æ™ºèƒ½åˆ†æ‰¹ - æ ¹æ® token æ•°é‡åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
"""

import asyncio
import logging
from typing import List, Optional
from dataclasses import dataclass

from openai import AsyncOpenAI

from app.core.config import settings
from app.utils.retry import llm_retry, is_retryable_error

logger = logging.getLogger(__name__)


@dataclass
class EmbeddingConfig:
    """Embedding æœåŠ¡é…ç½®"""
    # API é…ç½®
    api_base_url: str = "https://api.siliconflow.cn/v1"
    model_name: str = "BAAI/bge-m3"
    
    # æ‰¹å¤„ç†é…ç½®
    batch_size: int = 50              # æ¯æ‰¹æ–‡æœ¬æ•°é‡
    max_text_length: int = 8000       # å•ä¸ªæ–‡æœ¬æœ€å¤§å­—ç¬¦æ•°
    
    # å¹¶å‘æ§åˆ¶
    max_concurrent_batches: int = 5   # æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°
    
    # è¶…æ—¶é…ç½®
    timeout: int = 60                 # å•æ¬¡è¯·æ±‚è¶…æ—¶ (ç§’)


class EmbeddingService:
    """
    é«˜æ€§èƒ½ Embedding æœåŠ¡
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    service = EmbeddingService()
    
    # å•æ–‡æœ¬
    embedding = await service.embed_text("Hello world")
    
    # æ‰¹é‡æ–‡æœ¬ (è‡ªåŠ¨å¹¶å‘ä¼˜åŒ–)
    texts = ["text1", "text2", ..., "text100"]
    embeddings = await service.embed_batch(texts)
    ```
    """
    
    def __init__(self, config: Optional[EmbeddingConfig] = None):
        self.config = config or EmbeddingConfig()
        
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (SiliconFlow å…¼å®¹ OpenAI åè®®)
        self._client = AsyncOpenAI(
            api_key=settings.SILICON_API_KEY,
            base_url=self.config.api_base_url,
            timeout=self.config.timeout
        )
        
        # å¹¶å‘ä¿¡å·é‡
        self._semaphore = asyncio.Semaphore(self.config.max_concurrent_batches)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_texts": 0,
            "retried_requests": 0
        }
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬: ç§»é™¤æ¢è¡Œã€æˆªæ–­é•¿åº¦"""
        text = text.replace("\n", " ").strip()
        if len(text) > self.config.max_text_length:
            text = text[:self.config.max_text_length]
        return text
    
    @llm_retry
    async def _embed_single_batch(self, texts: List[str]) -> List[List[float]]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„ Embedding è¯·æ±‚ (å¸¦é‡è¯•)
        
        Args:
            texts: é¢„å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            embedding å‘é‡åˆ—è¡¨
        """
        self._stats["total_requests"] += 1
        
        response = await self._client.embeddings.create(
            input=texts,
            model=self.config.model_name
        )
        
        self._stats["successful_requests"] += 1
        return [item.embedding for item in response.data]
    
    async def _embed_batch_with_semaphore(
        self, 
        batch_texts: List[str], 
        batch_index: int
    ) -> tuple[int, List[List[float]]]:
        """
        å¸¦ä¿¡å·é‡æ§åˆ¶çš„æ‰¹æ¬¡å¤„ç†
        
        Returns:
            (batch_index, embeddings) - è¿”å›ç´¢å¼•ç”¨äºç»“æœæ’åº
        """
        async with self._semaphore:
            try:
                embeddings = await self._embed_single_batch(batch_texts)
                logger.debug(f"âœ… æ‰¹æ¬¡ {batch_index} å®Œæˆ: {len(batch_texts)} æ–‡æœ¬")
                return (batch_index, embeddings)
            except Exception as e:
                self._stats["failed_requests"] += 1
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_index} å¤±è´¥: {type(e).__name__}: {e}")
                raise
    
    async def embed_text(self, text: str) -> List[float]:
        """
        è·å–å•ä¸ªæ–‡æœ¬çš„ Embedding
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            embedding å‘é‡ï¼Œå¤±è´¥è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            processed = self._preprocess_text(text)
            if not processed:
                return []
            
            self._stats["total_texts"] += 1
            embeddings = await self._embed_single_batch([processed])
            return embeddings[0] if embeddings else []
        except Exception as e:
            logger.error(f"embed_text å¤±è´¥: {e}")
            return []
    
    async def embed_batch(
        self, 
        texts: List[str],
        show_progress: bool = False
    ) -> List[List[float]]:
        """
        æ‰¹é‡è·å– Embedding (å¹¶å‘ä¼˜åŒ–)
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ—¥å¿—
            
        Returns:
            embedding å‘é‡åˆ—è¡¨ (ä¸è¾“å…¥é¡ºåºä¸€è‡´)
            å¤±è´¥çš„æ–‡æœ¬å¯¹åº”ç©ºåˆ—è¡¨
        """
        if not texts:
            return []
        
        # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [self._preprocess_text(t) for t in texts]
        self._stats["total_texts"] += len(texts)
        
        # åˆ†æ‰¹
        batch_size = self.config.batch_size
        batches = [
            processed_texts[i:i + batch_size] 
            for i in range(0, len(processed_texts), batch_size)
        ]
        
        total_batches = len(batches)
        if show_progress:
            logger.info(
                f"ğŸ“Š Embedding: {len(texts)} æ–‡æœ¬ â†’ {total_batches} æ‰¹æ¬¡ "
                f"(å¹¶å‘: {self.config.max_concurrent_batches})"
            )
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ‰¹æ¬¡
        tasks = [
            self._embed_batch_with_semaphore(batch, idx)
            for idx, batch in enumerate(batches)
        ]
        
        # æ”¶é›†ç»“æœ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æŒ‰æ‰¹æ¬¡ç´¢å¼•æ’åºå¹¶åˆå¹¶ç»“æœ
        embeddings = []
        for result in sorted(results, key=lambda x: x[0] if isinstance(x, tuple) else float('inf')):
            if isinstance(result, tuple):
                batch_idx, batch_embeddings = result
                embeddings.extend(batch_embeddings)
            else:
                # å¼‚å¸¸æƒ…å†µ: å¡«å……ç©ºå‘é‡
                # æ‰¾å‡ºè¿™ä¸ªæ‰¹æ¬¡æœ‰å¤šå°‘æ–‡æœ¬
                failed_batch_size = batch_size  # ä¿å®ˆä¼°è®¡
                embeddings.extend([[] for _ in range(failed_batch_size)])
                logger.warning(f"æ‰¹æ¬¡å¤±è´¥ï¼Œå¡«å…… {failed_batch_size} ä¸ªç©ºå‘é‡")
        
        # ç¡®ä¿è¿”å›æ•°é‡ä¸è¾“å…¥ä¸€è‡´
        if len(embeddings) < len(texts):
            embeddings.extend([[] for _ in range(len(texts) - len(embeddings))])
        elif len(embeddings) > len(texts):
            embeddings = embeddings[:len(texts)]
        
        if show_progress:
            success_count = sum(1 for e in embeddings if e)
            logger.info(f"âœ… Embedding å®Œæˆ: {success_count}/{len(texts)} æˆåŠŸ")
        
        return embeddings
    
    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self._stats.copy()
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        for key in self._stats:
            self._stats[key] = 0


# å…¨å±€å•ä¾‹
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service(config: Optional[EmbeddingConfig] = None) -> EmbeddingService:
    """è·å– Embedding æœåŠ¡å•ä¾‹"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService(config)
    return _embedding_service


# ä¾¿æ·å‡½æ•°
async def embed_text(text: str) -> List[float]:
    """å¿«æ·æ–¹å¼: è·å–å•ä¸ªæ–‡æœ¬çš„ Embedding"""
    return await get_embedding_service().embed_text(text)


async def embed_batch(texts: List[str], show_progress: bool = False) -> List[List[float]]:
    """å¿«æ·æ–¹å¼: æ‰¹é‡è·å– Embedding"""
    return await get_embedding_service().embed_batch(texts, show_progress)
