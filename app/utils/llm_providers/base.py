# æ–‡ä»¶è·¯å¾„: app/utils/llm_providers/base.py
"""
LLM æä¾›å•†åŸºç±»å®šä¹‰

å®šä¹‰ç»Ÿä¸€çš„æ¥å£è§„èŒƒï¼Œæ‰€æœ‰ä¾›åº”å•†å®ç°éƒ½å¿…é¡»éµå¾ªæ­¤è§„èŒƒã€‚
é‡‡ç”¨é€‚é…å™¨æ¨¡å¼ï¼Œå°†ä¸åŒä¾›åº”å•†çš„ API ç»Ÿä¸€ä¸º OpenAI å…¼å®¹æ ¼å¼ã€‚
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, AsyncIterator, Union
from enum import Enum

from app.utils.retry import llm_retry, is_retryable_error

# é…ç½®æ—¥å¿—
logger = logging.getLogger("llm_provider")


class LLMProviderType(str, Enum):
    """æ”¯æŒçš„ LLM ä¾›åº”å•†ç±»å‹"""
    OPENAI = "openai"
    DEEPSEEK = "deepseek"
    ANTHROPIC = "anthropic"
    GEMINI = "gemini"


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    provider: LLMProviderType
    api_key: str
    model_name: str
    base_url: Optional[str] = None
    temperature: float = 0.1
    max_tokens: int = 4096
    timeout: int = 600
    extra_params: Dict[str, Any] = field(default_factory=dict)


@dataclass 
class LLMMessage:
    """æ¶ˆæ¯æ ¼å¼ (å…¼å®¹ OpenAI)"""
    role: str  # "system", "user", "assistant"
    content: str


@dataclass
class LLMUsage:
    """Token ä½¿ç”¨é‡"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0


@dataclass
class LLMChoice:
    """å“åº”é€‰é¡¹ (å…¼å®¹ OpenAI)"""
    index: int
    message: Optional[LLMMessage] = None
    delta: Optional[LLMMessage] = None  # æµå¼å“åº”æ—¶ä½¿ç”¨
    finish_reason: Optional[str] = None


@dataclass
class LLMResponse:
    """
    ç»Ÿä¸€çš„ LLM å“åº”æ ¼å¼
    
    è®¾è®¡ä¸ºå…¼å®¹ OpenAI çš„ ChatCompletion æ ¼å¼ï¼Œ
    ä½¿å¾—ç°æœ‰ä»£ç æ— éœ€å¤§å¹…ä¿®æ”¹å³å¯ä½¿ç”¨ã€‚
    """
    id: str
    model: str
    choices: List[LLMChoice]
    usage: Optional[LLMUsage] = None
    created: int = 0
    
    @property
    def content(self) -> str:
        """ä¾¿æ·æ–¹æ³•ï¼šè·å–ç¬¬ä¸€ä¸ªé€‰é¡¹çš„å†…å®¹"""
        if self.choices and self.choices[0].message:
            return self.choices[0].message.content
        return ""


# è¾…åŠ©ç±»å®šä¹‰ï¼ˆåœ¨ BaseLLMProvider å¤–éƒ¨ï¼Œé¿å…åµŒå¥—ç±»é—®é¢˜ï¼‰
class _CompletionsNamespace:
    """æ¨¡æ‹Ÿ client.chat.completions å‘½åç©ºé—´"""
    def __init__(self, provider: 'BaseLLMProvider'):
        self._provider = provider
    
    async def create(
        self,
        model: str = None,
        messages: List[Dict[str, str]] = None,
        temperature: float = None,
        max_tokens: int = None,
        stream: bool = False,
        timeout: int = None,
        **kwargs
    ) -> Union[LLMResponse, AsyncIterator[LLMResponse]]:
        """
        ç»Ÿä¸€çš„ completions.create æ¥å£
        
        å…¼å®¹ OpenAI SDK è°ƒç”¨æ–¹å¼:
        response = await client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": "Hello"}],
            stream=True
        )
        
        å†…ç½®é‡è¯•æœºåˆ¶:
        - è‡ªåŠ¨é‡è¯•ç½‘ç»œé”™è¯¯ã€è¶…æ—¶ã€é€Ÿç‡é™åˆ¶
        - æŒ‡æ•°é€€é¿ç­–ç•¥
        - æœ€å¤šé‡è¯• 3 æ¬¡
        """
        # åˆå¹¶é…ç½®
        _model = model or self._provider.config.model_name
        _temperature = temperature if temperature is not None else self._provider.config.temperature
        _max_tokens = max_tokens or self._provider.config.max_tokens
        _timeout = timeout or self._provider.config.timeout
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        _messages = [
            LLMMessage(role=m["role"], content=m["content"]) 
            for m in (messages or [])
        ]
        
        if stream:
            # æµå¼è¯·æ±‚: è¿”å›å¸¦é‡è¯•çš„å¼‚æ­¥ç”Ÿæˆå™¨
            return self._create_stream_with_retry(
                messages=_messages,
                model=_model,
                temperature=_temperature,
                max_tokens=_max_tokens,
                timeout=_timeout,
                **kwargs
            )
        else:
            # éæµå¼è¯·æ±‚: ä½¿ç”¨ tenacity é‡è¯•
            return await self._create_with_retry(
                messages=_messages,
                model=_model,
                temperature=_temperature,
                max_tokens=_max_tokens,
                timeout=_timeout,
                **kwargs
            )
    
    @llm_retry
    async def _create_with_retry(
        self,
        messages: List[LLMMessage],
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: int,
        **kwargs
    ) -> LLMResponse:
        """å¸¦é‡è¯•çš„éæµå¼è¯·æ±‚"""
        logger.debug(f"ğŸ”„ LLM è¯·æ±‚: model={model}, messages_count={len(messages)}")
        return await self._provider.chat_completions_create(
            messages=messages,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            **kwargs
        )
    
    async def _create_stream_with_retry(
        self,
        messages: List[LLMMessage],
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: int,
        max_retries: int = 3,
        **kwargs
    ) -> AsyncIterator[LLMResponse]:
        """
        å¸¦é‡è¯•çš„æµå¼è¯·æ±‚
        
        æ³¨æ„: æµå¼è¯·æ±‚çš„é‡è¯•ç­–ç•¥ä¸éæµå¼ä¸åŒ
        - å¦‚æœåœ¨è·å–æµä¹‹å‰å¤±è´¥ï¼Œå¯ä»¥é‡è¯•
        - å¦‚æœåœ¨æµä¼ è¾“è¿‡ç¨‹ä¸­å¤±è´¥ï¼Œéœ€è¦é‡æ–°å¼€å§‹
        """
        last_error = None
        
        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(f"ğŸ”„ LLM æµå¼è¯·æ±‚ (å°è¯• {attempt}/{max_retries}): model={model}")
                
                # è·å–æµç”Ÿæˆå™¨
                stream = self._provider.chat_completions_create_stream(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=timeout,
                    **kwargs
                )
                
                # è¿­ä»£æµå¹¶ yield
                async for chunk in stream:
                    yield chunk
                
                # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                return
                
            except Exception as e:
                last_error = e
                if is_retryable_error(e) and attempt < max_retries:
                    wait_time = min(2 ** attempt, 30)  # æŒ‡æ•°é€€é¿
                    logger.warning(
                        f"ğŸ”„ LLM æµå¼è¯·æ±‚å¤±è´¥ (å°è¯• {attempt}/{max_retries}): "
                        f"{type(e).__name__}: {e}. ç­‰å¾… {wait_time}s åé‡è¯•..."
                    )
                    import asyncio
                    await asyncio.sleep(wait_time)
                else:
                    # ä¸å¯é‡è¯•çš„é”™è¯¯æˆ–å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
                    logger.error(f"âŒ LLM æµå¼è¯·æ±‚æœ€ç»ˆå¤±è´¥: {type(e).__name__}: {e}")
                    raise
        
        # å¦‚æœèµ°åˆ°è¿™é‡Œï¼Œè¯´æ˜æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        if last_error:
            raise last_error


class _ChatNamespace:
    """æ¨¡æ‹Ÿ client.chat å‘½åç©ºé—´"""
    def __init__(self, provider: 'BaseLLMProvider'):
        self._provider = provider
        self.completions = _CompletionsNamespace(provider)


class BaseLLMProvider(ABC):
    """
    LLM æä¾›å•†æŠ½è±¡åŸºç±»
    
    æ‰€æœ‰ä¾›åº”å•†å®ç°éƒ½éœ€è¦ç»§æ‰¿æ­¤ç±»å¹¶å®ç°ä»¥ä¸‹æ–¹æ³•:
    - chat_completions_create: éæµå¼è¯·æ±‚
    - chat_completions_create_stream: æµå¼è¯·æ±‚
    
    ä¸ºäº†å…¼å®¹ç°æœ‰ä»£ç ï¼Œæä¾›ä¸€ä¸ªæ¨¡æ‹Ÿ OpenAI å®¢æˆ·ç«¯çš„ chat.completions æ¥å£ã€‚
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self._client = None
        # æ¨¡æ‹Ÿ OpenAI SDK çš„æ¥å£ç»“æ„
        self.chat = _ChatNamespace(self)
    
    @abstractmethod
    async def chat_completions_create(
        self,
        messages: List[LLMMessage],
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: int,
        **kwargs
    ) -> LLMResponse:
        """
        éæµå¼ Chat Completion è¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ Token æ•°
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            LLMResponse: ç»Ÿä¸€æ ¼å¼çš„å“åº”
        """
        pass
    
    @abstractmethod
    async def chat_completions_create_stream(
        self,
        messages: List[LLMMessage],
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: int,
        **kwargs
    ) -> AsyncIterator[LLMResponse]:
        """
        æµå¼ Chat Completion è¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ Token æ•°
            timeout: è¶…æ—¶æ—¶é—´
            
        Yields:
            LLMResponse: æµå¼å“åº”å—
        """
        pass
    
    @abstractmethod
    def validate_connection(self) -> bool:
        """éªŒè¯è¿æ¥æ˜¯å¦æ­£å¸¸"""
        pass
    
    @property
    def provider_name(self) -> str:
        """è·å–ä¾›åº”å•†åç§°"""
        return self.config.provider.value
    
    @property
    def model_name(self) -> str:
        """è·å–å½“å‰æ¨¡å‹åç§°"""
        return self.config.model_name
