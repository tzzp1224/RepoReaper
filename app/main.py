# æ–‡ä»¶è·¯å¾„: app/main.py
import sys
import io
import os
import time
import shutil
import asyncio
from contextlib import asynccontextmanager

# å¼ºåˆ¶ stdout ä½¿ç”¨ utf-8ï¼Œé˜²æ­¢ Windows æ§åˆ¶å°ä¹±ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import EventSourceResponse
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
import uvicorn

from app.core.config import settings
from app.services.agent_service import agent_stream
from app.services.chat_service import process_chat_stream, get_eval_data, clear_eval_data
from app.services.vector_service import vector_config, CHROMA_DIR, CONTEXT_DIR
from app.services.auto_evaluation_service import (
    init_auto_evaluation_service,
    get_auto_evaluation_service,
    EvaluationConfig
)
from evaluation.evaluation_framework import EvaluationEngine, EvaluationResult, DataRoutingEngine
from datetime import datetime
import uuid

settings.validate()

# === åå°æ¸…ç†ä»»åŠ¡ ===
async def cleanup_cron_job():
    """
    åå°ä»»åŠ¡ï¼šæ¯å°æ—¶è¿è¡Œä¸€æ¬¡ã€‚
    åˆ é™¤ Context ç›®å½•ä¸‹è¶…è¿‡ 24 å°æ—¶çš„ JSON æ–‡ä»¶ã€‚
    """
    while True:
        try:
            print(f"ğŸ§¹ [System] Starting scheduled data cleanup in {vector_config.DATA_DIR}...")
            now = time.time()
            cutoff = 24 * 3600  # 24å°æ—¶
            
            # 1. æ¸…ç† JSON Context æ–‡ä»¶
            if os.path.exists(CONTEXT_DIR):
                for filename in os.listdir(CONTEXT_DIR):
                    filepath = os.path.join(CONTEXT_DIR, filename)
                    # æ£€æŸ¥æœ€åä¿®æ”¹æ—¶é—´
                    if os.path.isfile(filepath) and (now - os.path.getmtime(filepath)) > cutoff:
                        try:
                            os.remove(filepath)
                            print(f"   - Deleted old context: {filename}")
                        except OSError as e:
                            print(f"   - Error deleting {filename}: {e}")

            # 2. ChromaDB æ¸…ç†ç­–ç•¥ (ä»…å ä½ï¼Œé€šå¸¸ä¸å»ºè®®æš´åŠ›åˆ é™¤)
            if os.path.exists(CHROMA_DIR):
                 pass 
            
        except Exception as e:
            print(f"âš ï¸ Cleanup Task Error: {e}")
        
        await asyncio.sleep(3600) # ç­‰å¾… 1 å°æ—¶

# === ç”Ÿå‘½å‘¨æœŸç®¡ç† ===
@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶è¿è¡Œ
    task = asyncio.create_task(cleanup_cron_job())
    yield
    # å…³é—­æ—¶è¿è¡Œ
    task.cancel()
    # æ¸…ç† GitHub å®¢æˆ·ç«¯è¿æ¥
    from app.utils.github_client import close_github_client
    await close_github_client()

app = FastAPI(title="GitHub RAG Agent", lifespan=lifespan)

# === åˆå§‹åŒ–è¯„ä¼°å¼•æ“ ===
from app.utils.llm_client import client
eval_engine = EvaluationEngine(llm_client=client, model_name=settings.default_model_name)
data_router = DataRoutingEngine()

# === åˆå§‹åŒ–è‡ªåŠ¨è¯„ä¼°æœåŠ¡ (Phase 1) ===
auto_eval_config = EvaluationConfig(
    enabled=True,
    use_ragas=False,              # Phase 1: å…ˆä¸ç”¨ Ragasï¼Œé¿å…é¢å¤–ä¾èµ–
    async_evaluation=True,        # å¼‚æ­¥æ¨¡å¼ï¼Œä¸é˜»å¡å“åº”
    min_quality_score=0.4,        # æœ€ä½åˆ†æ•°é˜ˆå€¼ï¼ˆ0.4 = åªæ‹’ç»æœ€å·®çš„ï¼‰
    min_query_length=10,          # æœ€å° query é•¿åº¦
    min_answer_length=100,        # æœ€å° answer é•¿åº¦
    require_repo_url=True,        # å¿…é¡»æœ‰ä»“åº“ URL
    require_code_in_context=True  # ä¸Šä¸‹æ–‡å¿…é¡»åŒ…å«ä»£ç 
)
auto_eval_service = init_auto_evaluation_service(
    eval_engine=eval_engine,
    data_router=data_router,
    config=auto_eval_config
)
print("âœ… Auto Evaluation Service Initialized")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"],
)

# === é™æ€æ–‡ä»¶ä¸å‰ç«¯ ===
app.mount("/static", StaticFiles(directory="app"), name="static")

@app.get("/", response_class=HTMLResponse)
async def read_root():
    # ç¡®ä¿ index.html è·¯å¾„æ­£ç¡®
    with open("frontend/index.html", "r", encoding="utf-8") as f:
        return f.read()

@app.get("/health")
def health_check():
    return {"status": "ok"}

@app.get("/analyze")
async def analyze(url: str, session_id: str, language: str = "en"): 
    if not session_id:
        return {"error": "Missing session_id"}
    return EventSourceResponse(agent_stream(url, session_id, language))

@app.post("/chat")
async def chat(request: Request):
    """
    èŠå¤©ç«¯ç‚¹ - è‡ªåŠ¨è¯„ä¼°ç‰ˆæœ¬
    
    æ”¹è¿›ç‚¹:
    1. ç«‹å³è¿”å›èŠå¤©ç»“æœï¼ˆä¸é˜»å¡ï¼‰
    2. åå°å¼‚æ­¥è¿›è¡Œè‡ªåŠ¨è¯„ä¼°
    3. è¯„ä¼°ç»“æœè‡ªåŠ¨å­˜å‚¨åˆ° evaluation/sft_data/
    """
    data = await request.json()
    user_query = data.get("query")
    session_id = data.get("session_id")
    repo_url = data.get("repo_url", "")
    
    if not user_query:
        return {"answer": "Please enter your question"}
    if not session_id:
        return {"answer": "Session lost"}

    # æ ‡è®°æµæ˜¯å¦å®Œæˆ
    stream_completed = False
    
    async def chat_stream_with_eval():
        """åŒ…è£… process_chat_streamï¼Œæµç»“æŸåè§¦å‘è¯„ä¼°"""
        nonlocal stream_completed
        
        # æ¸…é™¤æ—§çš„è¯„ä¼°æ•°æ®
        clear_eval_data(session_id)
        
        # æ‰§è¡ŒèŠå¤©æµ
        async for chunk in process_chat_stream(user_query, session_id):
            yield chunk
        
        # æµå®Œæˆåæ ‡è®°
        stream_completed = True
        
        # æµç»“æŸåè§¦å‘è¯„ä¼°ï¼ˆæ­¤æ—¶æ•°æ®å·²å­˜å‚¨åœ¨ chat_service ä¸­ï¼‰
        try:
            auto_eval_service = get_auto_evaluation_service()
            eval_data = get_eval_data(session_id)
            
            if auto_eval_service and eval_data and eval_data.answer:
                print(f"\nğŸ“Š [Auto-Eval] Starting evaluation for session {session_id}")
                print(f"   - Query: {user_query[:50]}...")
                print(f"   - Context length: {len(eval_data.retrieved_context)} chars")
                print(f"   - Answer length: {len(eval_data.answer)} chars")
                
                # å¼‚æ­¥æ‰§è¡Œè¯„ä¼°ï¼ˆä¸é˜»å¡æµç»“æŸï¼‰
                asyncio.create_task(
                    auto_eval_service.auto_evaluate_async(
                        query=user_query,
                        retrieved_context=eval_data.retrieved_context,
                        generated_answer=eval_data.answer,
                        session_id=session_id,
                        repo_url=repo_url,
                        language="zh" if any('\u4e00' <= c <= '\u9fff' for c in user_query) else "en"
                    )
                )
            else:
                if not auto_eval_service:
                    print("âš ï¸ Auto evaluation service not initialized")
                elif not eval_data:
                    print(f"âš ï¸ No eval data found for session {session_id}")
                elif not eval_data.answer:
                    print(f"âš ï¸ Empty answer for session {session_id}")
        except Exception as e:
            print(f"âš ï¸ Failed to trigger auto-eval: {e}")
            import traceback
            traceback.print_exc()
    
    # è¿”å›æµ
    return StreamingResponse(
        chat_stream_with_eval(),
        media_type="text/plain"
    )

# ===== Phase 2: æ–°å¢è¯„ä¼°ç«¯ç‚¹ =====

@app.post("/evaluate")
async def evaluate(request: Request):
    """
    è¯„ä¼°ç«¯ç‚¹: æ¥æ”¶ç”Ÿæˆç»“æœ,è¿›è¡Œå¤šç»´åº¦è¯„ä¼°
    
    POST /evaluate
    {
        "query": "ç”¨æˆ·é—®é¢˜",
        "retrieved_context": "æ£€ç´¢åˆ°çš„æ–‡ä»¶å†…å®¹",
        "generated_answer": "ç”Ÿæˆçš„å›ç­”",
        "session_id": "ä¼šè¯ID",
        "repo_url": "ä»“åº“URLï¼ˆå¯é€‰ï¼‰"
    }
    """
    try:
        data = await request.json()
        
        # æå–å¿…éœ€å­—æ®µ
        query = data.get("query")
        retrieved_context = data.get("retrieved_context", "")
        generated_answer = data.get("generated_answer")
        session_id = data.get("session_id", "unknown")
        repo_url = data.get("repo_url", "")
        
        if not query or not generated_answer:
            return {
                "error": "Missing required fields: query, generated_answer",
                "status": "failed"
            }
        
        # è°ƒç”¨è¯„ä¼°å¼•æ“è·å–ç”Ÿæˆå±‚æŒ‡æ ‡
        generation_metrics = await eval_engine.evaluate_generation(
            query=query,
            retrieved_context=retrieved_context,
            generated_answer=generated_answer
        )
        
        # æ„å»ºå®Œæ•´çš„è¯„ä¼°ç»“æœå¯¹è±¡
        evaluation_result = EvaluationResult(
            session_id=session_id,
            query=query,
            repo_url=repo_url,
            timestamp=datetime.now(),
            language="en",
            generation_metrics=generation_metrics
        )
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        evaluation_result.compute_overall_score()
        
        # æ•°æ®è·¯ç”±: æ ¹æ®å¾—åˆ†å°†æ ·æœ¬åˆ†ç±»
        quality_tier = data_router.route_sample(evaluation_result)
        
        return {
            "status": "success",
            "evaluation": {
                "faithfulness": generation_metrics.faithfulness,
                "answer_relevance": generation_metrics.answer_relevance,
                "answer_completeness": generation_metrics.answer_completeness,
                "overall_score": evaluation_result.overall_score
            },
            "quality_tier": quality_tier,
            "session_id": session_id
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "status": "failed"
        }


# ===== è‡ªåŠ¨è¯„ä¼°ç›¸å…³ç«¯ç‚¹ =====

@app.get("/auto-eval/review-queue")
async def get_review_queue():
    """
    è·å–éœ€è¦äººå·¥å®¡æŸ¥çš„æ ·æœ¬åˆ—è¡¨
    
    è¿™äº›æ˜¯è¯„ä¼°å‡ºç°å¼‚å¸¸ï¼ˆè‡ªå·±çš„åˆ†æ•°å’ŒRagasåˆ†æ•°å·®å¼‚è¿‡å¤§ï¼‰çš„æ ·æœ¬
    éœ€è¦äººå·¥åˆ¤æ–­å“ªä¸ªè¯„ä¼°å™¨æ›´å‡†ç¡®
    
    GET /auto-eval/review-queue
    """
    try:
        auto_eval_service = get_auto_evaluation_service()
        if not auto_eval_service:
            return {"error": "Auto evaluation service not initialized", "status": "failed"}
        
        queue = auto_eval_service.get_review_queue()
        
        return {
            "status": "success",
            "queue_size": len(queue),
            "samples": [
                {
                    "index": i,
                    "query": item["eval_result"].query,
                    "custom_score": item["custom_score"],
                    "ragas_score": item["ragas_score"],
                    "diff": item["diff"],
                    "quality_tier": item["eval_result"].data_quality_tier.value,
                    "timestamp": item["timestamp"]
                }
                for i, item in enumerate(queue)
            ]
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}


@app.post("/auto-eval/approve/{index}")
async def approve_sample(index: int):
    """
    äººå·¥æ‰¹å‡†æŸä¸ªæ ·æœ¬ï¼ˆæ¥å—è¯¥è¯„ä¼°ç»“æœï¼‰
    
    POST /auto-eval/approve/0
    """
    try:
        auto_eval_service = get_auto_evaluation_service()
        if not auto_eval_service:
            return {"error": "Auto evaluation service not initialized", "status": "failed"}
        
        auto_eval_service.approve_sample(index)
        
        return {
            "status": "success",
            "message": f"Sample {index} approved and stored"
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}


@app.post("/auto-eval/reject/{index}")
async def reject_sample(index: int):
    """
    äººå·¥æ‹’ç»æŸä¸ªæ ·æœ¬ï¼ˆæŠ›å¼ƒè¯¥è¯„ä¼°ç»“æœï¼‰
    
    POST /auto-eval/reject/0
    """
    try:
        auto_eval_service = get_auto_evaluation_service()
        if not auto_eval_service:
            return {"error": "Auto evaluation service not initialized", "status": "failed"}
        
        auto_eval_service.reject_sample(index)
        
        return {
            "status": "success",
            "message": f"Sample {index} rejected and removed from queue"
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}


@app.get("/auto-eval/stats")
async def auto_eval_stats():
    """
    è·å–è‡ªåŠ¨è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯
    
    GET /auto-eval/stats
    """
    try:
        auto_eval_service = get_auto_evaluation_service()
        if not auto_eval_service:
            return {"error": "Auto evaluation service not initialized", "status": "failed"}
        
        queue = auto_eval_service.get_review_queue()
        
        return {
            "status": "success",
            "auto_evaluation": {
                "enabled": auto_eval_service.config.enabled,
                "use_ragas": auto_eval_service.config.use_ragas,
                "async_mode": auto_eval_service.config.async_evaluation,
                "custom_weight": auto_eval_service.config.custom_weight,
                "ragas_weight": auto_eval_service.config.ragas_weight,
                "diff_threshold": auto_eval_service.config.diff_threshold
            },
            "review_queue_size": len(queue),
            "last_update": datetime.now().isoformat()
        }
    except Exception as e:
        return {"error": str(e), "status": "failed"}


@app.get("/evaluation/stats")
async def evaluation_stats():
    """
    è·å–è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯
    
    GET /evaluation/stats
    """
    try:
        stats = eval_engine.get_statistics()
        return {
            "status": "success",
            "statistics": {
                "total_evaluations": stats.get("total_evaluations", 0),
                "average_score": stats.get("average_score", 0),
                "quality_distribution": stats.get("quality_distribution", {}),
                "top_issues": stats.get("top_issues", [])
            }
        }
    except Exception as e:
        return {
            "error": str(e),
            "status": "failed"
        }


@app.get("/dashboard/quality-distribution")
async def quality_distribution():
    """
    è·å–æ•°æ®è´¨é‡åˆ†å¸ƒ (ç”¨äºä»ªè¡¨ç›˜)
    
    GET /dashboard/quality-distribution
    """
    try:
        distribution = data_router.get_distribution()
        return {
            "status": "success",
            "distribution": {
                "gold": distribution.get("gold", 0),
                "silver": distribution.get("silver", 0),
                "bronze": distribution.get("bronze", 0),
                "rejected": distribution.get("rejected", 0),
                "corrected": distribution.get("corrected", 0)
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "error": str(e),
            "status": "failed"
        }


@app.get("/dashboard/bad-cases")
async def bad_cases():
    """
    è·å–ä½è´¨é‡æ ·æœ¬ (ç”¨äºäººå·¥å®¡æ ¸)
    
    GET /dashboard/bad-cases
    """
    try:
        bad_samples = data_router.get_bad_samples(limit=10)
        return {
            "status": "success",
            "bad_cases": [
                {
                    "query": s.get("query", ""),
                    "issue": s.get("issue", ""),
                    "score": s.get("score", 0)
                }
                for s in bad_samples
            ],
            "total_bad_cases": len(bad_samples)
        }
    except Exception as e:
        return {
            "error": str(e),
            "status": "failed"
        }


if __name__ == "__main__":
    # ç”Ÿäº§æ¨¡å¼å»ºè®®å…³æ‰ reload
    uvicorn.run("app.main:app", host=settings.HOST, port=settings.PORT, reload=False)