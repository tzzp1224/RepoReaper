import sys
import io
import json
import asyncio
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from sse_starlette.sse import EventSourceResponse
from google import genai
from dotenv import load_dotenv
import os

# å¼•å…¥ä¹‹å‰çš„å·¥å…·
from tools_github import get_repo_structure, get_file_content
# å¼•å…¥æ–°å†™çš„å‘é‡åº“
from vector_store import VectorStore

load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
client = genai.Client(api_key=GEMINI_API_KEY)
MODEL_NAME = "gemini-3-flash-preview"

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# ğŸŒ å…¨å±€å”¯ä¸€çš„å‘é‡æ•°æ®åº“å®ä¾‹
# æ³¨æ„ï¼šæ¯æ¬¡é‡å¯æœåŠ¡ï¼Œå†…å­˜æ•°æ®åº“ä¼šæ¸…ç©ºï¼Œéœ€è¦é‡æ–°åˆ†æä¸€æ¬¡ä»“åº“
vector_db = VectorStore()

# ==========================================
# 1. åˆ†ææµç¨‹ (Indexing)
# ==========================================
async def agent_stream(repo_url: str):
    yield json.dumps({"step": "init", "message": f"ğŸš€ æ­£åœ¨è¿æ¥ GitHub: {repo_url}..."})
    await asyncio.sleep(0.5)
    
    try:
        # é‡ç½®æ•°æ®åº“ (é¿å…æ··æ·†ä¸åŒé¡¹ç›®)
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬è¿™é‡Œé‡æ–°å®ä¾‹åŒ–ä¸€ä¸ªï¼Œæˆ–è€…ä½ å¯ä»¥å†™ä¸ª clear æ–¹æ³•
        global vector_db
        vector_db = VectorStore() 

        file_list = get_repo_structure(repo_url)
        if not file_list:
            yield json.dumps({"step": "error", "message": "âŒ æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨ã€‚"})
            return

        yield json.dumps({"step": "fetched", "message": f"ğŸ“¦ è·å–æˆåŠŸï¼å…±å‘ç° {len(file_list)} ä¸ªæ–‡ä»¶ã€‚"})
        
        # æˆªå–
        limit = 400
        file_list_str = "\n".join(file_list[:limit])

        # Step 2: æ€è€ƒ
        yield json.dumps({"step": "thinking", "message": "ğŸ¤– Gemini æ­£åœ¨æŒ‘é€‰æ ¸å¿ƒä»£ç ..."})
        selection_prompt = f"""
        You are a Senior Software Architect.
        Repo Structure: {file_list_str}
        Identify top 3-5 critical files to understand the logic.
        Return raw JSON list. Example: ["README.md", "main.py"]
        """
        response = client.models.generate_content(model=MODEL_NAME, contents=selection_prompt)
        
        selected_files = ["README.md"]
        try:
            clean_text = response.text.replace("```json", "").replace("```", "").strip()
            selected_files = json.loads(clean_text)
        except:
            pass

        yield json.dumps({"step": "plan", "message": f"ğŸ¯ å†³å®šè¯»å–: {selected_files}"})
        
        # Step 3: ä¸‹è½½ + å»ºåº“ (Indexing)
        code_context = ""
        documents = []
        metadatas = []

        for i, file_path in enumerate(selected_files):
            yield json.dumps({"step": "download", "message": f"ğŸ“¥ [{i+1}/{len(selected_files)}] è¯»å–å¹¶å­˜å…¥çŸ¥è¯†åº“: {file_path}..."})
            content = get_file_content(repo_url, file_path)
            if content:
                # ç®€å•å¤„ç†ï¼šæŠŠæ•´ä¸ªæ–‡ä»¶å½“åšä¸€ä¸ª chunk (å®é™… RAG ä¸­ä¼šæŒ‰å­—ç¬¦åˆ‡åˆ†)
                # ä¸ºäº†é˜²æ­¢æ–‡ä»¶å¤ªå¤§ï¼Œæˆ‘ä»¬æˆªå–å‰ 8000 å­—ç¬¦
                snippet = content[:8000]
                documents.append(snippet)
                metadatas.append({"file": file_path})
                
                # æ‹¼æ¥ç”¨äºç”Ÿæˆæ€»ç»“
                code_context += f"\n\n=== FILE: {file_path} ===\n{snippet}"
        
        # â­ï¸ æ ¸å¿ƒåŠ¨ä½œï¼šå­˜å…¥å‘é‡æ•°æ®åº“
        yield json.dumps({"step": "indexing", "message": "ğŸ§  æ­£åœ¨æ„å»º RAG å‘é‡ç´¢å¼•..."})
        vector_db.add_documents(documents, metadatas)

        # Step 4: ç”ŸæˆæŠ¥å‘Š
        yield json.dumps({"step": "generating", "message": "ğŸ“ æ­£åœ¨æ’°å†™åˆ†ææŠ¥å‘Š..."})
        
        analysis_prompt = f"""
        You are a Tech Lead.
        Based on these files: {code_context}
        Write a concise technical report (in Chinese). Markdown format.
        """
        
        final_response = client.models.generate_content(
            model=MODEL_NAME, contents=analysis_prompt
        )
        
        # æ¨¡æ‹Ÿæµå¼æ¨é€
        final_text = final_response.text
        chunk_size = 50
        for i in range(0, len(final_text), chunk_size):
            chunk = final_text[i:i+chunk_size]
            yield json.dumps({"step": "report_chunk", "chunk": chunk})
            await asyncio.sleep(0.05) 

        yield json.dumps({"step": "finish", "message": "âœ… åˆ†æå®Œæˆï¼ç°åœ¨ä½ å¯ä»¥å‘æˆ‘æé—®äº†ã€‚"})

    except Exception as e:
        yield json.dumps({"step": "error", "message": f"ğŸ’¥ é”™è¯¯: {str(e)}"})

# ==========================================
# 2. èŠå¤©æ¥å£ (Retrieval & Chat)
# ==========================================
class ChatRequest(json.JSONEncoder):
    # Pydantic ä¹Ÿå¯ä»¥ï¼Œè¿™é‡Œå·æ‡’ç”¨ dict
    pass

@app.post("/chat")
async def chat(request: Request):
    data = await request.json()
    user_query = data.get("query")
    
    if not user_query:
        return {"answer": "è¯·è¾“å…¥é—®é¢˜"}

    print(f"User asked: {user_query}")

    # 1. æ£€ç´¢ (Retrieval)
    # å»å‘é‡åº“é‡Œæ‰¾ 3 ä¸ªæœ€ç›¸å…³çš„ä»£ç ç‰‡æ®µ
    relevant_docs = vector_db.search(user_query, top_k=3)
    
    context_str = ""
    for doc in relevant_docs:
        context_str += f"\n--- ç‰‡æ®µæ¥è‡ª {doc['file']} ---\n{doc['content'][:1000]}...\n"

    # 2. å¢å¼ºç”Ÿæˆ (Generation)
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç²¾é€šä»£ç çš„ AI åŠ©æ‰‹ã€‚
    æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä»£ç ä¸Šä¸‹æ–‡ (Context)ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚

    === Context ===
    {context_str}

    === Question ===
    {user_query}
    """

    try:
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=prompt
        )
        return {"answer": response.text, "sources": [d['file'] for d in relevant_docs]}
    except Exception as e:
        return {"answer": f"æŠ±æ­‰ï¼Œæ€è€ƒæ—¶å‡ºé”™äº†: {str(e)}"}

@app.get("/analyze")
async def analyze(url: str):
    return EventSourceResponse(agent_stream(url))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)